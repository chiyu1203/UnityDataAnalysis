{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a useful function to generate a data list for further analysis\n",
    "import os,json,sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import linalg as LA\n",
    "from scipy import stats\n",
    "from locustvr_converter import preprocess_matrex_data\n",
    "import scipy.stats as st\n",
    "##need to add this additional cell because useful tools are in another folder. Need to integrate these two folders one day\n",
    "current_working_directory = Path.cwd()\n",
    "parent_dir = current_working_directory.resolve().parents[0]\n",
    "sys.path.insert(0, str(parent_dir) + \"\\\\utilities\")\n",
    "from useful_tools import select_animals_gpt,find_file\n",
    "# sys.path.insert(0, str(parent_dir) + \"\\\\unityvr\\\\unityvr\\\\analysis\")\n",
    "# from headDirection import circDistAbs,circDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Session 0.1: Introducing helper functions for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/40642061/how-to-set-axis-ticks-in-multiples-of-pi-python-matplotlib\n",
    "def multiple_formatter(denominator=2, number=np.pi, latex='\\pi'):\n",
    "    def gcd(a, b):\n",
    "        while b:\n",
    "            a, b = b, a%b\n",
    "        return a\n",
    "    def _multiple_formatter(x, pos):\n",
    "        den = denominator\n",
    "        num = int(np.rint(den*x/number))\n",
    "        com = gcd(num,den)\n",
    "        (num,den) = (int(num/com),int(den/com))\n",
    "        if den==1:\n",
    "            if num==0:\n",
    "                return r'$0$'\n",
    "            if num==1:\n",
    "                return r'$%s$'%latex\n",
    "            elif num==-1:\n",
    "                return r'$-%s$'%latex\n",
    "            else:\n",
    "                return r'$%s%s$'%(num,latex)\n",
    "        else:\n",
    "            if num==1:\n",
    "                return r'$\\frac{%s}{%s}$'%(latex,den)\n",
    "            elif num==-1:\n",
    "                return r'$\\frac{-%s}{%s}$'%(latex,den)\n",
    "            else:\n",
    "                return r'$\\frac{%s%s}{%s}$'%(num,latex,den)\n",
    "    return _multiple_formatter\n",
    "\n",
    "class Multiple:\n",
    "    def __init__(self, denominator=2, number=np.pi, latex='\\pi'):\n",
    "        self.denominator = denominator\n",
    "        self.number = number\n",
    "        self.latex = latex\n",
    "    def locator(self):\n",
    "        return plt.MultipleLocator(self.number / self.denominator)\n",
    "    def formatter(self):\n",
    "        return plt.FuncFormatter(multiple_formatter(self.denominator, self.number, self.latex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Session 0.2: Load analysis methods in python dictionary form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = \"./analysis_methods_dictionary.json\"\n",
    "with open(json_file, \"r\") as f:\n",
    "    analysis_methods = json.loads(f.read())\n",
    "    \n",
    "#Put the folder of your Unity folder below\n",
    "#thisDataset =\"D:/MatrexVR_Swarm_Data/RunData\"\n",
    "thisDataset =\"D:/MatrexVR_blackbackground_Data/RunData\"\n",
    "#thisDataset =\"D:/MatrexVR_grass1_Data/RunData\"\n",
    "#parameter name means independent variable in the experiment\n",
    "parameter_name='mu'\n",
    "#parameter_name='agent_speed'\n",
    "analysis_window=analysis_methods.get(\"analysis_window\")\n",
    "monitor_fps=analysis_methods.get(\"monitor_fps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Session 1.0: select animals based on condition and return which a directory list and a list of vr rig number to specify which animal to analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your Excel file\n",
    "dir_list = []\n",
    "file_type=\".h5\"\n",
    "using_google_sheet=True\n",
    "sheet_name = \"Unity_MatrexVR\"\n",
    "experiment_name=analysis_methods.get(\"experiment_name\")\n",
    "# if type(thisDataset) == str:\n",
    "#     thisDataset = Path(thisDataset)\n",
    "if analysis_methods.get(\"load_individual_data\") == True:\n",
    "    if using_google_sheet==True:\n",
    "        # database_id = \"1UL4eEUrQMapx9xz11-IyOSlPBcep3I9vBJ2uGgVudb8\"\n",
    "        #         #https://docs.google.com/spreadsheets/d/1UL4eEUrQMapx9xz11-IyOSlPBcep3I9vBJ2uGgVudb8/edit?usp=sharing\n",
    "        # url = f\"https://docs.google.com/spreadsheets/d/{database_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
    "        database_id = \"1UL4eEUrQMapx9xz11-IyOSlPBcep1I9vBJ2uGgVudb8\"\n",
    "                #https://docs.google.com/spreadsheets/d/1UL4eEUrQMapx9xz11-IyOSlPBcep1I9vBJ2uGgVudb8/edit?usp=sharing\n",
    "        url = f\"https://docs.google.com/spreadsheets/d/{database_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
    "        #df = pd.read_excel(url, engine='openpyxl')## use this function if the file is not google sheet but uploaded excel file\n",
    "        df = pd.read_csv(url)\n",
    "    else:\n",
    "        excel_file_path = \"Z:/DATA/experiment_trackball_Optomotor/Locusts Management.xlsx\"\n",
    "        print(f\"using a database {excel_file_path} from the server but this file might be outdated\")\n",
    "        # Create a 'with' statement to open and read the Excel file\n",
    "        with pd.ExcelFile(excel_file_path) as xls:\n",
    "            # Read the Excel sheet into a DataFrame with the sheet name (folder name)\n",
    "            df = pd.read_excel(xls, sheet_name)\n",
    "        ##list up the conditions and answers as strings for input argument to select animal. One condition must pair with one answer\n",
    "    if analysis_methods.get(\"select_animals_by_condition\") == True:\n",
    "       #animal_of_interest=select_animals_gpt(df,\"Independent variable (list up all of them in the experiment)\",\"gregarious_leader_grass\",\"Excluding this animal from analysis (Usually when animals die or molt, T/F)\",\"F\")\n",
    "        #animal_of_interest=select_animals_gpt(df,\"Independent variable (list up all of them in the experiment)\",\"gregarious_leader_black\",\"Excluding this animal from analysis (Usually when animals die or molt, T/F)\",\"F\")\n",
    "        #animal_of_interest=select_animals_gpt(df,\"Independent variable (list up all of them in the experiment)\",parameter_name)\n",
    "        animal_of_interest=select_animals_gpt(df,\"Independent variable (list up all of them in the experiment)\",parameter_name,\"Excluding this animal from analysis (Usually when animals die or molt, T/F)\",\"F\")\n",
    "    else:\n",
    "        animal_of_interest=df\n",
    "    folder_name=animal_of_interest[\"folder name\"].values\n",
    "\n",
    "    dir_tile=np.tile(thisDataset, (len(folder_name), 1))\n",
    "    vr_no=animal_of_interest[\"VR number\"].values\n",
    "    dir_list = [''.join([x[0], '/', y]) for x,y in zip(dir_tile,folder_name)]\n",
    "    #dir_dict = itertools.zip_longest(dir_list, vr_no.tolist())\n",
    "    dir_dict = zip(dir_list, vr_no.tolist())\n",
    "else:\n",
    "    for root, dirs, files in os.walk(thisDataset):\n",
    "        for folder in dirs:\n",
    "            folder_path=os.path.join(root,folder)\n",
    "            if any(name.endswith(file_type) for name in os.listdir(folder_path)):\n",
    "                dir_list.append(folder_path.replace(\"\\\\\", \"/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session 1.1: introduce helper functions to make plot and calculate speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from matplotlib import cm\n",
    "class MplColorHelper:\n",
    "    def __init__(self, cmap_name, start_val, stop_val):\n",
    "        self.cmap_name = cmap_name\n",
    "        self.cmap = plt.get_cmap(cmap_name)\n",
    "        self.norm = mpl.colors.Normalize(vmin=start_val, vmax=stop_val)\n",
    "        self.scalarMap = cm.ScalarMappable(norm=self.norm, cmap=self.cmap)\n",
    "\n",
    "    def get_rgb(self, val):\n",
    "        return self.scalarMap.to_rgba(val)\n",
    "colormap_name = \"viridis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_speed(dif_x,dif_y,ts,number_frame_scene_changing=12):\n",
    "    focal_distance_fbf=np.sqrt(np.sum([dif_x**2,dif_y**2],axis=0))\n",
    "    focal_distance_fbf[0:number_frame_scene_changing]=np.nan\n",
    "    instant_speed=focal_distance_fbf/np.diff(ts)\n",
    "    return instant_speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unwrap_degree(angle_rad,number_frame_scene_changing):\n",
    "    angle_rad[np.isnan(angle_rad)] = 0\n",
    "    # angle_rad=np.unwrap(angle_rad)\n",
    "    # angular_velocity=np.diff(np.unwrap(angle_rad))\n",
    "    ang_deg = np.mod(np.rad2deg(angle_rad),360.) ## if converting the unit to degree\n",
    "    angular_velocity=np.diff(np.unwrap(ang_deg,period=360))##if converting the unit to degree\n",
    "    angle_rad[0:number_frame_scene_changing]=np.nan\n",
    "    angular_velocity[0:number_frame_scene_changing]=np.nan     \n",
    "    return angle_rad,angular_velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trajectory_analysis(df_XY):\n",
    "    trajec_lim=150\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=2, ncols=2, figsize=(18, 14), tight_layout=True\n",
    "    )\n",
    "    plt.rcParams.update(plt.rcParamsDefault)\n",
    "    plt.rcParams.update({'font.size': 8})\n",
    "    # Set the axis line width to 2\n",
    "    plt.rcParams['ytick.major.width'] = 2\n",
    "    plt.rcParams['xtick.major.width'] = 2\n",
    "    plt.rcParams['axes.linewidth'] = 2\n",
    "    #plt.rcParams['font.family'] = 'Helvetica'\n",
    "    cmap = plt.get_cmap('viridis')\n",
    "    ax1, ax2, ax3, ax4 = axes.flatten()\n",
    "    ax1.set(\n",
    "        xlim=(-1*trajec_lim, trajec_lim),\n",
    "        ylim=(-1*trajec_lim, trajec_lim),\n",
    "        yticks=([-1*trajec_lim, 0, trajec_lim]),\n",
    "        xticks=([-1*trajec_lim, 0, trajec_lim]),\n",
    "        aspect=('equal'),\n",
    "        title='ISI'\n",
    "    )\n",
    "    ax2.set(\n",
    "        xlim=(-1*trajec_lim, trajec_lim),\n",
    "        ylim=(-1*trajec_lim, trajec_lim),\n",
    "        yticks=([-1*trajec_lim, 0, trajec_lim]),\n",
    "        xticks=([-1*trajec_lim, 0, trajec_lim]),\n",
    "        aspect=('equal'),\n",
    "        title='Direction 0'\n",
    "    )\n",
    "    ax3.set(\n",
    "        xlim=(-1*trajec_lim, trajec_lim),\n",
    "        ylim=(-1*trajec_lim, trajec_lim),\n",
    "        yticks=([-1*trajec_lim, 0, trajec_lim]),\n",
    "        xticks=([-1*trajec_lim, 0, trajec_lim]),\n",
    "        aspect=('equal'),\n",
    "        title='Direction 45'\n",
    "    )\n",
    "    ax4.set(\n",
    "        xlim=(-1*trajec_lim, trajec_lim),\n",
    "        ylim=(-1*trajec_lim, trajec_lim),\n",
    "        yticks=([-1*trajec_lim, 0, trajec_lim]),\n",
    "        xticks=([-1*trajec_lim, 0, trajec_lim]),\n",
    "        aspect=('equal'),\n",
    "        title='Direction 315'\n",
    "    )      \n",
    "    for key, grp in df_XY.groupby('fname'):\n",
    "        if grp['type'][0]=='empty_trial':\n",
    "            ax1.scatter(grp[\"X\"].values,grp[\"Y\"].values, c=np.arange(grp.shape[0]), marker=\".\", alpha=0.5)\n",
    "        else:\n",
    "            if grp['mu'][0]==0:\n",
    "                ax2.scatter(grp[\"X\"].values,grp[\"Y\"].values, c=np.arange(grp.shape[0]), marker=\".\", alpha=0.5)\n",
    "            elif grp['mu'][0]==45:\n",
    "                ax3.scatter(grp[\"X\"].values,grp[\"Y\"].values, c=np.arange(grp.shape[0]), marker=\".\", alpha=0.5)\n",
    "            elif grp['mu'][0]==315:\n",
    "                ax4.scatter(grp[\"X\"].values,grp[\"Y\"].values, c=np.arange(grp.shape[0]), marker=\".\", alpha=0.5)\n",
    "            else:\n",
    "                print(\"unexpected direction\")\n",
    "    ax2.plot(df_agent.loc[df_agent['mu'] == 0]['X'].values,df_agent.loc[df_agent['mu'] == 0]['Y'].values,color='k')\n",
    "    ax3.plot(df_agent.loc[df_agent['mu'] == 45]['X'].values,df_agent.loc[df_agent['mu'] == 45]['Y'].values,color='k')\n",
    "    ax4.plot(df_agent.loc[df_agent['mu'] == 315]['X'].values,df_agent.loc[df_agent['mu'] == 315]['Y'].values,color='k')\n",
    "\n",
    "    #plt.savefig(fig_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_plot(target_distance,instant_speed,angles,analysis_window):\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=3, ncols=1, figsize=(9, 7), tight_layout=True\n",
    "    )\n",
    "    plt.rcParams.update(plt.rcParamsDefault)\n",
    "    plt.rcParams.update({'font.size': 8})\n",
    "    # Set the axis line width to 2\n",
    "    plt.rcParams['ytick.major.width'] = 2\n",
    "    plt.rcParams['xtick.major.width'] = 2\n",
    "    plt.rcParams['axes.linewidth'] = 2\n",
    "    cmap = plt.get_cmap('viridis')\n",
    "    ax1, ax2, ax3= axes.flatten()\n",
    "    ax1.set(\n",
    "        title='Distance'\n",
    "    )\n",
    "    ax2.set(\n",
    "        title='Instant Speed'\n",
    "    )\n",
    "    ax3.set(\n",
    "        # yticks=[0,90,180],\n",
    "        #ylim=(-np.pi-0.1, np.pi+0.1),\n",
    "        # ylim=(-5, 185),\n",
    "        title='angular deviation'\n",
    "    )\n",
    "    # ax3.yaxis.set_major_locator(plt.MultipleLocator(np.pi / 2))\n",
    "    # ax3.yaxis.set_minor_locator(plt.MultipleLocator(np.pi / 4))\n",
    "    # ax3.yaxis.set_major_formatter(plt.FuncFormatter(multiple_formatter()))\n",
    "    ax1.plot(np.arange(target_distance.shape[0]),target_distance)\n",
    "    ax2.plot(np.arange(instant_speed.shape[0]),instant_speed)\n",
    "    ax3.plot(np.arange(angles.shape[0]),angles)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.0: Pool animal's response together according to some criteria (from none criteria to criteria that can define the follow behaviour)\n",
    "Output1: a list 'follow_proportion_across_animals' showing the proportion of 'follow' time for each animal (across trials)\n",
    "\n",
    "Output2: a list 'relative_pos_all_animals' showing relative position between virtual and focal locusts across time. 1st and 2nd columns shown relative x and y, 3rd columns shown virtual animal's moving direction. 4th column shown the timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_methods.update({\"plotting_trajectory\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajec_lim=150\n",
    "max_target_distance=50\n",
    "number_frame_scene_changing=12\n",
    "extract_follow_epoches=True\n",
    "relative_pos_all_animals=[]\n",
    "trial_evaluation_across_animals=[]\n",
    "follow_proportion_across_animals=[]\n",
    "animal_id=0\n",
    "for this_dir,this_vr in zip(dir_list,vr_no):\n",
    "    if Path(this_dir).is_dir()==False:\n",
    "        continue\n",
    "    agent_pattern = f\"VR{this_vr}*agent_full.h5\"\n",
    "    xy_pattern = f\"VR{this_vr}*XY_full.h5\"\n",
    "    found_result = find_file(Path(this_dir), agent_pattern)\n",
    "    df_agent = pd.read_hdf(found_result)\n",
    "    found_result = find_file(Path(this_dir), xy_pattern)\n",
    "    df_XY = pd.read_hdf(found_result)\n",
    "    df_agent['this_vr']=this_vr\n",
    "    df_agent['fname']=df_agent['fname'].astype(str) + '_' + df_agent['this_vr'].astype(str)   \n",
    "    df_XY['this_vr']=this_vr\n",
    "    df_XY['fname']=df_XY['fname'].astype(str) + '_' + df_XY['this_vr'].astype(str)\n",
    "    if analysis_methods.get(\"plotting_trajectory\"):\n",
    "        trajectory_analysis(df_XY)\n",
    "    dif_across_trials=[]\n",
    "    follow_epochs_across_trials=np.ones((len(df_agent['fname'].unique()),1))\n",
    "    total_epochs_across_trials=np.ones((len(df_agent['fname'].unique()),1))\n",
    "    trial_evaluation_list=[]\n",
    "    trial_id=0\n",
    "    for key, grp in df_XY.groupby('fname'):\n",
    "        if grp['type'][0]=='empty_trial':\n",
    "            focal_xy=np.vstack((grp[\"X\"].to_numpy(),grp[\"Y\"].to_numpy()))\n",
    "            dif_x=np.diff(focal_xy[0])\n",
    "            dif_y=np.diff(focal_xy[1])\n",
    "            #distance_from_centre=np.sqrt(np.sum([focal_xy[0]**2,focal_xy[1]**2],axis=0))\n",
    "            ts=grp[\"ts\"].to_numpy()\n",
    "            instant_speed= calculate_speed(dif_x,dif_y,ts)\n",
    "            focal_distance_ISI=instant_speed*np.diff(ts)\n",
    "            heading_direction = grp[\"heading\"].to_numpy()\n",
    "            _,turn_degree_ISI=unwrap_degree(heading_direction,number_frame_scene_changing)\n",
    "            pre_stim_ISI=grp['duration'][0]\n",
    "            continue\n",
    "        else:\n",
    "            #print(f\"stimulus degree: {grp['mu'][0]}\")\n",
    "            focal_xy=np.vstack((grp[\"X\"].to_numpy(),grp[\"Y\"].to_numpy()))\n",
    "            dif_x=np.diff(focal_xy[0])\n",
    "            dif_y=np.diff(focal_xy[1])\n",
    "            distance_from_centre=np.sqrt(np.sum([focal_xy[0]**2,focal_xy[1]**2],axis=0))\n",
    "            ts=grp[\"ts\"].to_numpy()\n",
    "            instant_speed= calculate_speed(dif_x,dif_y,ts)\n",
    "            focal_distance_fbf=instant_speed*np.diff(ts)\n",
    "            heading_direction = grp[\"heading\"].to_numpy()\n",
    "            _,turn_degree_fbf=unwrap_degree(heading_direction,number_frame_scene_changing)\n",
    "            #angular_velocity=turn_degree_fbf/np.diff(ts)\n",
    "\n",
    "            agent_xy=np.vstack((df_agent[df_agent['fname']==key][\"X\"].to_numpy(),df_agent[df_agent['fname']==key][\"Y\"].to_numpy()))\n",
    "            agent_distance_fbf=np.sqrt(np.sum([np.diff(agent_xy)[0]**2,np.diff(agent_xy)[1]**2],axis=0))\n",
    "            vector_dif=agent_xy-focal_xy\n",
    "            theta = np.radians(grp['mu'][0]-360)  \n",
    "            rot_matrix = np.array([[np.cos(theta), -np.sin(theta)],\n",
    "                                [np.sin(theta), np.cos(theta)]])# calculate the rotation matrix to align the agent to move along the same direction\n",
    "            vector_dif_rotated=rot_matrix @ vector_dif\n",
    "            target_distance=LA.norm(vector_dif, axis=0)\n",
    "            dot_product=np.diag(np.matmul(np.transpose(np.diff(focal_xy)),np.diff(agent_xy)))\n",
    "            angles = np.arccos(dot_product/focal_distance_fbf/agent_distance_fbf)\n",
    "            angles_in_degree= angles*180/np.pi\n",
    "            if analysis_methods.get(\"plotting_trajectory\"):\n",
    "                time_series_plot(target_distance,instant_speed,angles_in_degree,analysis_window)\n",
    "\n",
    "            follow_sercan=np.logical_and(target_distance[1:]<max_target_distance, instant_speed>1,angles_in_degree<10)\n",
    "            if extract_follow_epoches:\n",
    "                epochs_of_interest=follow_sercan\n",
    "            else:\n",
    "                epochs_of_interest=np.ones((instant_speed.shape[0]))==1.0#created a all-true array\n",
    "            \n",
    "\n",
    "            vector_dif_rotated=vector_dif_rotated[:,1:]\n",
    "            degree_in_the_trial=np.repeat(grp['mu'][0],np.transpose(grp['ts'].values).shape[0])\n",
    "            degree_time=np.vstack((degree_in_the_trial,grp['ts'].values))\n",
    "            degree_time=degree_time[:,1:]\n",
    "            follow_wrap=np.concat((vector_dif_rotated[:,epochs_of_interest],degree_time[:,epochs_of_interest]))\n",
    "            dif_across_trials.append(pd.DataFrame(np.transpose(follow_wrap)))\n",
    "            df_summary = pd.DataFrame(\n",
    "                {\n",
    "                    \"trial_id\": [trial_id],\n",
    "                    \"mu\": [grp['mu'][0]],\n",
    "                    \"polar_angle\": [grp['polar_angle'][0]],\n",
    "                    \"this_vr\": [grp['this_vr'][0]],\n",
    "                    \"num_follow_epochs\": [sum(follow_sercan)],\n",
    "                    \"number_frames\": [grp.shape[0]-1],\n",
    "                    \"travel_distance\": [np.nansum(focal_distance_fbf)],\n",
    "                    \"turning_distance\": [np.nansum(abs(turn_degree_fbf))],\n",
    "                    \"travel_distance_ISI\": [np.nansum(focal_distance_ISI)],\n",
    "                    \"turning_distance_ISI\": [np.nansum(abs(turn_degree_ISI))],\n",
    "                    \"duration\": [grp['duration'][0]],\n",
    "                    \"duration_ISI\": [pre_stim_ISI],\n",
    "                    \"temperature\": [grp['temperature'][0]],\n",
    "                    \"humidity\": [grp['humidity'][0]],\n",
    "                }\n",
    "            )\n",
    "            trial_evaluation_list.append(df_summary)\n",
    "            trial_id=trial_id+1\n",
    "    tmp=pd.concat(dif_across_trials)\n",
    "    if tmp.shape[1]==2:\n",
    "        tmp.columns = ['x', 'y']\n",
    "    elif tmp.shape[1]==4:\n",
    "        tmp.columns = ['x', 'y','degree','ts']\n",
    "        \n",
    "    relative_pos_all_animals.append(tmp)\n",
    "    trial_evaluation=pd.concat(trial_evaluation_list)\n",
    "    trial_evaluation.insert(0, 'animal_id',np.repeat(animal_id,trial_evaluation.shape[0]))\n",
    "    trial_evaluation_across_animals.append(trial_evaluation)\n",
    "    animal_id=animal_id+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_evaluation=pd.concat(trial_evaluation_across_animals)\n",
    "#fair_follower_threshold=0.1667\n",
    "#good_follower_threshold=0.3333\n",
    "fair_follower_threshold=0.05\n",
    "good_follower_threshold=0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1D histogram to show the frequency of following response\n",
    "fig, axes = plt.subplots(\n",
    "        nrows=2, ncols=2, figsize=(9,10), tight_layout=True\n",
    "    )\n",
    "ax1, ax2, ax3, ax4 = axes.flatten()\n",
    "ax1.hist(all_evaluation.groupby(['animal_id'])['num_follow_epochs'].sum()/all_evaluation.groupby(['animal_id'])['number_frames'].sum())\n",
    "ax1.set(xticks=[0,0.25,0.5,0.75,1],xticklabels=(['0', '25', '50', '75', '100']),xlim=(0,1),yticks=[0,20],ylim=(0,20),title='proportion of time following aba')\n",
    "ax2.hist(all_evaluation['num_follow_epochs']/all_evaluation['number_frames'])\n",
    "ax2.set(xticks=[0,0.25,0.3,0.4,0.5,0.75,1],xticklabels=(['0', '25','30','40', '50', '75', '100']),xlim=(0,1),yticks=[0,100,200],title='proportion of time following tbt')\n",
    "follower_of_interest=all_evaluation.groupby(['animal_id'])['num_follow_epochs'].sum()/all_evaluation.groupby(['animal_id'])['number_frames'].sum()<good_follower_threshold\n",
    "rows_of_follower=follower_of_interest.repeat(int(all_evaluation.shape[0]/follower_of_interest.shape[0]))\n",
    "ax3.hist(all_evaluation[rows_of_follower.values]['num_follow_epochs']/all_evaluation[rows_of_follower.values]['number_frames'])\n",
    "ax3.set(xticks=[0,0.25,0.5,0.75,1],xlim=(0,1),title='proportion of time from animals below threshold')\n",
    "follower_of_interest=all_evaluation.groupby(['animal_id'])['num_follow_epochs'].sum()/all_evaluation.groupby(['animal_id'])['number_frames'].sum()>good_follower_threshold\n",
    "rows_of_follower=follower_of_interest.repeat(int(all_evaluation.shape[0]/follower_of_interest.shape[0]))\n",
    "ax4.hist(all_evaluation[rows_of_follower.values]['num_follow_epochs']/all_evaluation[rows_of_follower.values]['number_frames'])\n",
    "ax4.set(xticks=[0,0.25,0.5,0.75,1],xlim=(0,1),title='proportion of time from animals below threshold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##plot the relationship between travel distance and proportion of following time\n",
    "fig, axes = plt.subplots(\n",
    "        nrows=2, ncols=2, figsize=(9,10), tight_layout=True\n",
    "    )\n",
    "ax1, ax2, ax3, ax4 = axes.flatten()\n",
    "ax1.set(xticks=[0,0.25,0.5,0.75,1],xticklabels=(['0', '25', '50', '75', '100']),xlim=(0,1))\n",
    "ax2.set(xticks=[0,0.25,0.5,0.75,1],xticklabels=(['0', '25', '50', '75', '100']),xlim=(0,1))\n",
    "for keys, this_data in all_evaluation.groupby(['animal_id']):\n",
    "    p_follow=this_data['num_follow_epochs'].sum()/this_data['number_frames'].sum()\n",
    "    if p_follow>good_follower_threshold:\n",
    "        color='r'\n",
    "    elif (p_follow>fair_follower_threshold) and (p_follow<good_follower_threshold):\n",
    "        color='b'\n",
    "    else:\n",
    "        color='k'\n",
    "    ax1.scatter(this_data['num_follow_epochs'].sum()/this_data['number_frames'].sum(),this_data['travel_distance'].sum(),c=color)\n",
    "    ax2.scatter(this_data['num_follow_epochs']/this_data['number_frames'],this_data['travel_distance'],c=color)\n",
    "    ax3.scatter(this_data['num_follow_epochs'].sum()/this_data['number_frames'].sum(),this_data['turning_distance'].sum(),c=color)\n",
    "    ax4.scatter(this_data['num_follow_epochs']/this_data['number_frames'],this_data['turning_distance'],c=color)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 2.1: Using 1D histogram to plot proportion of time and 2D histogram to plot relative position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trials=pd.concat(relative_pos_all_animals)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(all_trials[\"degree\"].unique()),tight_layout=True,sharex=True, sharey=True)\n",
    "\n",
    "if extract_follow_epoches:\n",
    "    xlimit=(-20,40)\n",
    "    ylimit=(-20,20)\n",
    "else:\n",
    "    xlimit=(-20,100)\n",
    "    ylimit=(-45,45)\n",
    "\n",
    "i=0\n",
    "for key, grp in all_trials.groupby('degree'):\n",
    "    print(f\"stimulus degree: {key}\")\n",
    "    axes[i].hist2d(grp['x'].values,grp['y'].values,bins=400)\n",
    "    axes[i].set(adjustable='box', aspect='equal')\n",
    "    axes[i].set(\n",
    "    yticks=[-40,0,40],\n",
    "    xticks=[-20,0,20,40],\n",
    "    xlim=xlimit,ylim=ylimit)\n",
    "    i=i+1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##plot potential follow response in 2D histogram\n",
    "xlimit=(-30,30)\n",
    "ylimit=(-30,30)\n",
    "all_trials=pd.concat(relative_pos_all_animals)\n",
    "fig, ax = plt.subplots(dpi=300, figsize=(2,2))\n",
    "ax.hist2d(all_trials['x'].values,all_trials['y'].values,bins=100)\n",
    "ax.set(adjustable='box', aspect='equal')\n",
    "ax.set(\n",
    "    yticks=[-30,0.0,30],\n",
    "    xticks=[-30,0.0,30],\n",
    "    xticklabels=(['-0.3', '0.0', '0.3']),\n",
    "    yticklabels=(['-0.3', '0.0', '0.3']),\n",
    "    xlim=xlimit,ylim=ylimit)\n",
    "# fig, ax = plt.subplots(dpi=300, figsize=(2,2))\n",
    "# sns.histplot(all_trials,x='x',y='y')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 2.2: save data into Mat file if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##save the raw data into mat file for matlab\n",
    "from scipy.io import savemat\n",
    "all_raw=pd.concat(relative_pos_all_animals)\n",
    "#all_raw['trial_id']=all_raw['trial_id'].astype(int)\n",
    "data_dict = {name: col.values for name, col in all_raw.items()}\n",
    "summary_file_name = Path(thisDataset) /\"time_series_curated.mat\"\n",
    "savemat(summary_file_name, data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.0, analysis walking behavours before and after the presence of stimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_data_type(all_trials):\n",
    "\n",
    "    all_trials['id'] = all_trials['id'].astype(int)\n",
    "    all_trials['mu'] = all_trials['mu'].astype(int)\n",
    "    all_trials['velocity'] = all_trials['velocity'].astype(float)\n",
    "    all_trials['omega'] = all_trials['omega'].astype(float)\n",
    "    all_trials['normalised_v'] = all_trials['normalised_v'].astype(float)\n",
    "    all_trials['normalised_omega'] = all_trials['normalised_omega'].astype(float)\n",
    "    if 'density' in all_trials.columns:\n",
    "        all_trials['density'] = all_trials['density'].astype(int)\n",
    "    return all_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajec_lim=150\n",
    "good_follower_only=False\n",
    "duration_for_baseline=3\n",
    "align_with_isi_onset=analysis_methods.get(\"align_with_isi_onset\",False)\n",
    "number_frame_scene_changing=12\n",
    "dif_across_animals=[]\n",
    "animal_id=0\n",
    "for this_dir,this_vr in zip(dir_list,vr_no):\n",
    "    if Path(this_dir).is_dir()==False:\n",
    "        continue\n",
    "    agent_pattern = f\"VR{this_vr}*agent_full.h5\"\n",
    "    found_result = find_file(Path(this_dir), agent_pattern)\n",
    "    if found_result is not None:\n",
    "        df_agent = pd.read_hdf(found_result)\n",
    "    xy_pattern = f\"VR{this_vr}*XY_full.h5\"\n",
    "    found_result = find_file(Path(this_dir), xy_pattern)\n",
    "    if found_result is None:\n",
    "        continue\n",
    "    df_XY = pd.read_hdf(found_result)\n",
    "    df_XY['this_vr']=this_vr\n",
    "    df_XY['fname']=df_XY['fname'].astype(str) + '_' + df_XY['this_vr'].astype(str)\n",
    "    #trajectory_analysis(df_XY)\n",
    "    dif_across_trials=[]\n",
    "    if 'basedline_v' in locals():\n",
    "        del basedline_v\n",
    "    trial_id=0\n",
    "    for key, grp in df_XY.groupby('fname'):\n",
    "        focal_xy=np.vstack((grp[\"X\"].to_numpy(),grp[\"Y\"].to_numpy()))\n",
    "        distance_from_centre=np.sqrt(np.sum([focal_xy[0]**2,focal_xy[1]**2],axis=0))\n",
    "        ts=grp[\"ts\"].to_numpy()\n",
    "        dif_x=np.diff(focal_xy[0])\n",
    "        dif_y=np.diff(focal_xy[1])\n",
    "        instant_speed=calculate_speed(dif_x,dif_y,ts)\n",
    "        angle_rad = grp[\"heading\"].to_numpy()\n",
    "        _,angular_speed=unwrap_degree(angle_rad,number_frame_scene_changing)\n",
    "\n",
    "        if 'type' in df_XY.columns:\n",
    "            if align_with_isi_onset:\n",
    "                if grp['type'][0]=='empty_trial':\n",
    "                    frame_range=analysis_window[1]*monitor_fps\n",
    "                    d_of_interest=distance_from_centre[:frame_range]\n",
    "                    v_of_interest=instant_speed[:frame_range]\n",
    "                    w_of_interest=angular_speed[:frame_range]\n",
    "                else:\n",
    "                    frame_range=analysis_window[0]*monitor_fps\n",
    "                    d_of_interest=distance_from_centre[frame_range:]\n",
    "                    v_of_interest=instant_speed[frame_range:]\n",
    "                    w_of_interest=angular_speed[frame_range:]\n",
    "            else:\n",
    "                if grp['type'][0]=='empty_trial':\n",
    "                    print('ISI now')\n",
    "                    frame_range=analysis_window[0]*monitor_fps\n",
    "                    d_of_interest=distance_from_centre[frame_range:]\n",
    "                    v_of_interest=instant_speed[frame_range:]\n",
    "                    w_of_interest=angular_speed[frame_range:]\n",
    "                    basedline_v=np.mean(v_of_interest[-duration_for_baseline*monitor_fps:])\n",
    "                    normalised_v=np.repeat(np.nan,v_of_interest.shape[0])\n",
    "                    basedline_w=np.mean(w_of_interest[-duration_for_baseline*monitor_fps:])\n",
    "                    normalised_w=np.repeat(np.nan,w_of_interest.shape[0])\n",
    "\n",
    "                else:\n",
    "                    print('stim now')\n",
    "                    frame_range=analysis_window[1]*monitor_fps\n",
    "                    d_of_interest=distance_from_centre[:frame_range]\n",
    "                    v_of_interest=instant_speed[:frame_range]\n",
    "                    w_of_interest=angular_speed[:frame_range]\n",
    "                    if 'basedline_v' in locals():\n",
    "                        normalised_v=v_of_interest/basedline_v\n",
    "                    else:\n",
    "                        normalised_v=np.repeat(np.nan,v_of_interest.shape[0])\n",
    "                    if 'basedline_w' in locals():\n",
    "                        normalised_w=w_of_interest/basedline_w\n",
    "                    else:\n",
    "                        normalised_w=np.repeat(np.nan,w_of_interest.shape[0])\n",
    "\n",
    "        else:\n",
    "            if align_with_isi_onset:\n",
    "                if grp['density'][0]==0.0:\n",
    "                    frame_range=analysis_window[1]*monitor_fps\n",
    "                    d_of_interest=distance_from_centre[:frame_range]\n",
    "                    v_of_interest=instant_speed[:frame_range]\n",
    "                    w_of_interest=angular_speed[:frame_range]\n",
    "                    if 'basedline_v' in locals():\n",
    "                        normalised_v=v_of_interest/basedline_v\n",
    "                    else:\n",
    "                        normalised_v=np.repeat(np.nan,v_of_interest.shape[0])\n",
    "                    if 'basedline_w' in locals():\n",
    "                        normalised_w=w_of_interest/basedline_w\n",
    "                    else:\n",
    "                        normalised_w=np.repeat(np.nan,w_of_interest.shape[0])\n",
    "                else:\n",
    "                    frame_range=analysis_window[0]*monitor_fps\n",
    "                    d_of_interest=distance_from_centre[frame_range:]\n",
    "                    v_of_interest=instant_speed[frame_range:]\n",
    "                    w_of_interest=angular_speed[frame_range:]\n",
    "                    basedline_v=np.mean(v_of_interest[-duration_for_baseline*monitor_fps:])\n",
    "                    normalised_v=np.repeat(np.nan,v_of_interest.shape[0])\n",
    "                    basedline_w=np.mean(w_of_interest[-duration_for_baseline*monitor_fps:])\n",
    "                    normalised_w=np.repeat(np.nan,w_of_interest.shape[0])\n",
    "\n",
    "            else:\n",
    "                if grp['density'][0]==0.0:\n",
    "                    print('ISI now')\n",
    "                    frame_range=analysis_window[0]*monitor_fps\n",
    "                    d_of_interest=distance_from_centre[frame_range:]\n",
    "                    v_of_interest=instant_speed[frame_range:]\n",
    "                    w_of_interest=angular_speed[frame_range:]\n",
    "\n",
    "                else:\n",
    "                    print('Stim now')\n",
    "                    frame_range=analysis_window[1]*monitor_fps\n",
    "                    d_of_interest=distance_from_centre[:frame_range]\n",
    "                    v_of_interest=instant_speed[:frame_range]\n",
    "                    w_of_interest=angular_speed[:frame_range]\n",
    "\n",
    "\n",
    "        if 'type' in df_XY.columns:\n",
    "            con_matrex=(d_of_interest,v_of_interest,w_of_interest,normalised_v,normalised_w,np.repeat(trial_id,v_of_interest.shape[0]),np.repeat(grp['mu'][0],v_of_interest.shape[0]),np.repeat(grp['type'][0],v_of_interest.shape[0]))\n",
    "        else:\n",
    "            con_matrex=(d_of_interest,v_of_interest,w_of_interest,normalised_v,normalised_w,np.repeat(trial_id,v_of_interest.shape[0]),np.repeat(grp['mu'][0],v_of_interest.shape[0]),np.repeat(grp['density'][0],v_of_interest.shape[0]))\n",
    "        raw_data=np.vstack(con_matrex)\n",
    "        dif_across_trials.append(pd.DataFrame(np.transpose(raw_data)))\n",
    "        trial_id += 1\n",
    "    tmp=pd.concat(dif_across_trials)\n",
    "    if 'type' in df_XY.columns:\n",
    "        tmp.columns = ['distance_from_centre', 'velocity','omega','normalised_v','normalised_omega','id','mu','object']\n",
    "    else:\n",
    "        tmp.columns = ['distance_from_centre', 'velocity','omega','normalised_v','normalised_omega','id','mu','density']\n",
    "    tmp.insert(0, 'animal_id', np.repeat(animal_id,tmp.shape[0]))\n",
    "    dif_across_animals.append(tmp)\n",
    "    animal_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(dif_across_animals)==list:\n",
    "    all_trials=pd.concat(dif_across_animals)\n",
    "    all_trials=fix_data_type(all_trials)\n",
    "else:\n",
    "    all_trials=fix_data_type(dif_across_animals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_baseline_distribution(all_trials,analysis_methods,metrics_name='velocity',duration_for_baseline=3):\n",
    "    align_with_isi_onset=analysis_methods.get(\"align_with_isi_onset\",False)\n",
    "    these_baselines=[]\n",
    "    for keys, this_data in all_trials.groupby(['animal_id','id']):\n",
    "        #print(this_data['object'][1])\n",
    "        if align_with_isi_onset:\n",
    "            if this_data['object'][1]!='empty_trial':\n",
    "            #if keys[1]%2==0:#for the Swarm scene to only use stim trials to get the baseline\n",
    "                this_speed=this_data[metrics_name].values\n",
    "                #print(this_speed)\n",
    "                these_baselines.append(np.mean(this_speed[-duration_for_baseline*monitor_fps:]))\n",
    "        \n",
    "        else:\n",
    "            if this_data['object'][1]=='empty_trial':\n",
    "            #if keys[1]%2!=0:#for the Swarm scene to only use ISI trials\n",
    "                this_speed=this_data[metrics_name].values\n",
    "                these_baselines.append(np.mean(this_speed[-duration_for_baseline*monitor_fps:]))\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=1, ncols=1, figsize=(9,9), tight_layout=True\n",
    "    )\n",
    "    axes.hist(np.vstack(these_baselines),bins=500)\n",
    "    if metrics_name=='velocity':\n",
    "        axes.set(xlim=(0,2),ylim=(0, 60))\n",
    "    else:\n",
    "        #axes.set(xlim=(-0.001,0.001),ylim=(0, 60))#if used rad\n",
    "        axes.set(xlim=(-0.03,0.03),ylim=(0, 100))\n",
    "\n",
    "    plt.minorticks_on()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_name='omega'\n",
    "check_baseline_distribution(all_trials,analysis_methods,metrics_name)\n",
    "#check_baseline_distribution(all_trials,analysis_methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_trials(analysis_methods,all_trials,metrics_name='velocity',metrics_name2='normalised_v',walk_threshold=0.25,duration_for_baseline=3):\n",
    "    monitor_fps=analysis_methods.get(\"monitor_fps\")\n",
    "    walk_trials_boo=[]\n",
    "    these_metrics=[]\n",
    "    these_normalised_metrics=[]\n",
    "    for keys, this_data in all_trials.groupby(['animal_id','id']):\n",
    "        this_speed=this_data[metrics_name].values\n",
    "        baseline_speed=np.mean(this_speed[-duration_for_baseline*monitor_fps:])\n",
    "        walk_trials_boo.append(abs(baseline_speed)>walk_threshold)\n",
    "        these_metrics.append(this_speed)\n",
    "        these_normalised_metrics.append(this_data[metrics_name2].values)\n",
    "    return walk_trials_boo,these_metrics,these_normalised_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_name='omega'\n",
    "Is_move_trials,these_metrics,these_normalised_metrics=split_trials(analysis_methods,all_trials,metrics_name,'normalised_omega',0.008)#0.008 degree or 0.0002 rad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Is_move_trials,these_metrics,these_normalised_metrics=split_trials(analysis_methods,all_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_trial_index(walk_trials_boo,analysis_methods):\n",
    "    align_with_isi_onset=analysis_methods.get(\"align_with_isi_onset\",False)\n",
    "#int((trial_id)/2) means the number of stimulus trial\n",
    "    after_walk_ith_trial=[]\n",
    "    after_stationary_ith_trial=[]\n",
    "    if align_with_isi_onset:\n",
    "        after_walk_ith_trial=[i+1 for i, x in enumerate(walk_trials_boo[1::2]) if x and i % int((trial_id)/2) != (trial_id)/2-1]\n",
    "        after_stationary_ith_trial=[i+1 for i, x in enumerate(walk_trials_boo[1::2]) if x==False and i % int((trial_id)/2) != (trial_id)/2-1]\n",
    "    else:\n",
    "        after_walk_ith_trial=[i for i, x in enumerate(walk_trials_boo[::2]) if x]\n",
    "        after_stationary_ith_trial=[i for i, x in enumerate(walk_trials_boo[::2]) if x==False]\n",
    "    return after_walk_ith_trial,after_stationary_ith_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_move_ith_trial,after_stationary_ith_trial=extract_trial_index(Is_move_trials,analysis_methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fill_between_range(data,mean_data,using_confidence_interval=True):\n",
    "\n",
    "    ##to plot distribution with 95% confidence interval with normal distribution\n",
    "    if using_confidence_interval:\n",
    "        confidence_level = 0.95\n",
    "        cl95=st.norm.interval(confidence_level,loc=mean_data,scale=st.sem(data))\n",
    "        dif_y1=cl95[0][:]\n",
    "        dif_y2=cl95[1][:]\n",
    "    else:\n",
    "        sem_response = np.std(data, axis=0, ddof=1) / np.sqrt(data.shape[0])\n",
    "        dif_y1=mean_data + sem_response\n",
    "        dif_y2=mean_data - sem_response\n",
    "    return dif_y1,dif_y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visual_evoked_behaviour(these_metrics,these_normalised_metrics,after_move_ith_trial,after_stationary_ith_trial,analysis_methods,metrics_name='velocity',row_of_interest=None):\n",
    "    number_frame_scene_changing=12\n",
    "    align_with_isi_onset=analysis_methods.get(\"align_with_isi_onset\",False)\n",
    "    analysis_window=analysis_methods.get(\"analysis_window\")\n",
    "    all_animals=False\n",
    "    tmp=np.vstack(these_metrics)\n",
    "    tmp3=np.vstack(these_normalised_metrics)\n",
    "    if align_with_isi_onset:\n",
    "       stim_evoked_metrics=tmp[::2]\n",
    "       stim_evoked_norm_metrics=tmp3[::2]\n",
    "    else:\n",
    "       stim_evoked_metrics=tmp[1::2]\n",
    "       stim_evoked_norm_metrics=tmp3[1::2]\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "            nrows=4, ncols=2, figsize=(9,10), tight_layout=True\n",
    "        )\n",
    "    ax1, ax2, ax3, ax4,ax5,ax6,ax7,ax8 = axes.flatten()\n",
    "    plt.rcParams.update(plt.rcParamsDefault)\n",
    "    plt.rcParams.update({'font.size': 8})\n",
    "    # Set the axis line width to 2\n",
    "    plt.rcParams['ytick.major.width'] = 1\n",
    "    plt.rcParams['xtick.major.width'] = 1\n",
    "    plt.rcParams['axes.linewidth'] = 1\n",
    "    cmap = plt.get_cmap('viridis')\n",
    "    if all_animals==False and type(row_of_interest)==pd.Series:\n",
    "        animal_interest_stationary=[]\n",
    "        animal_interest_move=[]\n",
    "        for i in np.where(row_of_interest)[0].tolist():\n",
    "            if i in after_stationary_ith_trial:\n",
    "                animal_interest_stationary.append(i)\n",
    "            else:\n",
    "                animal_interest_move.append(i)\n",
    "        p3=stim_evoked_metrics[animal_interest_stationary,:]\n",
    "        p4=stim_evoked_metrics[animal_interest_move,:]\n",
    "        p5=stim_evoked_norm_metrics[animal_interest_stationary,:]\n",
    "        p6=stim_evoked_norm_metrics[animal_interest_move,:]\n",
    "\n",
    "    else:\n",
    "        p3=stim_evoked_metrics[after_stationary_ith_trial,:]\n",
    "        p4=stim_evoked_metrics[after_move_ith_trial,:]\n",
    "        p5=stim_evoked_norm_metrics[after_stationary_ith_trial,:]\n",
    "        p6=stim_evoked_norm_metrics[after_move_ith_trial,:]\n",
    "    x=np.arange(0,p3.shape[1])\n",
    "    p1=np.nancumsum(p3, axis=1)\n",
    "    p2=np.nancumsum(p4, axis=1)\n",
    "    ax1.plot(np.transpose(p1),linewidth=0.1)\n",
    "    mean_p1=np.nanmean(p1,axis=0)\n",
    "    ax1.plot(mean_p1,'k',linewidth=1)\n",
    "    #ax1.plot(np.nanmedian(p1,axis=0),'k--',linewidth=0.5)\n",
    "    dif_y1,dif_y2=get_fill_between_range(p1,mean_p1)\n",
    "    ax1.fill_between(x,dif_y1,dif_y2, alpha=0.4,color='k')\n",
    "    ax2.plot(np.transpose(p2),linewidth=0.1)\n",
    "    mean_p2=np.nanmean(p2,axis=0)\n",
    "\n",
    "    ax2.plot(mean_p2,'k',linewidth=1)\n",
    "    #ax2.plot(np.nanmedian(p2,axis=0),'k--',linewidth=0.5)\n",
    "    dif_y1,dif_y2=get_fill_between_range(p2,mean_p2)\n",
    "    ax2.fill_between(x,dif_y1,dif_y2, alpha=0.4,color='k')\n",
    "    ax3.plot(np.transpose(p3),linewidth=0.1)\n",
    "    mean_p3=np.nanmean(p3,axis=0)\n",
    "    ax3.plot(mean_p3,'k',linewidth=1)\n",
    "    dif_y1,dif_y2=get_fill_between_range(p3,mean_p3)\n",
    "    ax3.fill_between(x,dif_y1,dif_y2, alpha=0.4,color='k')\n",
    "    #ax3.plot(np.nanmedian(p3,axis=0),'k--',linewidth=0.5)\n",
    "    mean_p4=np.nanmean(p4,axis=0)\n",
    "    dif_y1,dif_y2=get_fill_between_range(p4,mean_p4)\n",
    "    ax4.plot(np.transpose(p4),linewidth=0.1)\n",
    "    ax4.plot(mean_p4,'k',linewidth=1)\n",
    "    ax4.fill_between(x,dif_y1,dif_y2, alpha=0.4,color='k')\n",
    "    #ax4.plot(np.nanmedian(p4,axis=0),'k--',linewidth=0.5)\n",
    "    mean_p5=np.nanmean(p5,axis=0)\n",
    "    dif_y1,dif_y2=get_fill_between_range(p5,mean_p5)\n",
    "    ax5.plot(np.transpose(p5),linewidth=0.1)\n",
    "    ax5.plot(mean_p5,'k',linewidth=1)\n",
    "    ax5.fill_between(x,dif_y1,dif_y2, alpha=0.4,color='k')\n",
    "    #ax5.plot(np.nanmedian(p5,axis=0),'k--',linewidth=0.5)\n",
    "    mean_p6=np.nanmean(p6,axis=0)\n",
    "    dif_y1,dif_y2=get_fill_between_range(p6,mean_p6)\n",
    "    ax6.plot(np.transpose(p6),linewidth=0.1)\n",
    "    ax6.plot(mean_p6,'k',linewidth=1)\n",
    "    ax6.fill_between(x,dif_y1,dif_y2,alpha=0.4,color='k')\n",
    "\n",
    "    #ax6.plot(np.nanmedian(p6,axis=0),'k--',linewidth=0.5)\n",
    "    if metrics_name=='velocity' and all_animals==False:\n",
    "        ylimit=10\n",
    "        ylimit_log=100\n",
    "        ax1.set_ylim([0,400*ylimit])\n",
    "        ax2.set_ylim([0,400*ylimit])\n",
    "        ax3.set_ylim([0,1*ylimit])\n",
    "        ax4.set_ylim([0,1*ylimit])\n",
    "    elif all_animals==False:\n",
    "        ylimit=2.5\n",
    "        ylimit_log=1000\n",
    "        # ax1.yaxis.set_major_locator(plt.MultipleLocator(np.pi / 2))\n",
    "        # ax1.yaxis.set_major_formatter(plt.FuncFormatter(multiple_formatter()))\n",
    "        # ax2.yaxis.set_major_locator(plt.MultipleLocator(np.pi / 2))\n",
    "        # ax2.yaxis.set_major_formatter(plt.FuncFormatter(multiple_formatter()))\n",
    "        # ax3.yaxis.set_major_locator(plt.MultipleLocator(np.pi / 2))\n",
    "        # ax3.yaxis.set_major_formatter(plt.FuncFormatter(multiple_formatter()))\n",
    "        # ax4.yaxis.set_major_locator(plt.MultipleLocator(np.pi / 2))\n",
    "        # ax4.yaxis.set_major_formatter(plt.FuncFormatter(multiple_formatter()))\n",
    "        ax1.set_ylim([-30*ylimit,30*ylimit])\n",
    "        ax2.set_ylim([-30*ylimit,30*ylimit])\n",
    "        ax3.set_ylim([-1*ylimit,1*ylimit])\n",
    "        ax4.set_ylim([-1*ylimit,1*ylimit])\n",
    "    elif metrics_name=='velocity':\n",
    "        ylimit=10\n",
    "        ylimit_log=100\n",
    "        ax1.set_ylim([0,400*ylimit])\n",
    "        ax2.set_ylim([0,400*ylimit])\n",
    "        ax3.set_ylim([0,1*ylimit])\n",
    "        ax4.set_ylim([0,1*ylimit])\n",
    "    else:\n",
    "        ylimit=1\n",
    "        ylimit_log=1000\n",
    "        # ax1.yaxis.set_major_locator(plt.MultipleLocator(np.pi / 2))\n",
    "        # ax1.yaxis.set_major_formatter(plt.FuncFormatter(multiple_formatter()))\n",
    "        # ax2.yaxis.set_major_locator(plt.MultipleLocator(np.pi / 2))\n",
    "        # ax2.yaxis.set_major_formatter(plt.FuncFormatter(multiple_formatter()))\n",
    "        # ax3.yaxis.set_major_locator(plt.MultipleLocator(np.pi / 2))\n",
    "        # ax3.yaxis.set_major_formatter(plt.FuncFormatter(multiple_formatter()))\n",
    "        # ax4.yaxis.set_major_locator(plt.MultipleLocator(np.pi / 2))\n",
    "        # ax4.yaxis.set_major_formatter(plt.FuncFormatter(multiple_formatter()))\n",
    "        ax1.set_ylim([-30*ylimit,30*ylimit])\n",
    "        ax2.set_ylim([-30*ylimit,30*ylimit])\n",
    "        ax3.set_ylim([-1*ylimit,1*ylimit])\n",
    "        ax4.set_ylim([-1*ylimit,1*ylimit])\n",
    "    ax1.set(\n",
    "        ylabel=f\"sum of {metrics_name}\",\n",
    "        #xlabel=\"frame\",\n",
    "        xlabel=\"Time (s)\",\n",
    "        xticks=[0,analysis_window[1]*monitor_fps],\n",
    "        xticklabels=(['0', str(analysis_window[1])]),\n",
    "    )\n",
    "    ax2.set(\n",
    "        ylabel=f\"sum of {metrics_name}\",\n",
    "        #xlabel=\"frame\",\n",
    "        xlabel=\"Time (s)\",\n",
    "        xticks=[0,analysis_window[1]*monitor_fps],\n",
    "        xticklabels=(['0', str(analysis_window[1])]),\n",
    "    )\n",
    "    ax3.set(\n",
    "        ylabel=metrics_name,\n",
    "        #xlabel=\"frame\",\n",
    "        xlabel=\"Time (s)\",\n",
    "        xticks=[0,analysis_window[1]*monitor_fps],\n",
    "        xticklabels=(['0', str(analysis_window[1])]),\n",
    "    )\n",
    "    ax4.set(\n",
    "        ylabel=metrics_name,\n",
    "        #xlabel=\"frame\",\n",
    "        xlabel=\"Time (s)\",\n",
    "        xticks=[0,analysis_window[1]*monitor_fps],\n",
    "        xticklabels=(['0', str(analysis_window[1])]),\n",
    "    )\n",
    "    ax5.set(\n",
    "        ylabel=\"normalised values (ratio)\",\n",
    "        xlabel=\"frame\",\n",
    "        ylim=[1/ylimit_log,ylimit_log],\n",
    "    )\n",
    "    ax6.set(\n",
    "        ylabel=\"normalised values (ratio)\",\n",
    "        xlabel=\"frame\",\n",
    "        ylim=[1/ylimit_log,ylimit_log],\n",
    "    )\n",
    "    # if metrics_name=='velocity':\n",
    "    #     peak_distribution_stationary=np.argmax(p3, axis=1)\n",
    "    #     peak_distribution_move=np.argmax(p4, axis=1)\n",
    "    # else:\n",
    "    peak_distribution_stationary=np.argmax(abs(p3[:,number_frame_scene_changing:]), axis=1)\n",
    "    peak_distribution_stationary=peak_distribution_stationary+number_frame_scene_changing\n",
    "    #print(peak_distribution_stationary.shape)\n",
    "    peak_distribution_move=np.argmax(abs(p4[:,number_frame_scene_changing:]), axis=1)\n",
    "    peak_distribution_move=peak_distribution_move+number_frame_scene_changing\n",
    "    #print(peak_distribution_move.shape)\n",
    "    ax5.set_yscale('log')\n",
    "    ax6.set_yscale('log')\n",
    "    ax7.hist(abs(peak_distribution_stationary))\n",
    "    ax7.set(\n",
    "        ylabel=\"Count\",\n",
    "        xlabel=\"Time (s)\",\n",
    "        xticks=[0,analysis_window[1]*monitor_fps],\n",
    "        xticklabels=(['0', str(analysis_window[1])]),\n",
    "    )\n",
    "    ax8.hist(abs(peak_distribution_move))\n",
    "    ax8.set(\n",
    "        ylabel=\"Count\",\n",
    "        xlabel=\"Time (s)\",\n",
    "        xticks=[0,analysis_window[1]*monitor_fps],\n",
    "        xticklabels=(['0', str(analysis_window[1])]),\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##criteria to sort out data from good followers\n",
    "use_follower_threshold=True\n",
    "if use_follower_threshold==True:\n",
    "    p_follow=all_evaluation.groupby(['animal_id'])['num_follow_epochs'].sum()/all_evaluation.groupby(['animal_id'])['number_frames'].sum()\n",
    "    #follower_of_interest=p_follow>good_follower_threshold\n",
    "    follower_of_interest=(p_follow>fair_follower_threshold) & (p_follow<good_follower_threshold)\n",
    "    rows_of_follower=follower_of_interest.repeat(int(all_evaluation.shape[0]/follower_of_interest.shape[0]))\n",
    "else:\n",
    "    good_trial_threshold=0.5\n",
    "    rows_of_follower=all_evaluation['num_follow_epochs']/all_evaluation['number_frames']>good_trial_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for this_condition in all_evaluation['mu'].unique():\n",
    "    print(this_condition)\n",
    "    row_of_interest=(all_evaluation['mu'].reset_index(drop=True)==this_condition) &(rows_of_follower.reset_index(drop=True))\n",
    "    plot_visual_evoked_behaviour(these_metrics,these_normalised_metrics,after_move_ith_trial,after_stationary_ith_trial,analysis_methods,metrics_name,row_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_name='velocity'\n",
    "for this_condition in all_evaluation['mu'].unique():\n",
    "    print(this_condition)\n",
    "    row_of_interest=(all_evaluation['mu'].reset_index(drop=True)==this_condition) &(rows_of_follower.reset_index(drop=True))\n",
    "    plot_visual_evoked_behaviour(these_metrics,these_normalised_metrics,after_move_ith_trial,after_stationary_ith_trial,analysis_methods,metrics_name,row_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_of_interest=rows_of_follower.reset_index(drop=True)\n",
    "plot_visual_evoked_behaviour(these_metrics,these_normalised_metrics,after_move_ith_trial,after_stationary_ith_trial,analysis_methods,'velocity',row_of_interest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matrexvr_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
