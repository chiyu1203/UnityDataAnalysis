{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session 0.0: introduce libraries, database path and main independent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a useful function to generate a data list for further analysis\n",
    "import os,json,sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from sorting_time_series_analysis import follow_behaviour_analysis,generate_points_within_rectangles,calculate_speed,diff_angular_degree,sort_raster_fictrac,plot_velocity_vector_field\n",
    "from plotting_follow_analysis import *\n",
    "from time_series_analysis import *\n",
    "from scipy.stats import circmean\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal import medfilt\n",
    "import seaborn as sns\n",
    "##need to add this additional cell because useful tools are in another folder. Need to integrate these two folders one day\n",
    "current_working_directory = Path.cwd()\n",
    "parent_dir = current_working_directory.resolve().parents[0]\n",
    "sys.path.insert(0, str(parent_dir) + \"\\\\utilities\")\n",
    "from useful_tools import select_animals_gpt,find_file,column_name_list,get_fill_between_range,read_seq_config\n",
    "from data_cleaning import findLongestConseqSubseq,interp_fill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Session 0.2: Load analysis methods in python dictionary form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = \"./analysis_methods_dictionary.json\"\n",
    "with open(json_file, \"r\") as f:\n",
    "    analysis_methods = json.loads(f.read())\n",
    "    \n",
    "#Put the folder of your Unity folder below\n",
    "#thisDataset =\"D:/MatrexVR_Swarm_Data/RunData\"\n",
    "#thisDataset =\"D:/MatrexVR_blackbackground_Data/RunData\"\n",
    "#thisDataset =\"D:/MatrexVR_grass1_Data/RunData\"\n",
    "thisDataset =\"D:/MatrexVR_2024_Data/RunData\"\n",
    "#thisDataset =\"D:/MatrexVR_2024_3_Data/RunData\"\n",
    "#thisDataset =r\"Z:\\DATA\\experiment_trackball_Optomotor\\locustVR\"\n",
    "#thisDataset =r\"C:\\Users\\neuroLaptop\\Documents\\MatrexVR_grass1_Data\\RunData\"\n",
    "#parameter name means independent variable in the experiment\n",
    "variable_name='mu'\n",
    "#variable_name='location'\n",
    "#variable_name='initial_position'\n",
    "#variable_name='agent_speed'\n",
    "#check trace in trial 115 from VR1_2024-11-16_155242_score_full, maybe there is a jump\n",
    "exp_name=analysis_methods.get(\"experiment_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Session 1.0: select animals based on condition and return which a directory list and a list of vr rig number to specify which animal to analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your Excel file\n",
    "dir_list = []\n",
    "file_type=\".h5\"\n",
    "using_google_sheet=True\n",
    "exp_name=analysis_methods.get(\"experiment_name\")\n",
    "if exp_name=='locustvr':\n",
    "    sheet_name = 'LocustVR'\n",
    "else:\n",
    "    sheet_name = \"Unity_MatrexVR\"\n",
    "\n",
    "# if type(thisDataset) == str:\n",
    "#     thisDataset = Path(thisDataset)\n",
    "if analysis_methods.get(\"load_individual_data\") == True:\n",
    "    if using_google_sheet==True:\n",
    "        # database_id = \"1UL4eEUrQMapx9xz11-IyOSlPBcep3I9vBJ2uGgVudb8\"\n",
    "        #         #https://docs.google.com/spreadsheets/d/1UL4eEUrQMapx9xz11-IyOSlPBcep3I9vBJ2uGgVudb8/edit?usp=sharing\n",
    "        # url = f\"https://docs.google.com/spreadsheets/d/{database_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
    "        database_id = \"1UL4eEUrQMapx9xz11-IyOSlPBcep1I9vBJ2uGgVudb8\"\n",
    "                #https://docs.google.com/spreadsheets/d/1UL4eEUrQMapx9xz11-IyOSlPBcep1I9vBJ2uGgVudb8/edit?usp=sharing\n",
    "        url = f\"https://docs.google.com/spreadsheets/d/{database_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
    "        #df = pd.read_excel(url, engine='openpyxl')## use this function if the file is not google sheet but uploaded excel file\n",
    "        df = pd.read_csv(url)\n",
    "    else:\n",
    "        excel_file_path = \"Z:/DATA/experiment_trackball_Optomotor/Locusts Management.xlsx\"\n",
    "        print(f\"using a database {excel_file_path} from the server but this file might be outdated\")\n",
    "        # Create a 'with' statement to open and read the Excel file\n",
    "        with pd.ExcelFile(excel_file_path) as xls:\n",
    "            # Read the Excel sheet into a DataFrame with the sheet name (folder name)\n",
    "            df = pd.read_excel(xls, sheet_name)\n",
    "        ##list up the conditions and answers as strings for input argument to select animal. One condition must pair with one answer\n",
    "    if analysis_methods.get(\"select_animals_by_condition\") == True:\n",
    "       #animal_of_interest=select_animals_gpt(df,\"Independent variable (list up all of them in the experiment)\",\"gregarious_leader_grass\",\"Excluding this animal from analysis (Usually when animals die or molt, T/F)\",\"F\")\n",
    "        #animal_of_interest=select_animals_gpt(df,\"Independent variable (list up all of them in the experiment)\",\"gregarious_leader_black\",\"Excluding this animal from analysis (Usually when animals die or molt, T/F)\",\"F\")\n",
    "        #animal_of_interest=select_animals_gpt(df,\"Independent variable1\",variable_name,\"Excluding this animal from analysis (Usually when animals die or molt, T/F)\",\"F\")\n",
    "        #animal_of_interest=select_animals_gpt(df,\"Independent variable1\",variable_name,\"Independent variable2\",\"translational_rotational_gain\",\"Excluding this animal from analysis (Usually when animals die or molt, T/F)\",\"F\")\n",
    "        #animal_of_interest=select_animals_gpt(df,\"Excluding this animal from analysis (Usually when animals die or molt, T/F)\",\"F\")\n",
    "        #animal_of_interest=select_animals_gpt(df,\"Independent variable1\",variable_name,\"Independent variable2\",\"bifuration_constant_speed_constant_distance\",\"Excluding this animal from analysis (Usually when animals die or molt, T/F)\",\"F\")\n",
    "        #animal_of_interest=select_animals_gpt(df,\"Independent variable1\",variable_name,\"Independent variable2\",\"choice_vr_locust_black_locust\",\"Excluding this animal from analysis (Usually when animals die or molt, T/F)\",\"F\")\n",
    "        #animal_of_interest=select_animals_gpt(df,\"Independent variable1\",variable_name,\"Independent variable2\",\"choice_vr_locust_sta_black_locust\",\"Background\",\"gray\",\"Excluding this animal from analysis (Usually when animals die or molt, T/F)\",\"F\")\n",
    "        animal_of_interest=select_animals_gpt(df,\"Independent variable1\",variable_name,\"Independent variable2\",\"choice_vr_locust_sta_black_locust\",\"Background\",\"grass1\",\"Excluding this animal from analysis (Usually when animals die or molt, T/F)\",\"F\")\n",
    "        #animal_of_interest=select_animals_gpt(df,\"Independent variable1\",variable_name,\"Independent variable2\",\"closed_loop_sta_black_locust_open_loop_sta_black_locust\",\"Excluding this animal from analysis (Usually when animals die or molt, T/F)\",\"F\")\n",
    "        #animal_of_interest=select_animals_gpt(df,\"Independent variable1\",variable_name,\"Independent variable2\",\"sta_black_locust_2dir_3_initial_position\",\"Excluding this animal from analysis (Usually when animals die or molt, T/F)\",\"F\")\n",
    "    else:\n",
    "        animal_of_interest=df\n",
    "\n",
    "    if exp_name=='locustvr':\n",
    "        ID_array=animal_of_interest[\"ID\"].values\n",
    "        print(f\"these are the ID of the animals {ID_array}\")\n",
    "        dir_list = [\n",
    "        root.replace(\"\\\\\", \"/\")\n",
    "        for root, _, files in os.walk(thisDataset)\n",
    "        if any(ID in root for ID in ID_array)\n",
    "        and any(file.endswith(file_type) for file in files)]\n",
    "    else:\n",
    "        folder_name=animal_of_interest[\"folder name\"].values\n",
    "        dir_tile=np.tile(thisDataset, (len(folder_name), 1))\n",
    "        vr_no=animal_of_interest[\"VR number\"].values\n",
    "        vr_no = vr_no.astype('int')\n",
    "        no_food=animal_of_interest[\"Food retriction (-1 or the number of hours)\"].values\n",
    "        no_food = no_food.astype('int')\n",
    "        dir_list = [''.join([x[0], '/', y]) for x,y in zip(dir_tile,folder_name)]\n",
    "        #dir_dict = itertools.zip_longest(dir_list, vr_no.tolist())\n",
    "        dir_dict = zip(dir_list, vr_no.tolist())\n",
    "else:\n",
    "    for root, dirs, files in os.walk(thisDataset):\n",
    "        for folder in dirs:\n",
    "            folder_path=os.path.join(root,folder)\n",
    "            if any(name.endswith(file_type) for name in os.listdir(folder_path)):\n",
    "                dir_list.append(folder_path.replace(\"\\\\\", \"/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list=dir_list[23:]\n",
    "vr_no=vr_no[23:]\n",
    "no_food=no_food[23:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list=dir_list[32:]\n",
    "vr_no=vr_no[32:]\n",
    "no_food=no_food[32:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list=dir_list[:32]\n",
    "vr_no=vr_no[:32]\n",
    "no_food=no_food[:32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.0: Pool animal's response together according to some criteria (from none criteria to criteria that can define the follow behaviour)\n",
    "Output1: a list 'follow_proportion_across_animals' showing the proportion of 'follow' time for each animal (across trials)\n",
    "\n",
    "Output2: a list 'relative_pos_all_animals' showing relative position between virtual and focal locusts across time. 1st and 2nd columns shown relative x and y, 3rd columns shown virtual animal's moving direction. 4th column shown the timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_methods.update({\"analysis_window\":[-10,10]})\n",
    "#analysis_methods.update({\"follow_within_distance\": 50})\n",
    "#analysis_methods.update({\"follow_above_speed\": 0})\n",
    "#analysis_methods.update({\"follow_within_angle\": 10})\n",
    "analysis_methods.update({\"follow_locustVR_criteria\": True})\n",
    "analysis_methods.update({\"plotting_trajectory\": False})\n",
    "analysis_methods.update({\"plotting_event_distribution\": False})\n",
    "analysis_methods.update({\"save_output\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_pos_all_animals=[]\n",
    "simulated_relative_pos_all_animals=[]\n",
    "trial_evaluation_across_animals=[]\n",
    "raster_across_animals_unity=[]\n",
    "raster_across_animals_fictrac=[]\n",
    "seq_config_all_animals=[]\n",
    "animal_id=0\n",
    "read_fictrac_data_only=analysis_methods.get(\"read_fictrac_data_only\")\n",
    "time_series_analysis = analysis_methods.get(\"time_series_analysis\")\n",
    "file_suffix = \"_full\" if time_series_analysis else \"\"\n",
    "for this_dir,this_vr,this_no_food in zip(dir_list,vr_no,no_food):\n",
    "    if Path(this_dir).is_dir()==False:\n",
    "        continue\n",
    "    if read_fictrac_data_only==False:\n",
    "        agent_pattern = f\"VR{this_vr}*agent{file_suffix}.h5\"\n",
    "        xy_pattern = f\"VR{this_vr}*XY{file_suffix}.h5\"\n",
    "        summary_pattern = f\"VR{this_vr}*score{file_suffix}.h5\"\n",
    "        agent_file = find_file(Path(this_dir), agent_pattern)\n",
    "        focal_animal_file = find_file(Path(this_dir), xy_pattern)\n",
    "        summary_file = find_file(Path(this_dir), summary_pattern)\n",
    "        relative_pos,trial_evaluation_list,raster_unity,num_unfilled_gap,simulated_relative_pos=follow_behaviour_analysis(summary_file,focal_animal_file,agent_file,analysis_methods)\n",
    "        if animal_id==0:\n",
    "            largest_unfilled_gap=num_unfilled_gap\n",
    "        elif num_unfilled_gap>largest_unfilled_gap:\n",
    "            largest_unfilled_gap=num_unfilled_gap\n",
    "        else:\n",
    "            pass\n",
    "        relative_pos_all_animals.append(relative_pos)\n",
    "        if len(simulated_relative_pos)>0:\n",
    "            simulated_relative_pos_all_animals.append(simulated_relative_pos)\n",
    "        trial_evaluation=pd.concat(trial_evaluation_list)\n",
    "        trial_evaluation.insert(0, 'VR',np.repeat(this_vr,trial_evaluation.shape[0]))\n",
    "        trial_evaluation.insert(0, 'no_food_hour',np.repeat(this_no_food,trial_evaluation.shape[0]))\n",
    "        trial_evaluation.insert(0, 'animal_id',np.repeat(animal_id,trial_evaluation.shape[0]))\n",
    "        trial_evaluation_across_animals.append(trial_evaluation)\n",
    "        raster_unity.insert(0, 'animal_id', np.repeat(animal_id,raster_unity.shape[0]))\n",
    "        raster_across_animals_unity.append(raster_unity)\n",
    "    seq_config_pattern=f\"*sequenceConfig.json\"\n",
    "    seq_config_file=find_file(Path(this_dir), seq_config_pattern)\n",
    "    seq_config_pd=read_seq_config(seq_config_file)\n",
    "    seq_config_pd.insert(0, 'step_id',np.arange(seq_config_pd.shape[0]))\n",
    "    seq_config_pd.insert(0, 'animal_id',np.repeat(animal_id,seq_config_pd.shape[0]))\n",
    "    seq_config_all_animals.append(seq_config_pd)\n",
    "    pa_pattern=f\"VR{this_vr}*motion{file_suffix}.parquet\"\n",
    "    pa_file=find_file(Path(this_dir), pa_pattern)\n",
    "    raster=pd.read_parquet(pa_file, engine='pyarrow')\n",
    "    raster.insert(0, 'animal_id', np.repeat(animal_id,raster.shape[0]))\n",
    "    raster_across_animals_fictrac.append(raster)\n",
    "    animal_id=animal_id+1\n",
    "if \"largest_unfilled_gap\" in locals():\n",
    "    analysis_methods['largest_unfilled_gap']= largest_unfilled_gap\n",
    "elif len(raster_across_animals_fictrac)==0:\n",
    "    print(\"'largest_unfilled_gap' is not defined and fictrac files are collected. Probably because the wrong folder of database is selected or portable USB is not inserted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 2.1: convert the trial by trial analysis across animals into a panda dataframe and calculate some additional information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_evaluation=pd.concat(trial_evaluation_across_animals)\n",
    "follow_time_tbt=all_evaluation['num_follow_epochs']/all_evaluation['number_frames']\n",
    "all_evaluation[\"follow_ratio_previous_trial\"]=pd.concat([pd.Series(np.nan),follow_time_tbt[:-1]],ignore_index=True).to_list()\n",
    "all_evaluation[\"follow_ratio_next_trial\"]=pd.concat([follow_time_tbt[1:],pd.Series(np.nan)],ignore_index=True).to_list()\n",
    "all_evaluation[\"follow_ratio_this_trial\"]=follow_time_tbt\n",
    "all_evaluation.reset_index(drop=True, inplace=True)\n",
    "all_evaluation.loc[all_evaluation.index[all_evaluation['trial_id']==0].tolist(),'follow_ratio_previous_trial']=np.nan\n",
    "all_evaluation.loc[all_evaluation.index[all_evaluation['trial_id']==all_evaluation['trial_id'].max()].tolist(),'follow_ratio_next_trial']=np.nan\n",
    "#all_evaluation.to_csv(\"all_evaluation.csv\",index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 2.2: Analyse follow ratio in different situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell analyses the follow ratio in different length of ISI\n",
    "graph_colour_code = analysis_methods.get(\"graph_colour_code\")\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,5), tight_layout=True)\n",
    "ax1, ax2= axes.flatten()\n",
    "i=0\n",
    "for keys, grp in all_evaluation.groupby(['duration_ISI']):\n",
    "    print(keys)\n",
    "    this_color=graph_colour_code[i]\n",
    "    ax1.scatter(grp['follow_ratio_previous_trial'],grp['follow_ratio_this_trial'],c=this_color)\n",
    "    ax2.scatter(grp['follow_ratio_this_trial'],grp['follow_ratio_next_trial'],c=this_color)\n",
    "    i=i+1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "follow_time_aba,follow_time_tbt,_,_=plot_follow_response_distribution(all_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "follow_time_aba,follow_time_tbt,_,_=plot_follow_response_distribution(all_evaluation[all_evaluation['speed']==2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gain_list=[0.25,0.5,1,2.0,4.0]\n",
    "gain_list=[0.75,1,1.25,1.5]\n",
    "for this_gain in gain_list:\n",
    "    follow_time_aba,follow_time_tbt,_,_=plot_follow_response_distribution(all_evaluation[all_evaluation['rotation_gain']==this_gain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##plot the relationship between travel distance and proportion of following time\n",
    "# print(\"first 1/3 best of followers:\", np.quantile(follow_time_aba, 0.66))\n",
    "# print(\"middle 1/3 best of followers:\", np.quantile(follow_time_aba, 0.33))\n",
    "camera_fps=analysis_methods.get(\"camera_fps\")\n",
    "fair_follower_threshold=np.quantile(follow_time_aba, 0.33)\n",
    "good_follower_threshold=np.quantile(follow_time_aba, 0.66)\n",
    "con_follow_time_list=[]\n",
    "con_travel_distance_list=[]\n",
    "exp_follow_time_list=[]\n",
    "exp_travel_distance_list=[]\n",
    "fig, axes = plt.subplots(\n",
    "        nrows=2, ncols=2, figsize=(9,10), tight_layout=True\n",
    "    )\n",
    "ax1, ax2, ax3, ax4 = axes.flatten()\n",
    "ax1.set(xticks=[0,0.25,0.5,0.75,1],xticklabels=(['0', '25', '50', '75', '100']),xlim=(0,0.5))\n",
    "ax2.set(xticks=[0,0.25,0.5,0.75,1],xticklabels=(['0', '25', '50', '75', '100']),xlim=(0,1))\n",
    "for keys, this_data in all_evaluation.groupby(['animal_id']):\n",
    "    p_follow=this_data['num_follow_epochs'].sum()/this_data['number_frames'].sum()\n",
    "    if p_follow>good_follower_threshold:\n",
    "        color='k'\n",
    "    elif (p_follow>fair_follower_threshold) and (p_follow<good_follower_threshold):\n",
    "        color='k'\n",
    "    else:\n",
    "        color='k'\n",
    "    ax1.scatter(this_data['num_follow_epochs'].sum()/this_data['number_frames'].sum(),this_data['travel_distance'].sum(),c=color,s=8)\n",
    "    ax3.scatter(this_data['num_follow_epochs'].sum()/this_data['number_frames'].sum(),this_data['total_turning'].sum(),c=color,s=8)\n",
    "    # if p_follow>good_follower_threshold:\n",
    "    #     pass\n",
    "    # elif (p_follow>fair_follower_threshold) and (p_follow<good_follower_threshold):\n",
    "    #     pass\n",
    "    # else:\n",
    "    #     continue\n",
    "    for i,this_object in enumerate(sorted(this_data['object'].unique(), key=len)):\n",
    "        these_num_follow_epochs=this_data['num_follow_epochs'][this_data['object']==this_object]\n",
    "        these_num_frames=this_data['number_frames'][this_data['object']==this_object]\n",
    "        these_travel_distance=this_data['travel_distance'][this_data['object']==this_object]\n",
    "        these_total_turning=this_data['total_turning'][this_data['object']==this_object]\n",
    "        if this_object=='mov_glocust' or this_object== \"LeaderLocust\":\n",
    "            # ax2.scatter(these_num_follow_epochs/these_num_frames,these_travel_distance,c=color,s=8)\n",
    "            # ax4.scatter(these_num_follow_epochs/these_num_frames,these_total_turning,c=color,s=8)\n",
    "            ax2.scatter(these_num_follow_epochs/these_num_frames,these_travel_distance,c='k',s=8,alpha=0.5)\n",
    "            ax4.scatter(these_num_follow_epochs/these_num_frames,these_total_turning,c='k',s=8,alpha=0.5)\n",
    "            follow_time_ratio_tbt=these_num_follow_epochs/these_num_frames\n",
    "            con_follow_time_list.append(follow_time_ratio_tbt.values)\n",
    "            con_travel_distance_list.append(these_travel_distance.values)\n",
    "        else:\n",
    "            # ax2.scatter(these_num_follow_epochs/these_num_frames,these_travel_distance,edgecolors=color,marker='o', facecolors='none',s=8)\n",
    "            # ax4.scatter(these_num_follow_epochs/these_num_frames,these_total_turning,edgecolors=color,marker='o', facecolors='none',s=8)\n",
    "            ax2.scatter(these_num_follow_epochs/these_num_frames,these_travel_distance,c='r',s=8,alpha=0.5)\n",
    "            ax4.scatter(these_num_follow_epochs/these_num_frames,these_total_turning,c='r',s=8,alpha=0.5)\n",
    "            follow_time_ratio_tbt=these_num_follow_epochs/these_num_frames\n",
    "            exp_follow_time_list.append(follow_time_ratio_tbt.values)\n",
    "            exp_travel_distance_list.append(these_travel_distance.values)\n",
    "ax3.set(xticks=[0,0.25,0.5,0.75,1],xticklabels=(['0', '25', '50', '75', '100']),xlim=(0,0.5))\n",
    "ax4.set(xticks=[0,0.25,0.5,0.75,1],xticklabels=(['0', '25', '50', '75', '100']),xlim=(0,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_follow_time_arr=np.hstack(con_follow_time_list)\n",
    "con_travel_distance_arr=np.hstack(con_travel_distance_list)\n",
    "exp_follow_time_arr=np.hstack(exp_follow_time_list)\n",
    "exp_travel_distance_arr=np.hstack(exp_travel_distance_list)\n",
    "mean_con_follow_time=np.nanmean(con_follow_time_arr)\n",
    "sem_con_follow_time=np.nanstd(con_follow_time_arr, ddof=1) / np.sqrt(con_follow_time_arr.shape[0])\n",
    "mean_con_travel_distance=np.nanmean(con_travel_distance_arr)\n",
    "sem_con_travel_distance=np.nanstd(con_travel_distance_arr, ddof=1) / np.sqrt(con_travel_distance_arr.shape[0])\n",
    "mean_exp_follow_time=np.nanmean(exp_follow_time_arr)\n",
    "sem_exp_follow_time=np.nanstd(exp_follow_time_arr, ddof=1) / np.sqrt(exp_follow_time_arr.shape[0])\n",
    "mean_exp_travel_distance=np.nanmean(exp_travel_distance_arr)\n",
    "sem_exp_travel_distance=np.nanstd(exp_travel_distance_arr, ddof=1) / np.sqrt(exp_travel_distance_arr.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(figsize=(3,3), dpi=300)\n",
    "save_output=True\n",
    "palette = {\n",
    "    'InanimatedLeaderLocust_black': 'tab:grey',\n",
    "    'LeaderLocust': 'tab:red',\n",
    "}\n",
    "svg_name = f\"follow_time_vs_travel_distance.svg\"\n",
    "png_name = f\"follow_time_vs_travel_distance.png\"\n",
    "sns.jointplot(data=all_evaluation, x=\"follow_ratio_this_trial\", y=\"travel_distance\", hue=\"object\",palette=palette,marginal_ticks=True)\n",
    "#sns.kdeplot(con_follow_time_arr, cut=0, color=\"b\", fill=True, alpha=0.7)\n",
    "plt.xlim(0, 1)\n",
    "plt.xticks([0, 0.5])\n",
    "plt.ylim(0,700)\n",
    "plt.yticks([0,600])\n",
    "if save_output==True:    \n",
    "    plt.savefig(svg_name)\n",
    "    plt.savefig(png_name)\n",
    "# sns.pairplot(\n",
    "#     all_evaluation,\n",
    "#     x_vars=[\"follow_ratio_this_trial\", \"travel_distance\",\"total_turning\"],\n",
    "#     y_vars=[\"follow_ratio_this_trial\", \"travel_distance\",\"total_turning\"],hue=\"object\",palette=palette,corner=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort data based on when the experiment is done\n",
    "all_evaluation['batchs'] = pd.cut(all_evaluation['animal_id'], bins=3, labels=['1st', '2nd', '3rd'])\n",
    "graph_colour_code =analysis_methods.get(\"graph_colour_code\")\n",
    "camera_fps=analysis_methods.get(\"camera_fps\")\n",
    "fig, axes = plt.subplots(\n",
    "        nrows=1, ncols=2, figsize=(9,10), tight_layout=True\n",
    "    )\n",
    "ax1, ax2 = axes.flatten()\n",
    "for keys, grp in all_evaluation.groupby(['animal_id']):\n",
    "    if grp['batchs'].values[0]=='1st':\n",
    "        this_color=graph_colour_code[0]\n",
    "    elif grp['batchs'].values[0]=='2nd':\n",
    "        this_color=graph_colour_code[1]\n",
    "    else:\n",
    "        this_color=graph_colour_code[2]\n",
    "    travel_distance=grp['travel_distance'].sum()/camera_fps\n",
    "    total_turning=grp['total_turning'].sum()/camera_fps\n",
    "    num_follow_epochs=grp['num_follow_epochs'].sum()/grp['number_frames'].sum()\n",
    "    ax1.scatter(grp['num_follow_epochs'].sum()/grp['number_frames'].sum(),grp['travel_distance'].sum()/camera_fps,c=this_color,s=8)\n",
    "    ax2.scatter(grp['num_follow_epochs'].sum()/grp['number_frames'].sum(),grp['total_turning'].sum()/camera_fps,c=this_color,s=8)\n",
    "for keys, grp in all_evaluation.groupby(['batchs']):\n",
    "    if grp['batchs'].values[0]=='1st':\n",
    "        this_color=graph_colour_code[0]\n",
    "    elif grp['batchs'].values[0]=='2nd':\n",
    "        this_color=graph_colour_code[1]\n",
    "    else:\n",
    "        this_color=graph_colour_code[2]\n",
    "    ax1.scatter(grp['num_follow_epochs'].sum()/grp['number_frames'].sum(),grp['travel_distance'].sum()/camera_fps/grp['animal_id'].unique().shape[0],edgecolors=this_color,s=20,facecolors='none')\n",
    "    ax2.scatter(grp['num_follow_epochs'].sum()/grp['number_frames'].sum(),grp['total_turning'].sum()/camera_fps/grp['animal_id'].unique().shape[0],edgecolors=this_color,s=20,facecolors='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chop the data into 3 groups depends on time. Then we can analyse the 1st, 2nd and 3rd of the data\n",
    "all_evaluation['time_group'] = pd.cut(all_evaluation['trial_id'], bins=3, labels=['1st', '2nd', '3rd'])\n",
    "num_follow_list=[]\n",
    "travel_distance_list=[]\n",
    "for keys, grp in all_evaluation.groupby(['animal_id','time_group']):\n",
    "    num_follow_list.append(grp[\"num_follow_epochs\"].sum())\n",
    "    travel_distance_list.append(grp[\"travel_distance\"].sum())\n",
    "num_follow_arr=np.array(num_follow_list).reshape(-1,3)\n",
    "travel_distance_arr=np.array(travel_distance_list).reshape(-1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "        nrows=1, ncols=2, figsize=(9,10), tight_layout=True\n",
    "    )\n",
    "ax1, ax2 = axes.flatten()\n",
    "ax1.plot(np.transpose(num_follow_arr), 'o-')\n",
    "ax1.plot(np.mean(num_follow_arr,axis=0),'k',linewidth=5)\n",
    "ax1.plot(np.median(num_follow_arr,axis=0),'k',linewidth=5,alpha=0.6)\n",
    "ax2.plot(np.transpose(travel_distance_arr), 'o-')\n",
    "ax2.plot(np.mean(travel_distance_arr,axis=0),'k',linewidth=5)\n",
    "ax2.plot(np.median(travel_distance_arr,axis=0),'k',linewidth=5,alpha=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 2.3: Using 1D histogram to plot proportion of time and 2D histogram to plot relative position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trials=pd.concat(relative_pos_all_animals)\n",
    "if len(simulated_relative_pos_all_animals)>0:\n",
    "    all_simulated_trials=pd.concat(simulated_relative_pos_all_animals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trials_gray=all_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trials_grass=all_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trials=pd.concat((all_trials_grass,all_trials_gray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### data with rotation gain and translation gain\n",
    "\n",
    "extract_follow_epoches=analysis_methods.get(\"extract_follow_epoches\",True)\n",
    "distribution_with_entire_body=analysis_methods.get(\"distribution_with_entire_body\",False)\n",
    "#distribution_with_entire_body=False\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(all_trials['rotation_gain'].unique()),figsize=(18, 6),tight_layout=True,sharex=True, sharey=True)\n",
    "i=0\n",
    "if extract_follow_epoches:\n",
    "    xlimit=(-5,40)\n",
    "    ylimit=(-15,15)\n",
    "else:\n",
    "    xlimit=(-20,100)\n",
    "    ylimit=(-45,45)\n",
    "for keys, grp in all_trials.groupby(['rotation_gain','translation_gain']):\n",
    "        print(keys)\n",
    "        if distribution_with_entire_body:\n",
    "            body_points=generate_points_within_rectangles(grp['x'].values,grp['y'].values,1,4,2,21)\n",
    "            axes[i].hist2d(body_points[:,0],body_points[:,1],bins=1000)\n",
    "        else:\n",
    "            axes[i].hist2d(grp['x'].values,grp['y'].values,bins=400)\n",
    "        axes[i].set(\n",
    "        yticks=[ylimit[0],0,ylimit[1]],\n",
    "        xticks=[-5,0,4,20,40],\n",
    "        xlim=xlimit,ylim=ylimit,title=f'rotation_gain:{keys[0]},translation_gain:{(keys[1])}',adjustable='box', aspect='equal')\n",
    "        #rect = patches.Rectangle((0,-1), 6, 2, linewidth=1, edgecolor='red',linestyle=\"--\",facecolor='none')\n",
    "        #cir1 = patches.Circle((4,0), radius=0.5, linewidth=0.5, edgecolor='yellow')\n",
    "        cir2 = patches.Circle((0,0), radius=0.4, linewidth=0.4, edgecolor='red',facecolor='red')\n",
    "        #axes[i].add_patch(cir1)\n",
    "        axes[i].add_patch(cir2)\n",
    "        i=i+1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_follow_epoches=analysis_methods.get(\"extract_follow_epoches\",True)\n",
    "distribution_with_entire_body=analysis_methods.get(\"distribution_with_entire_body\",False)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(all_trials[\"type\"].unique())*len(all_trials[\"degree\"].unique()),figsize=(18, 6),tight_layout=True,sharex=True, sharey=True)\n",
    "i=0\n",
    "plotting_agent2focal=True\n",
    "for keys, grp in all_trials.groupby(['type','degree']):\n",
    "    if extract_follow_epoches:\n",
    "        xlimit=(-5,40)\n",
    "        ylimit=(-15,15)\n",
    "    else:\n",
    "        xlimit=(-20,100)\n",
    "        ylimit=(-45,45)\n",
    "    print(keys)\n",
    "    if plotting_agent2focal:\n",
    "        x=grp['x'].values*-1\n",
    "        y=grp['y'].values*-1\n",
    "        xticks_values=[-40,-20,-4,0,5]\n",
    "        xlimit=(xlimit[1]*-1,xlimit[0]*-1)\n",
    "    else:\n",
    "        x=grp['x'].values\n",
    "        y=grp['y'].values\n",
    "        xticks_values=[-5,0,4,20,40]\n",
    "        \n",
    "    if distribution_with_entire_body:\n",
    "        body_points=generate_points_within_rectangles(x,y,1,4,2,21)\n",
    "        axes[i].hist2d(body_points[:,0],body_points[:,1],bins=1000)\n",
    "    else:\n",
    "        axes[i].hist2d(x,y,bins=400)\n",
    "    axes[i].set(\n",
    "    yticks=[ylimit[0],0,ylimit[1]],\n",
    "    xticks=xticks_values,\n",
    "    xlim=xlimit,ylim=ylimit,title=f'agent:{keys[0]},deg:{int(keys[1])}',adjustable='box', aspect='equal')\n",
    "        #rect = patches.Rectangle((0,-1), 6, 2, linewidth=1, edgecolor='red',linestyle=\"--\",facecolor='none')\n",
    "        #cir1 = patches.Circle((4,0), radius=0.5, linewidth=0.5, edgecolor='yellow')\n",
    "    cir2 = patches.Circle((0,0), radius=0.4, linewidth=0.4, edgecolor='red',facecolor='red')\n",
    "        #axes[i].add_patch(cir1)\n",
    "    axes[i].add_patch(cir2)\n",
    "    i=i+1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=len(all_simulated_trials[\"type\"].unique())*len(all_simulated_trials[\"degree\"].unique()),figsize=(18, 6),tight_layout=True,sharex=True, sharey=True)\n",
    "i=0\n",
    "if extract_follow_epoches:\n",
    "    xlimit=(-5,40)\n",
    "    ylimit=(-15,15)\n",
    "else:\n",
    "    xlimit=(-20,100)\n",
    "    ylimit=(-45,45)\n",
    "for keys, grp in all_simulated_trials.groupby(['type','degree']):\n",
    "        if distribution_with_entire_body:\n",
    "            body_points=generate_points_within_rectangles(grp['x'].values,grp['y'].values,1,4,2,21)\n",
    "            axes[i].hist2d(body_points[:,0],body_points[:,1],bins=1000)\n",
    "        else:\n",
    "            axes[i].hist2d(grp['x'].values,grp['y'].values,bins=400)\n",
    "        axes[i].set(\n",
    "        yticks=[ylimit[0],0,ylimit[1]],\n",
    "        xticks=[-5,0,4,20,40],\n",
    "        xlim=xlimit,ylim=ylimit,title=f'agent:{keys[0]},deg:{int(keys[1])}',adjustable='box', aspect='equal')\n",
    "        #rect = patches.Rectangle((0,-1), 6, 2, linewidth=1, edgecolor='red',linestyle=\"--\",facecolor='none')\n",
    "        #cir1 = patches.Circle((4,0), radius=0.5, linewidth=0.5, edgecolor='yellow')\n",
    "        cir2 = patches.Circle((0,0), radius=0.4, linewidth=0.4, edgecolor='red',facecolor='red')\n",
    "        #axes[i].add_patch(cir1)\n",
    "        axes[i].add_patch(cir2)\n",
    "        i=i+1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##plot the distribution of follow epochs in a trial\n",
    "##In the future, will use agent based model to show the temporal distribution\n",
    "for keys, grp in all_trials.groupby(['type','degree']):\n",
    "    sim_grp=all_simulated_trials[(all_simulated_trials['type']==keys[0])&(all_simulated_trials['degree']==keys[1])]\n",
    "    print(keys)\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=1, ncols=2, figsize=(9,5), tight_layout=True\n",
    "    )\n",
    "    ax, ax2 = axes.flatten()\n",
    "    ax.hist(grp['ts'].values,bins=100,density=False,color='r')\n",
    "    ax.hist(sim_grp['ts'].values,bins=100,density=False,color='tab:gray',alpha=0.3)\n",
    "    ax2.hist(grp['ts'].values,bins=100,color='r',density=True,histtype=\"step\",cumulative=True,label=\"Cumulative histogram\")\n",
    "    ax2.hist(sim_grp['ts'].values,bins=100,color='tab:gray',alpha=0.3,density=True,histtype=\"step\",cumulative=True,label=\"Cumulative histogram\")\n",
    "    ax.set(xlim=(0,60),title=f'agent:{keys[0]},deg:{int(keys[1])}')\n",
    "    ax.set(ylim=(0,2600))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_methods.update({\"distribution_with_entire_body\": True})\n",
    "from scipy.optimize import curve_fit\n",
    "#https://stackoverflow.com/questions/78384537/fitting-2d-histograms-with-2d-gaussians\n",
    "def twoD_Gaussian(xy, amplitude, xo, yo, sigma_x, sigma_y, rho):\n",
    "    x, y = xy\n",
    "    xo = float(xo)\n",
    "    yo = float(yo)\n",
    "    a = 1 / sigma_x ** 2\n",
    "    b = rho / sigma_x / sigma_y\n",
    "    c = 1 / sigma_y ** 2\n",
    "    exponent=  - (1 / (2 * (1 - rho ** 2)))\\\n",
    "                 * (a * (x - xo) ** 2 - 2 * b * (x - xo) * (y - yo) + c * (y - yo) ** 2)\n",
    "    g = amplitude * np.exp(exponent)\n",
    "    return g.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbr_bins = 100\n",
    "B, xedges, yedges = np.histogram2d(grp['x'].values,grp['y'].values, bins=nbr_bins)\n",
    "extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n",
    "bin_centers_bx = (xedges[:-1] + xedges[1:]) / 2.0\n",
    "bin_centers_by = (yedges[:-1] + yedges[1:]) / 2.0\n",
    "X, Y = np.meshgrid(bin_centers_bx, bin_centers_by)\n",
    "data = B.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0, 4]\n",
    "p0 = (B.max(),mean[0],mean[1],1.7,1.0,0.9)\n",
    "coeff, var_matrix = curve_fit(twoD_Gaussian, (X, Y), data, p0=p0)\n",
    "print('hist fit', coeff)\n",
    "print('var_matrix', var_matrix)\n",
    "data_fitted_hist = twoD_Gaussian((X, Y), *coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "xlimit=(-30,30)\n",
    "ylimit=(-30,30)\n",
    "ax[0].imshow(data.reshape((nbr_bins, nbr_bins)), origin=\"lower\",\n",
    "             extent=extent, interpolation=\"none\")\n",
    "ax[1].imshow(data.reshape((nbr_bins, nbr_bins)), origin=\"lower\",\n",
    "             extent=extent, interpolation=\"none\")\n",
    "ax[1].contour(X, Y, data_fitted_hist.reshape(nbr_bins, nbr_bins), 2, colors=\"w\",linestyles='dashed',linewidths=0.5)\n",
    "ax[0].set(\n",
    "        yticks=[-30,0.0,30],\n",
    "        xticks=[-30,0.0,30],\n",
    "        xlim=xlimit,ylim=ylimit)\n",
    "ax[1].set(\n",
    "        yticks=[-30,0.0,30],\n",
    "        xticks=[-30,0.0,30],\n",
    "        xlim=xlimit,ylim=ylimit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### section 2.3.5 plot velocity vector field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##plot the spatial distribution of virtual locusts in a 2D histogram\n",
    "plotting_agent2focal=True\n",
    "analysis_methods.update({\"distribution_with_entire_body\":False})\n",
    "distribution_with_entire_body=analysis_methods.get(\"distribution_with_entire_body\",True)\n",
    "extract_follow_epoches=analysis_methods.get(\"extract_follow_epoches\",True)\n",
    "if extract_follow_epoches:\n",
    "    xlimit=ylimit=(-30,30)\n",
    "else:\n",
    "    xlimit=(-20,100)\n",
    "    ylimit=(-45,45)\n",
    "#for keys, grp in all_trials.groupby(['rotation_gain']):\n",
    "    # if keys[0]!=float(analysis_methods.get(\"follow_above_speed\")):\n",
    "    #     continue\n",
    "for keys, grp in all_trials.groupby(['type']):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1,tight_layout=True,dpi=300,figsize=(4,2))\n",
    "    if plotting_agent2focal:\n",
    "        x=grp['x'].values*-1\n",
    "        y=grp['y'].values*-1\n",
    "        xticks_values=[-40,-20,-4,0,5]\n",
    "        xlimit=(xlimit[1]*-1,xlimit[0]*-1)\n",
    "    else:\n",
    "        x=grp['x'].values\n",
    "        y=grp['y'].values\n",
    "    if distribution_with_entire_body:\n",
    "        body_points=generate_points_within_rectangles(x,y,1,4,2,21)\n",
    "        ax.hist2d(body_points[:,0],body_points[:,1],bins=225)\n",
    "    else:\n",
    "        ax.hist2d(x,y,bins=225)\n",
    "    ax.set(\n",
    "        adjustable='box', aspect='equal',\n",
    "        yticks=[-30,0.0,30],\n",
    "        xticks=[-30,0.0,30],\n",
    "        xticklabels=(['-0.3', '0.0', '0.3']),\n",
    "        yticklabels=(['-0.3', '0.0', '0.3']),\n",
    "        xlim=xlimit,ylim=ylimit)\n",
    "    '''\n",
    "    the position of virtual locust if their butt is at the origin\n",
    "    rect = patches.Rectangle((0,-1), 6, 2, linewidth=1, edgecolor='red',linestyle=\"--\",facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    '''\n",
    "    #cir = patches.Circle((0,0), radius=0.5, linewidth=0.5, edgecolor='red')\n",
    "    #ax.add_patch(cir)\n",
    "    fig.suptitle(f'agent:{keys[0]}')\n",
    "    fig.savefig(f\"{keys[0]}_heatmap_agent_center.jpg\")\n",
    "    fig1,fig2=plot_velocity_vector_field(grp)\n",
    "    fig1.savefig(f\"{keys[0]}_velocity_vector_field.jpg\")\n",
    "    fig2.savefig(f\"{keys[0]}_velocity_distance_analysis.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 2.4: Plot temporal distribution of follow epoch throughout the trial course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##plot the temporal distribution of follow epochs in a trial\n",
    "for keys, grp in all_trials.groupby(['type']):\n",
    "    print(keys)\n",
    "    sim_grp=all_simulated_trials[all_simulated_trials['type']==keys[0]]\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=1, ncols=2, figsize=(9,5), tight_layout=True\n",
    "    )\n",
    "    ax, ax2 = axes.flatten()\n",
    "    ax.hist(grp['ts'].values,bins=100,density=False,color='r')\n",
    "    ax.hist(sim_grp['ts'].values,bins=100,density=False,color='tab:gray',alpha=0.5)\n",
    "    ax2.hist(grp['ts'].values,bins=100,density=True,color='r',histtype=\"step\",cumulative=True,label=\"Cumulative histogram\")\n",
    "    ax2.hist(sim_grp['ts'].values,bins=100,color='tab:gray',alpha=0.5,density=True,histtype=\"step\",cumulative=True,label=\"Cumulative histogram\")\n",
    "    ax.set(xlim=(0,60))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 2.5: estimation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is for dabest analysis. However, it is not good for experiment design with multiple trials\n",
    "# import dabest\n",
    "# con_follow_time_arr=np.hstack(con_follow_time_list)\n",
    "# exp_follow_time_arr=np.hstack(exp_follow_time_list)\n",
    "# dabest_pd = pd.DataFrame({\"Control\": con_follow_time_arr, \"Test\": exp_follow_time_arr})\n",
    "# two_groups_unpaired = dabest.load(dabest_pd, idx=(\"Control\", \"Test\"))\n",
    "# two_groups_unpaired.mean_diff.plot()\n",
    "''' a format that seperates trials and animals. Rows are animals and columns are trials\n",
    "c_name_list=column_name_list(con_follow_time_list[0].shape[0],'con')\n",
    "tmp = pd.DataFrame(con_follow_time_list,columns=c_name_list)\n",
    "c_name_list=column_name_list(exp_follow_time_list[0].shape[0],'exp')\n",
    "tmp2 = pd.DataFrame(exp_follow_time_list,columns=c_name_list)\n",
    "df_combined=pd.concat([tmp.reset_index(drop=True), tmp2.reset_index(drop=True)], axis=1)\n",
    "df_combined=df_combined.reset_index(names='ID')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 2.6: Exporting dataset into Mat or csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##save the raw data into mat file for matlab\n",
    "from scipy.io import savemat\n",
    "all_raw=pd.concat(relative_pos_all_animals)\n",
    "#all_raw['trial_id']=all_raw['trial_id'].astype(int)\n",
    "data_dict = {name: col.values for name, col in all_raw.items()}\n",
    "summary_file_name = Path(thisDataset) /\"time_series_curated.mat\"\n",
    "savemat(summary_file_name, data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A helper function to export the data into csv file\n",
    "def summarise_as_csv(input_list,agent_type='glocust'):\n",
    "    df=pd.DataFrame(np.hstack(input_list))\n",
    "    df.insert(0, 'agent_type', np.repeat(agent_type,df.shape[0]).tolist())\n",
    "    df.insert(0, 'ID', np.repeat(np.arange(len(input_list)),int(df.shape[0]/len(input_list))).tolist())\n",
    "    df.to_csv(f'{agent_type}.csv')\n",
    "def summarise_as_csv2(list1,list2,type1='gregarious_locust',type2='black_locust'):\n",
    "    df=pd.DataFrame({\"ratio\":np.concat([np.hstack(list1),np.hstack(list2)])})\n",
    "    df.insert(0, 'agent_type', np.concat((np.repeat(type1,len(list1)*list1[0].shape[0]),np.repeat(type2,len(list2)*list2[0].shape[0]))).tolist())\n",
    "    df.insert(0, 'times',np.repeat(np.tile(np.arange(exp_follow_time_list[0].shape[0]),len(exp_follow_time_list)),2).tolist())\n",
    "    df.insert(0, 'ID', np.repeat(np.repeat(np.arange(len(exp_follow_time_list)),exp_follow_time_list[0].shape[0]),2).tolist())\n",
    "    df.to_csv(f'{type1}_vs_{type2}_repeated_measure.csv')\n",
    "#summarise_as_csv(exp_follow_time_list,'exp_agent')\n",
    "#summarise_as_csv(con_follow_time_list)\n",
    "summarise_as_csv2(con_follow_time_list,exp_follow_time_list,'con','exp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.0: analysis walking behavours before and after the presence of stimulus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Session 3.1: combine list together and check their distribution to look for a threshold that separate stationary and behavioural states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(raster_across_animals_unity)==list:\n",
    "    all_trials=pd.concat(raster_across_animals_unity)\n",
    "    all_trials=fix_data_type(all_trials)\n",
    "else:\n",
    "    all_trials=fix_data_type(raster_across_animals_unity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_name='omega'\n",
    "check_baseline_distribution(all_trials,analysis_methods,metrics_name,3)\n",
    "# check_baseline_distribution(all_trials,analysis_methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Session 3.2: split trials based on stationary and moving state and retrieve their index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for simplicity, classifying trial only based on stationary and walk trails\n",
    "classify_trials=False\n",
    "trial_classfier='velocity'#'velocity' or 'omega'\n",
    "metrics_name='omega'#'velocity' or 'omega'\n",
    "_,these_metrics,these_normalised_metrics=split_trials(analysis_methods,all_trials,metrics_name,'normalised_omega',1)#1 degree or 0.0002 rad\n",
    "if classify_trials:\n",
    "    movement_trial_boolean,_,_=split_trials(analysis_methods,all_trials,trial_classfier)\n",
    "else:\n",
    "    movement_trial_boolean=[True]*len(these_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#movement_trial_boolean,these_metrics,these_normalised_metrics=split_trials(analysis_methods,all_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_movement_ith_trial,after_no_movement_ith_trial=extract_trial_index(movement_trial_boolean,len(all_trials['animal_id'].unique()),analysis_methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Session 3.3: select animals or trial of interest (related to follow response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##criteria to sort out data from good followers\n",
    "# fair_follower_threshold=0.20\n",
    "#fair_follower_threshold=0.1667\n",
    "# good_follower_threshold=0.3\n",
    "#data from grass1 secene\n",
    "# first 1/3 best of followers: 0.04542389707280022\n",
    "# middle 1/3 best of followers: 0.024365961537554066\n",
    "def select_animal_or_trial_to_analyse(all_evaluation,use_aba_threshold,based_on_follow_walk_ratio,threshold_value=0.25):\n",
    "    if based_on_follow_walk_ratio:\n",
    "        denominator='num_walk_epochs'\n",
    "    else:\n",
    "        denominator='number_frames'\n",
    "    if use_aba_threshold==True:\n",
    "        p_follow=all_evaluation.groupby(['animal_id'])['num_follow_epochs'].sum()/all_evaluation.groupby(['animal_id'])[denominator].sum()\n",
    "        #follower_of_interest=(p_follow>fair_follower_threshold) & (p_follow<good_follower_threshold)\n",
    "        #data_of_interest=p_follow>0.14 ### this data of interest means subjects of interest\n",
    "        data_of_interest=p_follow>threshold_value\n",
    "        #rows_of_follower=follower_of_interest.repeat(int(all_evaluation.shape[0]/follower_of_interest.shape[0]))\n",
    "    else:\n",
    "        data_of_interest=all_evaluation['num_follow_epochs']/all_evaluation[denominator]>threshold_value\n",
    "        #data_of_interest=(all_evaluation['num_follow_epochs']/all_evaluation[denominator]<threshold_value)&(all_evaluation['num_follow_epochs']/all_evaluation[denominator]>0.1)\n",
    "    return data_of_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_aba_threshold=True\n",
    "based_on_follow_walk_ratio=True\n",
    "data_of_interest=select_animal_or_trial_to_analyse(all_evaluation,use_aba_threshold,based_on_follow_walk_ratio,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_of_interest=select_animal_or_trial_to_analyse(all_evaluation[all_evaluation['speed']==2],use_aba_threshold,based_on_follow_walk_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##these vars are for the experiment \"sta_black_locust_2dir_3_initial_position\"\n",
    "#var1='radial_distance'\n",
    "#var2='mu'\n",
    "##these vars are for the experiment \"closed_loop_sta_black_locust_open_loop_sta_black_locust\"\n",
    "var1='speed'\n",
    "var2='mu'\n",
    "##this is for the experiment \"choice_vr_locust_sta_black_locust\"\n",
    "# if all_evaluation['object'].unique().shape[0]>1:\n",
    "# var1='object'\n",
    "# var2='mu'\n",
    "var1='mu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Session 3.4: plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for type_key in all_evaluation[var1].unique():\n",
    "    print(type_key)\n",
    "    if use_aba_threshold==True:\n",
    "        rows_of_follower=data_of_interest.repeat(int(all_evaluation.shape[0]/data_of_interest.shape[0]))\n",
    "    if \"var2\" in locals():\n",
    "        for this_condition in all_evaluation[var2].unique():\n",
    "            print(this_condition)\n",
    "            if use_aba_threshold==True:\n",
    "                row_of_interest=(all_evaluation[var2].reset_index(drop=True)==this_condition)&(all_evaluation[var1].reset_index(drop=True)==type_key)&(rows_of_follower.reset_index(drop=True))\n",
    "            else:\n",
    "                row_of_interest=(all_evaluation[var2].reset_index(drop=True)==this_condition)&(all_evaluation[var1].reset_index(drop=True)==type_key)&(data_of_interest.reset_index(drop=True))\n",
    "            plot_visual_evoked_behaviour(these_metrics,these_normalised_metrics,after_movement_ith_trial,after_no_movement_ith_trial,analysis_methods,metrics_name,row_of_interest,type_key,this_condition)\n",
    "    else:\n",
    "        if use_aba_threshold==True:\n",
    "            row_of_interest=(all_evaluation[var1].reset_index(drop=True)==type_key)&rows_of_follower.reset_index(drop=True)\n",
    "        else:\n",
    "            row_of_interest=(all_evaluation[var1].reset_index(drop=True)==type_key)&(data_of_interest.reset_index(drop=True))\n",
    "        plot_visual_evoked_behaviour(these_metrics,these_normalised_metrics,after_movement_ith_trial,after_no_movement_ith_trial,analysis_methods,metrics_name,row_of_interest,type_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for type_key in all_evaluation[var1].unique():\n",
    "    print(type_key)\n",
    "    if use_aba_threshold==True:\n",
    "        rows_of_follower=data_of_interest.repeat(int(all_evaluation.shape[0]/data_of_interest.shape[0]))\n",
    "    for this_condition in all_evaluation[var2].unique():\n",
    "        print(this_condition)\n",
    "        if use_aba_threshold==True:\n",
    "            row_of_interest=(all_evaluation[var2].reset_index(drop=True)==this_condition)&(all_evaluation[var1].reset_index(drop=True)==type_key)&(rows_of_follower.reset_index(drop=True))\n",
    "        else:\n",
    "            row_of_interest=(all_evaluation[var2].reset_index(drop=True)==this_condition)&(all_evaluation[var1].reset_index(drop=True)==type_key)&(data_of_interest.reset_index(drop=True))\n",
    "        plot_visual_evoked_behaviour(these_metrics,these_normalised_metrics,after_movement_ith_trial,after_no_movement_ith_trial,analysis_methods,metrics_name,row_of_interest,type_key,this_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for simplicity, classifying trial only based on stationary and walk trails\n",
    "classify_trials=True\n",
    "trial_classfier='velocity'#'velocity' or 'omega'\n",
    "metrics_name='velocity'#'velocity' or 'omega'\n",
    "_,these_metrics,these_normalised_metrics=split_trials(analysis_methods,all_trials)\n",
    "if classify_trials:\n",
    "    movement_trial_boolean,_,_=split_trials(analysis_methods,all_trials,trial_classfier)\n",
    "else:\n",
    "    movement_trial_boolean=[True]*len(these_metrics)\n",
    "after_movement_ith_trial,after_no_movement_ith_trial=extract_trial_index(movement_trial_boolean,len(all_trials['animal_id'].unique()),analysis_methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_name='velocity'\n",
    "for type_key in all_evaluation[var1].unique():\n",
    "    print(type_key)\n",
    "    if use_aba_threshold==True:\n",
    "        rows_of_follower=data_of_interest.repeat(int(all_evaluation.shape[0]/data_of_interest.shape[0]))\n",
    "    for this_condition in all_evaluation[var2].unique():\n",
    "        print(this_condition)\n",
    "        if use_aba_threshold==True:\n",
    "            row_of_interest=(all_evaluation[var2].reset_index(drop=True)==this_condition)&(all_evaluation[var1].reset_index(drop=True)==type_key)&(rows_of_follower.reset_index(drop=True))\n",
    "        else:\n",
    "            row_of_interest=(all_evaluation[var2].reset_index(drop=True)==this_condition)&(all_evaluation[var1].reset_index(drop=True)==type_key)&(data_of_interest.reset_index(drop=True))\n",
    "        plot_visual_evoked_behaviour(these_metrics,these_normalised_metrics,after_movement_ith_trial,after_no_movement_ith_trial,analysis_methods,metrics_name,row_of_interest,type_key,this_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_name='velocity'\n",
    "for type_key in all_evaluation[var1].unique():\n",
    "    print(type_key)\n",
    "    if use_aba_threshold==True:\n",
    "        rows_of_follower=data_of_interest.repeat(int(all_evaluation.shape[0]/data_of_interest.shape[0]))\n",
    "        row_of_interest=(all_evaluation[var1].reset_index(drop=True)==type_key)&rows_of_follower.reset_index(drop=True)\n",
    "    else:\n",
    "        row_of_interest=(all_evaluation[var1].reset_index(drop=True)==type_key)&(data_of_interest.reset_index(drop=True))\n",
    "    plot_visual_evoked_behaviour(these_metrics,these_normalised_metrics,after_movement_ith_trial,after_no_movement_ith_trial,analysis_methods,'velocity',row_of_interest,type_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session 4: New codes to analysis data for raster plot. These functions take data from fictrac directly so can analyse what happened before the stimulus onset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Session 4.0: summarise fictrac data with sequence config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.concat(raster_across_animals_fictrac)\n",
    "rot_y=test['heading_angle'].values\n",
    "dif_x=np.diff(test['X'].values)\n",
    "dif_y=np.diff(test['Y'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, turn_degree_fbf = diff_angular_degree(rot_y,0,False)\n",
    "focal_distance_fbf = np.sqrt(np.sum([dif_x**2, dif_y**2], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(nrows=2, ncols=1, figsize=(9, 7), tight_layout=True)\n",
    "ax1.hist(turn_degree_fbf,bins=10000)\n",
    "ax2.hist(focal_distance_fbf,bins=300000)\n",
    "ax1.set(xlim=(-0.02,0.02))\n",
    "ax2.set(xlim=(0,0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_methods.update({\"analysis_window\":[-25,25]})\n",
    "var1='mu'\n",
    "var2=None\n",
    "step_interest=np.arange(1, seq_config_all_animals[0].shape[0], 2)\n",
    "export_to_matlab_list=[]\n",
    "step_interest=np.arange(1, seq_config_all_animals[0].shape[0], 2)\n",
    "## to avoid memory crushed, analyse each animal one by one. In this case we need to comment out the line 85-88 in sorting time series analysis\n",
    "for animal_interest in range(len(seq_config_all_animals)):\n",
    "    ready_to_plot=sort_raster_fictrac(raster_across_animals_fictrac,[animal_interest],step_interest,analysis_methods,all_evaluation,var1,var2)\n",
    "    export_to_matlab_list.append(ready_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## export the data into matlab format\n",
    "from scipy.io import savemat\n",
    "all_raw=pd.concat(export_to_matlab_list)\n",
    "all_raw.drop(['run_trial', 'index'], axis=1,inplace=True)\n",
    "data_dict = {name: col.values for name, col in all_raw.items()}\n",
    "summary_file_name = Path(thisDataset) /\"time_series_curated.mat\"\n",
    "savemat(summary_file_name, data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Session 4.1: select a time window for the analysis and pick whether to choose animal of interest or trial of interest (related to follow behaviours) for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##this can select follower of interest\n",
    "use_aba_threshold=True\n",
    "threshold=0.045\n",
    "based_on_follow_walk_ratio=False\n",
    "if based_on_follow_walk_ratio:\n",
    "    denominator='num_walk_epochs'\n",
    "else:\n",
    "    denominator='number_frames'\n",
    "if \"all_evaluation\" in locals():\n",
    "    pass\n",
    "else:\n",
    "    print('For some response, this boolean overwrite the existing all_evaluation')\n",
    "    # all_evaluation=pd.concat(seq_config_all_animals)\n",
    "if \"configFile\" in all_evaluation.columns:\n",
    "    animal_interest=np.arange(len(seq_config_all_animals))\n",
    "    step_interest=np.arange(1, seq_config_all_animals[0].shape[0], 2)\n",
    "else:\n",
    "    if use_aba_threshold:\n",
    "        all_evaluation_aba = all_evaluation.groupby('animal_id').agg(\n",
    "            travel_distance_ISI=('travel_distance_ISI', 'sum'),\n",
    "            gross_turning_ISI=('gross_turning_ISI', 'sum'),\n",
    "            total_turning_ISI=('total_turning_ISI', 'sum'),\n",
    "            num_follow_epochs=('num_follow_epochs', 'sum'),\n",
    "            num_walk_epochs=('num_walk_epochs', 'sum'),\n",
    "            number_frames=('number_frames','sum'),\n",
    "            vr_no=('VR', 'first'),\n",
    "        )\n",
    "        animal_interest=all_evaluation_aba[all_evaluation_aba['num_follow_epochs']/all_evaluation_aba[denominator]>threshold].index\n",
    "        step_interest=np.arange(1, 2*len(all_evaluation['trial_id'].unique()), 2)\n",
    "    else:\n",
    "        ##this can select follow trials of interest and convert that into step_id\n",
    "        animal_interest=all_evaluation[all_evaluation['num_follow_epochs']/all_evaluation[denominator]>threshold]['animal_id']\n",
    "        step_interest=all_evaluation[all_evaluation['num_follow_epochs']/all_evaluation[denominator]>threshold]['trial_id']*2+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=sns.jointplot(data=all_evaluation_aba, x=\"travel_distance_ISI\", y=\"gross_turning_ISI\", hue=\"vr_no\")\n",
    "## add an additional marginal plot to show the distribution of the data\n",
    "a.plot_marginals(sns.rugplot, height=0.15, clip_on=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Session 4.2: create a pandas dataframe based on the time window, animal or trial of interest. The dataframe includes instead speed and angular velocity frame by frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var1='configFile'\n",
    "# var2=None\n",
    "var1='mu'\n",
    "var2='object'\n",
    "#this_summary_table=pd.concat(seq_config_all_animals)\n",
    "this_summary_table=all_evaluation\n",
    "analysis_methods.update({\"analysis_window\":[-2,5]})\n",
    "analysis_window=analysis_methods.get(\"analysis_window\")\n",
    "monitor_fps=analysis_methods.get(\"monitor_fps\")\n",
    "n_datapoints=(analysis_window[1]-analysis_window[0])*monitor_fps\n",
    "ready_to_plot=sort_raster_fictrac(raster_across_animals_fictrac,animal_interest,step_interest,analysis_methods,this_summary_table,var1,var2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Session 4.3: plot the results. Using seaborn line plots for average response, customised codes for trial by trial responses. Note: there is not yet to use circular statistics on seaborn-related plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.set_theme(style=\"darkgrid\")\n",
    "# Plot the responses for different events and regions\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=1, ncols=2, figsize=(9,5), tight_layout=True\n",
    ")\n",
    "i=0\n",
    "for keys, this_data in ready_to_plot.groupby(['run_trial']):\n",
    "    print(keys)\n",
    "    sns.lineplot(x=\"frame_count\", y=\"instant_speed\",hue=var1,data=this_data,ax=axes[i])\n",
    "    #axes[i].set_ylim([0,10])\n",
    "    axes[i].set(\n",
    "        xlabel=\"Time (s)\",\n",
    "        xticks=[0,abs(analysis_window[0]*monitor_fps)-1,n_datapoints-1],\n",
    "        xticklabels=([str(analysis_window[0]),'0', str(analysis_window[1])]),\n",
    "    )\n",
    "    # sns.move_legend(\n",
    "    #     axes[i], \"lower center\",\n",
    "    #     bbox_to_anchor=(.5, 1), ncol=3, title=None, frameon=False,\n",
    "    # )\n",
    "    i=i+1\n",
    "# fig_name = f\"constant_speed_constand_distance_speed1.jpg\"\n",
    "# fig.savefig(fig_name)\n",
    "# fig_name = f\"constant_speed_constand_distance_speed1.svg\"\n",
    "# fig.savefig(fig_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    nrows=1, ncols=2, figsize=(9,5), tight_layout=True\n",
    ")\n",
    "i=0\n",
    "for keys, this_data in ready_to_plot.groupby(['run_trial']):\n",
    "    print(keys)\n",
    "    #arg=lambda x: (x.min(), x.max())\n",
    "    sns.lineplot(x=\"frame_count\",\n",
    "                y=\"heading\",\n",
    "                #y='instant_angular_velocity',\n",
    "                hue=var1,estimator='median',\n",
    "                #errorbar=arg,\n",
    "                data=this_data,ax=axes[i])\n",
    "    axes[i].set(\n",
    "        xlabel=\"Time (s)\",\n",
    "        xticks=[0,abs(analysis_window[0]*monitor_fps)-1,n_datapoints-1],\n",
    "        xticklabels=([str(analysis_window[0]),'0', str(analysis_window[1])]),\n",
    "        # xticklabels=([str(-1),'0', str(2)]),\n",
    "        # xlim=[1*monitor_fps,4*monitor_fps],\n",
    "        # ylim=[-10,10]\n",
    "    )\n",
    "    sns.move_legend(\n",
    "        axes[i], \"upper left\", bbox_to_anchor=(1, 1)\n",
    "    )\n",
    "    i=i+1\n",
    "# fig_name = f\"constant_speed_constand_distance_angular_velocity1.jpg\"\n",
    "# fig.savefig(fig_name)\n",
    "# fig_name = f\"constant_speed_constand_distance_angular_velocity1.svg\"\n",
    "# fig.savefig(fig_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    nrows=2, ncols=1, figsize=(9,12), tight_layout=True\n",
    ")\n",
    "i=0\n",
    "for keys, this_data in ready_to_plot.groupby(['run_trial']):\n",
    "    print(keys)\n",
    "    #p1=np.reshape(this_data['instant_speed'].to_numpy(),(n_datapoints,-1))\n",
    "    p1=np.reshape(this_data['instant_speed'].to_numpy(),(-1,n_datapoints))\n",
    "    axes[i].plot(np.transpose(p1),linewidth=0.1)\n",
    "    mean_p1=np.nanmean(p1,axis=0)\n",
    "    axes[i].plot(mean_p1,'k',linewidth=1)\n",
    "    dif_y1,dif_y2=get_fill_between_range(p1,True,False)\n",
    "    \n",
    "    axes[i].fill_between(np.arange(n_datapoints),dif_y1,dif_y2, alpha=0.4,color='k')\n",
    "    axes[i].set_ylim([0,10])\n",
    "    axes[i].set(\n",
    "        xlabel=\"Time (s)\",\n",
    "        xticks=[0,abs(analysis_window[0]*monitor_fps)-1,n_datapoints-1],\n",
    "        xticklabels=([str(analysis_window[0]),'0', str(analysis_window[1])]),\n",
    "    )\n",
    "    i=i+1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    nrows=6, ncols=1, figsize=(9,18), tight_layout=True\n",
    ")\n",
    "i=0\n",
    "for keys, this_data in ready_to_plot.groupby([var1]):\n",
    "    print(keys)\n",
    "    #p1=np.reshape(this_data['instant_speed'].to_numpy(),(n_datapoints,-1))\n",
    "    p1=np.reshape(this_data['heading'].to_numpy(),(-1,n_datapoints))\n",
    "\n",
    "    axes[i].plot(np.transpose(p1),linewidth=0.1)\n",
    "    #mean_p1=np.median(p1,axis=0)\n",
    "    mean_p1=circmean(p1,high=180,low=-180,axis=0)\n",
    "    axes[i].plot(mean_p1,'k',linewidth=1)\n",
    "    dif_y1,dif_y2=get_fill_between_range(p1,False,True)\n",
    "    axes[i].fill_between(np.arange(n_datapoints),dif_y1,dif_y2, alpha=0.4,color='k')\n",
    "    axes[i].set_ylim([-10,10])\n",
    "    axes[i].set(\n",
    "        xlabel=\"Time (s)\",\n",
    "        xticks=[0,abs(analysis_window[0]*monitor_fps)-1,n_datapoints-1],\n",
    "        xticklabels=([str(analysis_window[0]),'0', str(analysis_window[1])]),\n",
    "    )\n",
    "    i=i+1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session 5: analyse time-series response with locustvr database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Session 5.0: introduce the function to sort the data into columns within time windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_raster_locustvr(focal_animal_file,summary_file,analysis_methods,count):\n",
    "    analysis_window=analysis_methods.get(\"analysis_window\")\n",
    "    camera_fps=analysis_methods.get(\"camera_fps\")\n",
    "    df_focal_animal = pd.read_hdf(focal_animal_file)\n",
    "    df_summary = pd.read_hdf(summary_file)\n",
    "    column_list = [\"labels\", \"elapsed_time\", \"instant_speed\", \"instant_angular_velocity\"]\n",
    "    X=df_focal_animal[\"X\"].to_numpy()\n",
    "    Y=df_focal_animal[\"Y\"].to_numpy()\n",
    "    #trial_id=df_focal_animal[\"trial_id\"].to_numpy()\n",
    "    state_type=df_focal_animal[\"state_type\"].to_numpy()\n",
    "    elapsed_time=df_focal_animal[\"ts\"].to_numpy()\n",
    "    #df_focal_animal[\"heading\"].to_csv(\"heading_angle.csv\", header=False, index=False)\n",
    "    rot_y=df_focal_animal[\"heading\"].to_numpy()\n",
    "    rot_y = interp_fill(rot_y)\n",
    "    unwrapped_rad = np.unwrap(rot_y)\n",
    "    ## chatGPT suggested to use \n",
    "    # degree_array = np.degrees(unwrapped_rad)\n",
    "    # instant_angular_velocity = np.gradient(degree_array, 0.01) \n",
    "    # filtered_velocity = medfilt(instant_angular_velocity, kernel_size=5)\n",
    "    # smoothed_velocity=gaussian_filter1d(filtered_velocity, sigma=2)\n",
    "    # but not sure\n",
    "    rad_diff = np.diff(unwrapped_rad)\n",
    "    instant_angular_velocity = rad_diff*camera_fps\n",
    "    filtered_delta_theta = medfilt(instant_angular_velocity, kernel_size=5)\n",
    "    delta_degrees = (180/np.pi) * filtered_delta_theta\n",
    "    # fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,5), tight_layout=True)\n",
    "    # ax1, ax2= axes.flatten()\n",
    "    # ax1.plot(filtered_delta_theta)\n",
    "    # #ax1.hist(filtered_delta_theta,bins=10000)\n",
    "    # ax2.plot(delta_degrees)\n",
    "    # #ax2.hist(delta_degrees,bins=10000)\n",
    "    # trajec_lim=360\n",
    "    # ax1.set(ylim=(-1 * trajec_lim, trajec_lim))\n",
    "    # ax2.set(ylim=(-1 * trajec_lim, trajec_lim))\n",
    "    instant_speed = calculate_speed(np.diff(X),np.diff(Y),elapsed_time,0)\n",
    "    start_idx=np.where(np.diff(state_type)>0)[0]+analysis_window[0]*camera_fps\n",
    "    start_big=np.repeat(start_idx,(analysis_window[1]-analysis_window[0])*camera_fps)\n",
    "    pts_tile=np.tile(np.arange((analysis_window[1]-analysis_window[0])*camera_fps),start_idx.shape[0])\n",
    "    label_tiles=np.repeat(df_summary['trial_label'].values,(analysis_window[1]-analysis_window[0])*camera_fps)\n",
    "    tmp = np.vstack(\n",
    "    (\n",
    "        label_tiles,\n",
    "        elapsed_time[start_big+pts_tile],\n",
    "        instant_speed[start_big+pts_tile],\n",
    "        #delta_degrees[start_big+pts_tile]\n",
    "        instant_angular_velocity[start_big+pts_tile]\n",
    "    ))\n",
    "    dynamics_pd = pd.DataFrame(np.transpose(tmp))\n",
    "    dynamics_pd.columns=column_list\n",
    "    dynamics_pd.insert(0, 'frame_count', pts_tile)\n",
    "    dynamics_pd.insert(0, 'animal_id',count)\n",
    "    return dynamics_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Session 5.1: load data from locustvr database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_analysis = analysis_methods.get(\"time_series_analysis\")\n",
    "analysis_methods.update({\"analysis_window\":[-2,5]})\n",
    "file_suffix = \"_full\" if time_series_analysis else \"\"\n",
    "agent_pattern = f\"agent{file_suffix}.h5\"\n",
    "xy_pattern = f\"XY{file_suffix}.h5\"\n",
    "summary_pattern = f\"summary{file_suffix}.h5\"\n",
    "pd_list=[]\n",
    "for index, this_dir in enumerate(dir_list):\n",
    "    agent_file = find_file(Path(this_dir), agent_pattern)\n",
    "    focal_animal_file = find_file(Path(this_dir), xy_pattern)\n",
    "    summary_file = find_file(Path(this_dir), summary_pattern)\n",
    "    this_pd=sort_raster_locustvr(focal_animal_file,summary_file,analysis_methods,index)\n",
    "    pd_list.append(this_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_to_plot=pd.concat(pd_list,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_to_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Session 5.2: plot the result with seaborn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_fps=analysis_methods.get(\"monitor_fps\")\n",
    "analysis_window=analysis_methods.get(\"analysis_window\")\n",
    "n_datapoints=(analysis_window[1]-analysis_window[0])*monitor_fps\n",
    "fig, (ax1,ax2) = plt.subplots(\n",
    "    nrows=2, ncols=1, figsize=(9,5), tight_layout=True\n",
    ")\n",
    "\n",
    "sns.lineplot(x=\"frame_count\", y=\"instant_angular_velocity\",hue=\"labels\",data=ready_to_plot,ax=ax1)\n",
    "ylimit=5\n",
    "ax1.set(\n",
    "    ylim=(-1 * ylimit, ylimit),\n",
    "    xlabel=\"Time (s)\",\n",
    "    xticks=[0,abs(analysis_window[0]*monitor_fps)-1,n_datapoints-1],\n",
    "    xticklabels=([str(analysis_window[0]),'0', str(analysis_window[1])]),\n",
    ")\n",
    "sns.lineplot(x=\"frame_count\", y=\"instant_speed\",hue=\"labels\",data=ready_to_plot,ax=ax2)\n",
    "ylimit=10\n",
    "ax2.set(\n",
    "    ylim=(0, ylimit),\n",
    "    xlabel=\"Time (s)\",\n",
    "    xticks=[0,abs(analysis_window[0]*monitor_fps)-1,n_datapoints-1],\n",
    "    xticklabels=([str(analysis_window[0]),'0', str(analysis_window[1])]),\n",
    ")\n",
    "    # sns.move_legend(\n",
    "    #     axes[i], \"lower center\",\n",
    "    #     bbox_to_anchor=(.5, 1), ncol=3, title=None, frameon=False,\n",
    "    # )\n",
    "# fig_name = f\"constant_speed_constand_distance_speed1.jpg\"\n",
    "# fig.savefig(fig_name)\n",
    "# fig_name = f\"constant_speed_constand_distance_speed1.svg\"\n",
    "# fig.savefig(fig_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matrexvr_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
