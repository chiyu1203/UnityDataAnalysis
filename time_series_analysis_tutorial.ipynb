{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a useful function to generate a data list for further analysis\n",
    "import os,json,sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from sorting_data_pairwise_comparison import calculate_relative_position,generate_points_within_rectangles\n",
    "import scipy.stats as st\n",
    "\n",
    "##need to add this additional cell because useful tools are in another folder. Need to integrate these two folders one day\n",
    "current_working_directory = Path.cwd()\n",
    "parent_dir = current_working_directory.resolve().parents[0]\n",
    "sys.path.insert(0, str(parent_dir) + \"\\\\utilities\")\n",
    "from useful_tools import select_animals_gpt,find_file\n",
    "from data_cleaning import findLongestConseqSubseq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Session 0.2: Load analysis methods in python dictionary form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = \"./analysis_methods_dictionary.json\"\n",
    "with open(json_file, \"r\") as f:\n",
    "    analysis_methods = json.loads(f.read())\n",
    "    \n",
    "#Put the folder of your Unity folder below\n",
    "#thisDataset =\"D:/MatrexVR_Swarm_Data/RunData\"\n",
    "#thisDataset =\"D:/MatrexVR_blackbackground_Data/RunData\"\n",
    "thisDataset =\"D:/MatrexVR_grass1_Data/RunData\"\n",
    "#thisDataset =\"D:/MatrexVR_2024_Data/RunData\"\n",
    "#parameter name means independent variable in the experiment\n",
    "variable_name='mu'\n",
    "#variable_name='initial_position'\n",
    "#variable_name='agent_speed'\n",
    "#check trace in trial 115 from VR1_2024-11-16_155242_score_full, maybe there is a jump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Session 1.0: select animals based on condition and return which a directory list and a list of vr rig number to specify which animal to analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your Excel file\n",
    "dir_list = []\n",
    "file_type=\".h5\"\n",
    "using_google_sheet=True\n",
    "sheet_name = \"Unity_MatrexVR\"\n",
    "experiment_name=analysis_methods.get(\"experiment_name\")\n",
    "# if type(thisDataset) == str:\n",
    "#     thisDataset = Path(thisDataset)\n",
    "if analysis_methods.get(\"load_individual_data\") == True:\n",
    "    if using_google_sheet==True:\n",
    "        # database_id = \"1UL4eEUrQMapx9xz11-IyOSlPBcep3I9vBJ2uGgVudb8\"\n",
    "        #         #https://docs.google.com/spreadsheets/d/1UL4eEUrQMapx9xz11-IyOSlPBcep3I9vBJ2uGgVudb8/edit?usp=sharing\n",
    "        # url = f\"https://docs.google.com/spreadsheets/d/{database_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
    "        database_id = \"1UL4eEUrQMapx9xz11-IyOSlPBcep1I9vBJ2uGgVudb8\"\n",
    "                #https://docs.google.com/spreadsheets/d/1UL4eEUrQMapx9xz11-IyOSlPBcep1I9vBJ2uGgVudb8/edit?usp=sharing\n",
    "        url = f\"https://docs.google.com/spreadsheets/d/{database_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
    "        #df = pd.read_excel(url, engine='openpyxl')## use this function if the file is not google sheet but uploaded excel file\n",
    "        df = pd.read_csv(url)\n",
    "    else:\n",
    "        excel_file_path = \"Z:/DATA/experiment_trackball_Optomotor/Locusts Management.xlsx\"\n",
    "        print(f\"using a database {excel_file_path} from the server but this file might be outdated\")\n",
    "        # Create a 'with' statement to open and read the Excel file\n",
    "        with pd.ExcelFile(excel_file_path) as xls:\n",
    "            # Read the Excel sheet into a DataFrame with the sheet name (folder name)\n",
    "            df = pd.read_excel(xls, sheet_name)\n",
    "        ##list up the conditions and answers as strings for input argument to select animal. One condition must pair with one answer\n",
    "    if analysis_methods.get(\"select_animals_by_condition\") == True:\n",
    "       #animal_of_interest=select_animals_gpt(df,\"Independent variable (list up all of them in the experiment)\",\"gregarious_leader_grass\",\"Excluding this animal from analysis (Usually when animals die or molt, T/F)\",\"F\")\n",
    "        #animal_of_interest=select_animals_gpt(df,\"Independent variable (list up all of them in the experiment)\",\"gregarious_leader_black\",\"Excluding this animal from analysis (Usually when animals die or molt, T/F)\",\"F\")\n",
    "        animal_of_interest=select_animals_gpt(df,\"Independent variable1\",variable_name,\"Excluding this animal from analysis (Usually when animals die or molt, T/F)\",\"F\")\n",
    "        #animal_of_interest=select_animals_gpt(df,\"Independent variable1\",variable_name,\"Independent variable2\",\"closed_loop_sta_black_locust_open_loop_sta_black_locust\",\"Excluding this animal from analysis (Usually when animals die or molt, T/F)\",\"F\")\n",
    "        #animal_of_interest=select_animals_gpt(df,\"Independent variable1\",variable_name,\"Independent variable2\",\"sta_black_locust_2dir_3_initial_position\",\"Excluding this animal from analysis (Usually when animals die or molt, T/F)\",\"F\")\n",
    "    else:\n",
    "        animal_of_interest=df\n",
    "    folder_name=animal_of_interest[\"folder name\"].values\n",
    "\n",
    "    dir_tile=np.tile(thisDataset, (len(folder_name), 1))\n",
    "    vr_no=animal_of_interest[\"VR number\"].values\n",
    "    dir_list = [''.join([x[0], '/', y]) for x,y in zip(dir_tile,folder_name)]\n",
    "    #dir_dict = itertools.zip_longest(dir_list, vr_no.tolist())\n",
    "    dir_dict = zip(dir_list, vr_no.tolist())\n",
    "else:\n",
    "    for root, dirs, files in os.walk(thisDataset):\n",
    "        for folder in dirs:\n",
    "            folder_path=os.path.join(root,folder)\n",
    "            if any(name.endswith(file_type) for name in os.listdir(folder_path)):\n",
    "                dir_list.append(folder_path.replace(\"\\\\\", \"/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list=dir_list[32:]\n",
    "vr_no=vr_no[32:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list=dir_list[:32]\n",
    "vr_no=vr_no[:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dir_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session 1.1: introduce helper functions to make plot and calculate speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.0: Pool animal's response together according to some criteria (from none criteria to criteria that can define the follow behaviour)\n",
    "Output1: a list 'follow_proportion_across_animals' showing the proportion of 'follow' time for each animal (across trials)\n",
    "\n",
    "Output2: a list 'relative_pos_all_animals' showing relative position between virtual and focal locusts across time. 1st and 2nd columns shown relative x and y, 3rd columns shown virtual animal's moving direction. 4th column shown the timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_methods.update({\"follow_locustVR_criteria\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_pos_all_animals=[]\n",
    "trial_evaluation_across_animals=[]\n",
    "raster_across_animals=[]\n",
    "animal_id=0\n",
    "for this_dir,this_vr in zip(dir_list,vr_no):\n",
    "    if Path(this_dir).is_dir()==False:\n",
    "        continue\n",
    "    agent_pattern = f\"VR{this_vr}*agent_full.h5\"\n",
    "    xy_pattern = f\"VR{this_vr}*XY_full.h5\"\n",
    "    summary_pattern = f\"VR{this_vr}*score_full.h5\"\n",
    "    agent_file = find_file(Path(this_dir), agent_pattern)\n",
    "    focal_animal_file = find_file(Path(this_dir), xy_pattern)\n",
    "    summary_file = find_file(Path(this_dir), summary_pattern)\n",
    "    relative_pos,trial_evaluation_list,raster_pd,num_unfilled_gap=calculate_relative_position(summary_file,focal_animal_file,agent_file,analysis_methods)\n",
    "    if animal_id==0:\n",
    "        largest_unfilled_gap=num_unfilled_gap\n",
    "    elif num_unfilled_gap>largest_unfilled_gap:\n",
    "        largest_unfilled_gap=num_unfilled_gap\n",
    "    else:\n",
    "        pass\n",
    "    relative_pos_all_animals.append(relative_pos)\n",
    "    trial_evaluation=pd.concat(trial_evaluation_list)\n",
    "    trial_evaluation.insert(0, 'animal_id',np.repeat(animal_id,trial_evaluation.shape[0]))\n",
    "    trial_evaluation_across_animals.append(trial_evaluation)\n",
    "    raster_pd.insert(0, 'animal_id', np.repeat(animal_id,raster_pd.shape[0]))\n",
    "    raster_across_animals.append(raster_pd)\n",
    "    animal_id=animal_id+1\n",
    "analysis_methods['largest_unfilled_gap']= largest_unfilled_gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trial_evaluation_across_animals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_evaluation=pd.concat(trial_evaluation_across_animals)\n",
    "##for grass background 1 agent\n",
    "# fair_follower_threshold=0.2\n",
    "# good_follower_threshold=0.3\n",
    "##for grass background 2 agent\n",
    "# fair_follower_threshold=0.1667\n",
    "# good_follower_threshold=0.25\n",
    "##for blackground\n",
    "# fair_follower_threshold=0.05\n",
    "# good_follower_threshold=0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chop the data into 3 groups depends on time. Then we can analyse the 1st, 2nd and 3rd of the data\n",
    "all_evaluation['time_group'] = pd.cut(all_evaluation['trial_id'], bins=3, labels=['1st', '2nd', '3rd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_follow_list=[]\n",
    "travel_distance_list=[]\n",
    "for keys, grp in all_evaluation.groupby(['animal_id','time_group']):\n",
    "    num_follow_list.append(grp[\"num_follow_epochs\"].sum())\n",
    "    travel_distance_list.append(grp[\"travel_distance\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_follow_arr=np.array(num_follow_list).reshape(-1,3)\n",
    "travel_distance_arr=np.array(travel_distance_list).reshape(-1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "        nrows=1, ncols=2, figsize=(9,10), tight_layout=True\n",
    "    )\n",
    "ax1, ax2 = axes.flatten()\n",
    "ax1.plot(np.transpose(num_follow_arr), 'o-')\n",
    "ax1.plot(np.mean(num_follow_arr,axis=0),'k',linewidth=5)\n",
    "ax1.plot(np.median(num_follow_arr,axis=0),'k',linewidth=5,alpha=0.6)\n",
    "ax2.plot(np.transpose(travel_distance_arr), 'o-')\n",
    "ax2.plot(np.mean(travel_distance_arr,axis=0),'k',linewidth=5)\n",
    "ax2.plot(np.median(travel_distance_arr,axis=0),'k',linewidth=5,alpha=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1D histogram to show the frequency of following response\n",
    "fig, axes = plt.subplots(\n",
    "        nrows=2, ncols=2, figsize=(9,10), tight_layout=True\n",
    "    )\n",
    "ax1, ax2, ax3, ax4 = axes.flatten()\n",
    "follow_time_aba=all_evaluation.groupby(['animal_id'])['num_follow_epochs'].sum()/all_evaluation.groupby(['animal_id'])['number_frames'].sum()\n",
    "follow_time_tbt=all_evaluation['num_follow_epochs']/all_evaluation['number_frames']\n",
    "print(\"first 1/3 best of followers:\", np.quantile(follow_time_aba, 0.66))\n",
    "print(\"middle 1/3 best of followers:\", np.quantile(follow_time_aba, 0.33))\n",
    "print(\"first 1/3 best of follow epochs:\", np.quantile(follow_time_tbt, 0.5))\n",
    "print(\"middle 1/3 best of follow epochs:\", np.quantile(follow_time_tbt, 0.33))\n",
    "fair_follower_threshold=np.quantile(follow_time_aba, 0.33)\n",
    "good_follower_threshold=np.quantile(follow_time_aba, 0.66)\n",
    "ax1.hist(follow_time_aba)\n",
    "ax1.set(xticks=[0,0.25,0.5,0.75,1],xticklabels=(['0', '25', '50', '75', '100']),xlim=(0,1),yticks=[0,20],ylim=(0,20),title='proportion of time following aba')\n",
    "ax2.hist(follow_time_tbt,density=True)\n",
    "ax2.set(xticks=[0,0.25,0.3,0.4,0.5,0.75,1],xticklabels=(['0', '25','30','40', '50', '75', '100']),xlim=(0,1),title='proportion of time following tbt')\n",
    "follower_of_interest=all_evaluation.groupby(['animal_id'])['num_follow_epochs'].sum()/all_evaluation.groupby(['animal_id'])['number_frames'].sum()<good_follower_threshold\n",
    "rows_of_follower=follower_of_interest.repeat(int(all_evaluation.shape[0]/follower_of_interest.shape[0]))\n",
    "ax3.hist(all_evaluation[rows_of_follower.values]['num_follow_epochs']/all_evaluation[rows_of_follower.values]['number_frames'],density=True)\n",
    "ax3.set(xticks=[0,0.25,0.5,0.75,1],xlim=(0,1),title='proportion of time from animals below threshold')\n",
    "follower_of_interest=all_evaluation.groupby(['animal_id'])['num_follow_epochs'].sum()/all_evaluation.groupby(['animal_id'])['number_frames'].sum()>good_follower_threshold\n",
    "rows_of_follower=follower_of_interest.repeat(int(all_evaluation.shape[0]/follower_of_interest.shape[0]))\n",
    "ax4.hist(all_evaluation[rows_of_follower.values]['num_follow_epochs']/all_evaluation[rows_of_follower.values]['number_frames'],density=True)\n",
    "ax4.set(xticks=[0,0.25,0.5,0.75,1],xlim=(0,1),title='proportion of time from animals above threshold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##plot the relationship between travel distance and proportion of following time\n",
    "# print(\"first 1/3 best of followers:\", np.quantile(follow_time_aba, 0.66))\n",
    "# print(\"middle 1/3 best of followers:\", np.quantile(follow_time_aba, 0.33))\n",
    "camera_fps=analysis_methods.get(\"camera_fps\")\n",
    "fair_follower_threshold=np.quantile(follow_time_aba, 0.33)\n",
    "good_follower_threshold=np.quantile(follow_time_aba, 0.66)\n",
    "glocust_list=[]\n",
    "exp_agent_list=[]\n",
    "fig, axes = plt.subplots(\n",
    "        nrows=2, ncols=2, figsize=(9,10), tight_layout=True\n",
    "    )\n",
    "ax1, ax2, ax3, ax4 = axes.flatten()\n",
    "ax1.set(xticks=[0,0.25,0.5,0.75,1],xticklabels=(['0', '25', '50', '75', '100']),xlim=(0,0.5))\n",
    "ax2.set(xticks=[0,0.25,0.5,0.75,1],xticklabels=(['0', '25', '50', '75', '100']),xlim=(0,1))\n",
    "for keys, this_data in all_evaluation.groupby(['animal_id']):\n",
    "    p_follow=this_data['num_follow_epochs'].sum()/this_data['number_frames'].sum()\n",
    "    if p_follow>good_follower_threshold:\n",
    "        color='k'\n",
    "    elif (p_follow>fair_follower_threshold) and (p_follow<good_follower_threshold):\n",
    "        color='k'\n",
    "    else:\n",
    "        color='k'\n",
    "    ax1.scatter(this_data['num_follow_epochs'].sum()/this_data['number_frames'].sum(),this_data['travel_distance'].sum()/camera_fps,c=color,s=8)\n",
    "    ax3.scatter(this_data['num_follow_epochs'].sum()/this_data['number_frames'].sum(),this_data['turning_distance'].sum()/camera_fps,c=color,s=8)\n",
    "    # if p_follow>good_follower_threshold:\n",
    "    #     pass\n",
    "    # elif (p_follow>fair_follower_threshold) and (p_follow<good_follower_threshold):\n",
    "    #     pass\n",
    "    # else:\n",
    "    #     continue\n",
    "\n",
    "    for this_object in this_data['object'].unique():\n",
    "        these_num_follow_epochs=this_data['num_follow_epochs'][this_data['object']==this_object]\n",
    "        these_num_frames=this_data['number_frames'][this_data['object']==this_object]\n",
    "        these_travel_distance=this_data['travel_distance'][this_data['object']==this_object]\n",
    "        these_turning_distance=this_data['turning_distance'][this_data['object']==this_object]\n",
    "        if this_object=='mov_glocust' or this_object== \"LeaderLocust\":\n",
    "            # ax2.scatter(these_num_follow_epochs/these_num_frames,these_travel_distance,c=color,s=8)\n",
    "            # ax4.scatter(these_num_follow_epochs/these_num_frames,these_turning_distance,c=color,s=8)\n",
    "            ax2.scatter(these_num_follow_epochs/these_num_frames,these_travel_distance,c='b',s=8)\n",
    "            ax4.scatter(these_num_follow_epochs/these_num_frames,these_turning_distance,c='b',s=8)\n",
    "            follow_time_ratio_tbt=these_num_follow_epochs/these_num_frames\n",
    "            glocust_list.append(follow_time_ratio_tbt.values)\n",
    "        else:\n",
    "            # ax2.scatter(these_num_follow_epochs/these_num_frames,these_travel_distance,edgecolors=color,marker='o', facecolors='none',s=8)\n",
    "            # ax4.scatter(these_num_follow_epochs/these_num_frames,these_turning_distance,edgecolors=color,marker='o', facecolors='none',s=8)\n",
    "            ax2.scatter(these_num_follow_epochs/these_num_frames,these_travel_distance,c='r',s=8)\n",
    "            ax4.scatter(these_num_follow_epochs/these_num_frames,these_turning_distance,c='r',s=8)\n",
    "            follow_time_ratio_tbt=these_num_follow_epochs/these_num_frames\n",
    "            exp_agent_list.append(follow_time_ratio_tbt.values)\n",
    "ax3.set(xticks=[0,0.25,0.5,0.75,1],xticklabels=(['0', '25', '50', '75', '100']),xlim=(0,0.5))\n",
    "ax4.set(xticks=[0,0.25,0.5,0.75,1],xticklabels=(['0', '25', '50', '75', '100']),xlim=(0,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise_as_csv(input_list,agent_type='glocust'):\n",
    "    df=pd.DataFrame(np.hstack(input_list))\n",
    "    df.insert(0, 'agent_type', np.repeat(agent_type,df.shape[0]).tolist())\n",
    "    df.insert(0, 'ID', np.repeat(np.arange(len(input_list)),int(df.shape[0]/len(input_list))).tolist())\n",
    "    df.to_csv(f'{agent_type}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarise_as_csv(exp_agent_list,'exp_agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarise_as_csv(exp_agent_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 2.1: Using 1D histogram to plot proportion of time and 2D histogram to plot relative position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trials=pd.concat(relative_pos_all_animals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_follow_epoches=analysis_methods.get(\"extract_follow_epoches\",True)\n",
    "distribution_with_entire_body=analysis_methods.get(\"distribution_with_entire_body\",True)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(all_trials[\"type\"].unique())*len(all_trials[\"degree\"].unique()),figsize=(18, 6),tight_layout=True,sharex=True, sharey=True)\n",
    "i=0\n",
    "if extract_follow_epoches:\n",
    "    xlimit=(-5,40)\n",
    "    ylimit=(-15,15)\n",
    "else:\n",
    "    xlimit=(-20,100)\n",
    "    ylimit=(-45,45)\n",
    "for keys, grp in all_trials.groupby(['type','degree']):\n",
    "        if distribution_with_entire_body:\n",
    "            body_points=generate_points_within_rectangles(grp['x'].values,grp['y'].values,1,4,2,21)\n",
    "            axes[i].hist2d(body_points[:,0],body_points[:,1],bins=1000)\n",
    "        else:\n",
    "            axes[i].hist2d(grp['x'].values,grp['y'].values,bins=400)\n",
    "        axes[i].set(\n",
    "        yticks=[ylimit[0],0,ylimit[1]],\n",
    "        xticks=[-5,0,4,20,40],\n",
    "        xlim=xlimit,ylim=ylimit,title=f'agent:{keys[0]},deg:{int(keys[1])}',adjustable='box', aspect='equal')\n",
    "        #rect = patches.Rectangle((0,-1), 6, 2, linewidth=1, edgecolor='red',linestyle=\"--\",facecolor='none')\n",
    "        #cir1 = patches.Circle((4,0), radius=0.5, linewidth=0.5, edgecolor='yellow')\n",
    "        cir2 = patches.Circle((0,0), radius=0.5, linewidth=0.5, edgecolor='red')\n",
    "        #axes[i].add_patch(cir1)\n",
    "        axes[i].add_patch(cir2)\n",
    "        i=i+1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##plot the distribution of follow epochs in a trial\n",
    "for keys, grp in all_trials.groupby(['type','degree']):\n",
    "    print(keys)\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=1, ncols=2, figsize=(9,5), tight_layout=True\n",
    "    )\n",
    "    ax, ax2 = axes.flatten()\n",
    "    ax.hist(grp['ts'].values,bins=100,density=True)\n",
    "    ax2.hist(grp['ts'].values,bins=100,density=True,histtype=\"step\",cumulative=True,label=\"Cumulative histogram\")\n",
    "    ax.set(xlim=(0,60),ylim=(0,0.05),title=f'agent:{keys[0]},deg:{int(keys[1])}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "#https://stackoverflow.com/questions/78384537/fitting-2d-histograms-with-2d-gaussians\n",
    "def twoD_Gaussian(xy, amplitude, xo, yo, sigma_x, sigma_y, rho):\n",
    "    x, y = xy\n",
    "    xo = float(xo)\n",
    "    yo = float(yo)\n",
    "    a = 1 / sigma_x ** 2\n",
    "    b = rho / sigma_x / sigma_y\n",
    "    c = 1 / sigma_y ** 2\n",
    "    exponent=  - (1 / (2 * (1 - rho ** 2)))\\\n",
    "                 * (a * (x - xo) ** 2 - 2 * b * (x - xo) * (y - yo) + c * (y - yo) ** 2)\n",
    "    g = amplitude * np.exp(exponent)\n",
    "    return g.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbr_bins = 100\n",
    "B, xedges, yedges = np.histogram2d(grp['x'].values,grp['y'].values, bins=nbr_bins)\n",
    "extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n",
    "bin_centers_bx = (xedges[:-1] + xedges[1:]) / 2.0\n",
    "bin_centers_by = (yedges[:-1] + yedges[1:]) / 2.0\n",
    "X, Y = np.meshgrid(bin_centers_bx, bin_centers_by)\n",
    "data = B.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0, 4]\n",
    "p0 = (B.max(),mean[0],mean[1],1.7,1.0,0.9)\n",
    "coeff, var_matrix = curve_fit(twoD_Gaussian, (X, Y), data, p0=p0)\n",
    "print('hist fit', coeff)\n",
    "print('var_matrix', var_matrix)\n",
    "data_fitted_hist = twoD_Gaussian((X, Y), *coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "xlimit=(-30,30)\n",
    "ylimit=(-30,30)\n",
    "ax[0].imshow(data.reshape((nbr_bins, nbr_bins)), origin=\"lower\",\n",
    "             extent=extent, interpolation=\"none\")\n",
    "ax[1].imshow(data.reshape((nbr_bins, nbr_bins)), origin=\"lower\",\n",
    "             extent=extent, interpolation=\"none\")\n",
    "ax[1].contour(X, Y, data_fitted_hist.reshape(nbr_bins, nbr_bins), 2, colors=\"w\",linestyles='dashed',linewidths=0.5)\n",
    "ax[0].set(\n",
    "        yticks=[-30,0.0,30],\n",
    "        xticks=[-30,0.0,30],\n",
    "        xlim=xlimit,ylim=ylimit)\n",
    "ax[1].set(\n",
    "        yticks=[-30,0.0,30],\n",
    "        xticks=[-30,0.0,30],\n",
    "        xlim=xlimit,ylim=ylimit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_methods.update({\"distribution_with_entire_body\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##plot the spatial distribution of virtual locusts in a 2D histogram\n",
    "\n",
    "distribution_with_entire_body=analysis_methods.get(\"distribution_with_entire_body\",True)\n",
    "xlimit=(-30,30)\n",
    "ylimit=(-30,30)\n",
    "for keys, grp in all_trials.groupby(['type']):\n",
    "    fig, ax = plt.subplots(dpi=300, figsize=(2,2))\n",
    "    if distribution_with_entire_body:\n",
    "        body_points=generate_points_within_rectangles(grp['x'].values,grp['y'].values,1,4,2,21)\n",
    "        ax.hist2d(body_points[:,0],body_points[:,1],bins=1000)\n",
    "    else:\n",
    "        ax.hist2d(grp['x'].values,grp['y'].values,bins=100)\n",
    "    ax.set(adjustable='box', aspect='equal')\n",
    "    ax.set(\n",
    "        yticks=[-30,0.0,30],\n",
    "        xticks=[-30,0.0,30],\n",
    "        xticklabels=(['-0.3', '0.0', '0.3']),\n",
    "        yticklabels=(['-0.3', '0.0', '0.3']),\n",
    "        xlim=xlimit,ylim=ylimit,title=f'agent:{keys[0]}')\n",
    "    '''\n",
    "    the position of virtual locust if their butt is at the origin\n",
    "    rect = patches.Rectangle((0,-1), 6, 2, linewidth=1, edgecolor='red',linestyle=\"--\",facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    '''\n",
    "    #cir = patches.Circle((0,0), radius=0.5, linewidth=0.5, edgecolor='red')\n",
    "    #ax.add_patch(cir)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##plot the temporal distribution of follow epochs in a trial\n",
    "for keys, grp in all_trials.groupby(['type']):\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=1, ncols=2, figsize=(9,5), tight_layout=True\n",
    "    )\n",
    "    ax, ax2 = axes.flatten()\n",
    "    ax.hist(grp['ts'].values,bins=100,density=True)\n",
    "    ax2.hist(grp['ts'].values,bins=100,density=True,histtype=\"step\",cumulative=True,label=\"Cumulative histogram\")\n",
    "    ax.set(xlim=(0,60))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 2.2: save data into Mat file if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##save the raw data into mat file for matlab\n",
    "from scipy.io import savemat\n",
    "all_raw=pd.concat(relative_pos_all_animals)\n",
    "#all_raw['trial_id']=all_raw['trial_id'].astype(int)\n",
    "data_dict = {name: col.values for name, col in all_raw.items()}\n",
    "summary_file_name = Path(thisDataset) /\"time_series_curated.mat\"\n",
    "savemat(summary_file_name, data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.0, analysis walking behavours before and after the presence of stimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_data_type(all_trials):\n",
    "    all_trials['id'] = all_trials['id'].astype(int)\n",
    "    all_trials['mu'] = all_trials['mu'].astype(int)\n",
    "    all_trials['velocity'] = all_trials['velocity'].astype(float)\n",
    "    all_trials['omega'] = all_trials['omega'].astype(float)\n",
    "    all_trials['normalised_v'] = all_trials['normalised_v'].astype(float)\n",
    "    all_trials['normalised_omega'] = all_trials['normalised_omega'].astype(float)\n",
    "    if 'density' in all_trials.columns:\n",
    "        all_trials['density'] = all_trials['density'].astype(int)\n",
    "    return all_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(raster_across_animals)==list:\n",
    "    all_trials=pd.concat(raster_across_animals)\n",
    "    all_trials=fix_data_type(all_trials)\n",
    "else:\n",
    "    all_trials=fix_data_type(raster_across_animals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_baseline_distribution(all_trials,analysis_methods,metrics_name='velocity',duration_for_baseline=3):\n",
    "    monitor_fps=analysis_methods.get(\"monitor_fps\")\n",
    "    align_with_isi_onset=analysis_methods.get(\"align_with_isi_onset\",False)\n",
    "    these_baselines=[]\n",
    "    for keys, this_data in all_trials.groupby(['animal_id','id']):\n",
    "        #print(this_data['object'][1])\n",
    "        if align_with_isi_onset:\n",
    "            if this_data['object'][1]!='empty_trial':\n",
    "            #if keys[1]%2==0:#for the Swarm scene to only use stim trials to get the baseline\n",
    "                this_metrics=this_data[metrics_name].values\n",
    "                #print(this_metrics)\n",
    "                these_baselines.append(np.mean(this_metrics[-duration_for_baseline*monitor_fps:]))\n",
    "        \n",
    "        else:\n",
    "            if this_data['object'][1]=='empty_trial':\n",
    "            #if keys[1]%2!=0:#for the Swarm scene to only use ISI trials\n",
    "                this_metrics=this_data[metrics_name].values\n",
    "                these_baselines.append(np.mean(this_metrics[-duration_for_baseline*monitor_fps:]))\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=1, ncols=1, figsize=(9,9), tight_layout=True\n",
    "    )\n",
    "    axes.hist(np.vstack(these_baselines),bins=500)\n",
    "    if metrics_name=='velocity':\n",
    "        axes.set(xlim=(0,10),ylim=(0, 250))\n",
    "    else:\n",
    "        #axes.set(xlim=(-0.001,0.001),ylim=(0, 60))#if used rad\n",
    "        axes.set(xlim=(-3,3),ylim=(0, 400))\n",
    "\n",
    "    plt.minorticks_on()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_name='omega'\n",
    "check_baseline_distribution(all_trials,analysis_methods,metrics_name,3)\n",
    "# check_baseline_distribution(all_trials,analysis_methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_trials(analysis_methods,all_trials,metrics_name='velocity',metrics_name2='normalised_v',walk_threshold=1,duration_for_baseline=3):\n",
    "    monitor_fps=analysis_methods.get(\"monitor_fps\")\n",
    "    movement_trial_boolean=[]\n",
    "    these_metrics=[]\n",
    "    these_normalised_metrics=[]\n",
    "    for keys, this_data in all_trials.groupby(['animal_id','id']):\n",
    "        this_metrics=this_data[metrics_name].values\n",
    "        baseline_metrics=np.mean(this_metrics[-duration_for_baseline*monitor_fps:])\n",
    "        movement_trial_boolean.append(abs(baseline_metrics)>walk_threshold)\n",
    "        these_metrics.append(this_metrics)\n",
    "        these_normalised_metrics.append(this_data[metrics_name2].values)\n",
    "    return movement_trial_boolean,these_metrics,these_normalised_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for simplicity, classifying trial only based on stationary and walk trails\n",
    "metrics_name='omega'\n",
    "_,these_metrics,these_normalised_metrics=split_trials(analysis_methods,all_trials,metrics_name,'normalised_omega',1)#1 degree or 0.0002 rad\n",
    "movement_trial_boolean,_,_=split_trials(analysis_methods,all_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#movement_trial_boolean,these_metrics,these_normalised_metrics=split_trials(analysis_methods,all_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_trial_index(movement_trial_boolean,analysis_methods):\n",
    "    align_with_isi_onset=analysis_methods.get(\"align_with_isi_onset\",False)\n",
    "#int((trial_id)/2) means the number of stimulus trial\n",
    "    after_movement_ith_trial=[]\n",
    "    after_no_movement_ith_trial=[]\n",
    "    if align_with_isi_onset:\n",
    "        after_movement_ith_trial=[i+1 for i, x in enumerate(movement_trial_boolean[1::2]) if x and i % int((trial_id)/2) != (trial_id)/2-1]\n",
    "        after_no_movement_ith_trial=[i+1 for i, x in enumerate(movement_trial_boolean[1::2]) if x==False and i % int((trial_id)/2) != (trial_id)/2-1]\n",
    "    else:\n",
    "        after_movement_ith_trial=[i for i, x in enumerate(movement_trial_boolean[::2]) if x]\n",
    "        after_no_movement_ith_trial=[i for i, x in enumerate(movement_trial_boolean[::2]) if x==False]\n",
    "    return after_movement_ith_trial,after_no_movement_ith_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_movement_ith_trial,after_no_movement_ith_trial=extract_trial_index(movement_trial_boolean,analysis_methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fill_between_range(data,mean_data,using_confidence_interval=True):\n",
    "\n",
    "    ##to plot distribution with 95% confidence interval with t distribution (since the sample is usually not big)\n",
    "    if using_confidence_interval:\n",
    "        confidence_level = 0.95\n",
    "        #cl95=st.t.interval(confidence=0.95, df=len(data)-1, loc=np.mean(data), scale=st.sem(data)) \n",
    "        cl95=st.norm.interval(confidence_level,loc=mean_data,scale=st.sem(data))\n",
    "        dif_y1=cl95[0][:]\n",
    "        dif_y2=cl95[1][:]\n",
    "    else:\n",
    "        sem_response = np.std(data, axis=0, ddof=1) / np.sqrt(data.shape[0])\n",
    "        dif_y1=mean_data + sem_response\n",
    "        dif_y2=mean_data - sem_response\n",
    "    return dif_y1,dif_y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visual_evoked_behaviour(these_metrics,these_normalised_metrics,after_movement_ith_trial,after_no_movement_ith_trial,analysis_methods,metrics_name='velocity',row_of_interest=None,type_key=\"\",variable_values=None):\n",
    "    exp_name=analysis_methods.get('experiment_name')\n",
    "    number_frame_scene_changing=analysis_methods.get(\"largest_unfilled_gap\",12)\n",
    "    number_frame_scene_changing=10#set an arbitrary value to escape from the effect of missing value when plotting the histogram\n",
    "    align_with_isi_onset=analysis_methods.get(\"align_with_isi_onset\",False)\n",
    "    save_output=analysis_methods.get(\"save_output\",False)\n",
    "    analysis_window=analysis_methods.get(\"analysis_window\")\n",
    "    monitor_fps=analysis_methods.get(\"monitor_fps\")\n",
    "    camera_fps=analysis_methods.get(\"camera_fps\")\n",
    "    all_animals=False\n",
    "    tmp=np.vstack(these_metrics)\n",
    "    tmp3=np.vstack(these_normalised_metrics)\n",
    "    if align_with_isi_onset:\n",
    "       stim_evoked_metrics=tmp[::2]\n",
    "       stim_evoked_norm_metrics=tmp3[::2]\n",
    "    else:\n",
    "       stim_evoked_metrics=tmp[1::2]\n",
    "       stim_evoked_norm_metrics=tmp3[1::2]\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "            nrows=4, ncols=2, figsize=(9,10), tight_layout=True\n",
    "        )\n",
    "    ax1, ax2, ax3, ax4,ax5,ax6,ax7,ax8 = axes.flatten()\n",
    "    plt.rcParams.update(plt.rcParamsDefault)\n",
    "    plt.rcParams.update({'font.size': 8})\n",
    "    # Set the axis line width to 2\n",
    "    plt.rcParams['ytick.major.width'] = 1\n",
    "    plt.rcParams['xtick.major.width'] = 1\n",
    "    plt.rcParams['axes.linewidth'] = 1\n",
    "    cmap = plt.get_cmap('viridis')\n",
    "    if all_animals==False and type(row_of_interest)==pd.Series:\n",
    "        animal_interest_stationary=[]\n",
    "        animal_interest_move=[]\n",
    "        for i in np.where(row_of_interest)[0].tolist():\n",
    "            if i in after_no_movement_ith_trial:\n",
    "                animal_interest_stationary.append(i)\n",
    "            else:\n",
    "                animal_interest_move.append(i)\n",
    "        p3=stim_evoked_metrics[animal_interest_stationary,:]\n",
    "        p4=stim_evoked_metrics[animal_interest_move,:]\n",
    "        p5=stim_evoked_norm_metrics[animal_interest_stationary,:]\n",
    "        p6=stim_evoked_norm_metrics[animal_interest_move,:]\n",
    "\n",
    "    else:\n",
    "        p3=stim_evoked_metrics[after_no_movement_ith_trial,:]\n",
    "        p4=stim_evoked_metrics[after_movement_ith_trial,:]\n",
    "        p5=stim_evoked_norm_metrics[after_no_movement_ith_trial,:]\n",
    "        p6=stim_evoked_norm_metrics[after_movement_ith_trial,:]\n",
    "    x=np.arange(0,p3.shape[1])\n",
    "    p1=np.nancumsum(p3, axis=1)/camera_fps\n",
    "    p2=np.nancumsum(p4, axis=1)/camera_fps\n",
    "    ax1.plot(np.transpose(p1),linewidth=0.1)\n",
    "    mean_p1=np.nanmean(p1,axis=0)\n",
    "    ax1.plot(mean_p1,'k',linewidth=1)\n",
    "    #ax1.plot(np.nanmedian(p1,axis=0),'k--',linewidth=0.5)\n",
    "    dif_y1,dif_y2=get_fill_between_range(p1,mean_p1)\n",
    "    ax1.fill_between(x,dif_y1,dif_y2, alpha=0.4,color='k')\n",
    "    ax2.plot(np.transpose(p2),linewidth=0.1)\n",
    "    mean_p2=np.nanmean(p2,axis=0)\n",
    "\n",
    "    ax2.plot(mean_p2,'k',linewidth=1)\n",
    "    #ax2.plot(np.nanmedian(p2,axis=0),'k--',linewidth=0.5)\n",
    "    dif_y1,dif_y2=get_fill_between_range(p2,mean_p2)\n",
    "    ax2.fill_between(x,dif_y1,dif_y2, alpha=0.4,color='k')\n",
    "    ax3.plot(np.transpose(p3),linewidth=0.1)\n",
    "    mean_p3=np.nanmean(p3,axis=0)\n",
    "    ax3.plot(mean_p3,'k',linewidth=1)\n",
    "    dif_y1,dif_y2=get_fill_between_range(p3,mean_p3)\n",
    "    ax3.fill_between(x,dif_y1,dif_y2, alpha=0.4,color='k')\n",
    "    #ax3.plot(np.nanmedian(p3,axis=0),'k--',linewidth=0.5)\n",
    "    mean_p4=np.nanmean(p4,axis=0)\n",
    "    dif_y1,dif_y2=get_fill_between_range(p4,mean_p4)\n",
    "    ax4.plot(np.transpose(p4),linewidth=0.1)\n",
    "    ax4.plot(mean_p4,'k',linewidth=1)\n",
    "    ax4.fill_between(x,dif_y1,dif_y2, alpha=0.4,color='k')\n",
    "    #ax4.plot(np.nanmedian(p4,axis=0),'k--',linewidth=0.5)\n",
    "    mean_p5=np.nanmean(p5,axis=0)\n",
    "    dif_y1,dif_y2=get_fill_between_range(p5,mean_p5)\n",
    "    ax5.plot(np.transpose(p5),linewidth=0.1)\n",
    "    ax5.plot(mean_p5,'k',linewidth=1)\n",
    "    ax5.fill_between(x,dif_y1,dif_y2, alpha=0.4,color='k')\n",
    "    #ax5.plot(np.nanmedian(p5,axis=0),'k--',linewidth=0.5)\n",
    "    mean_p6=np.nanmean(p6,axis=0)\n",
    "    dif_y1,dif_y2=get_fill_between_range(p6,mean_p6)\n",
    "    ax6.plot(np.transpose(p6),linewidth=0.1)\n",
    "    ax6.plot(mean_p6,'k',linewidth=1)\n",
    "    ax6.fill_between(x,dif_y1,dif_y2,alpha=0.4,color='k')\n",
    "\n",
    "    #ax6.plot(np.nanmedian(p6,axis=0),'k--',linewidth=0.5)\n",
    "    if metrics_name=='velocity' and all_animals==False:\n",
    "        ylimit=10\n",
    "        ylimit_log=100\n",
    "        ax1.set_ylim([0,5*ylimit])\n",
    "        ax2.set_ylim([0,5*ylimit])\n",
    "        ax3.set_ylim([0,1*ylimit])\n",
    "        ax4.set_ylim([0,1*ylimit])\n",
    "    elif all_animals==False:\n",
    "        ylimit=80\n",
    "        ylimit_log=1000\n",
    "        # ax1.yaxis.set_major_locator(plt.MultipleLocator(np.pi / 2))\n",
    "        # ax1.yaxis.set_major_formatter(plt.FuncFormatter(multiple_formatter()))\n",
    "        # ax2.yaxis.set_major_locator(plt.MultipleLocator(np.pi / 2))\n",
    "        # ax2.yaxis.set_major_formatter(plt.FuncFormatter(multiple_formatter()))\n",
    "        # ax3.yaxis.set_major_locator(plt.MultipleLocator(np.pi / 2))\n",
    "        # ax3.yaxis.set_major_formatter(plt.FuncFormatter(multiple_formatter()))\n",
    "        # ax4.yaxis.set_major_locator(plt.MultipleLocator(np.pi / 2))\n",
    "        # ax4.yaxis.set_major_formatter(plt.FuncFormatter(multiple_formatter()))\n",
    "        ax1.set_ylim([-1*ylimit,1*ylimit])\n",
    "        ax2.set_ylim([-1*ylimit,1*ylimit])\n",
    "        ax3.set_ylim([-1*ylimit,1*ylimit])\n",
    "        ax4.set_ylim([-1*ylimit,1*ylimit])\n",
    "    elif metrics_name=='velocity':\n",
    "        ylimit=10\n",
    "        ylimit_log=100\n",
    "        ax1.set_ylim([0,5*ylimit])\n",
    "        ax2.set_ylim([0,5*ylimit])\n",
    "        ax3.set_ylim([0,1*ylimit])\n",
    "        ax4.set_ylim([0,1*ylimit])\n",
    "    else:\n",
    "        ylimit=80\n",
    "        ylimit_log=1000\n",
    "        # ax1.yaxis.set_major_locator(plt.MultipleLocator(np.pi / 2))\n",
    "        # ax1.yaxis.set_major_formatter(plt.FuncFormatter(multiple_formatter()))\n",
    "        # ax2.yaxis.set_major_locator(plt.MultipleLocator(np.pi / 2))\n",
    "        # ax2.yaxis.set_major_formatter(plt.FuncFormatter(multiple_formatter()))\n",
    "        # ax3.yaxis.set_major_locator(plt.MultipleLocator(np.pi / 2))\n",
    "        # ax3.yaxis.set_major_formatter(plt.FuncFormatter(multiple_formatter()))\n",
    "        # ax4.yaxis.set_major_locator(plt.MultipleLocator(np.pi / 2))\n",
    "        # ax4.yaxis.set_major_formatter(plt.FuncFormatter(multiple_formatter()))\n",
    "        ax1.set_ylim([-1*ylimit,1*ylimit])\n",
    "        ax2.set_ylim([-1*ylimit,1*ylimit])\n",
    "        ax3.set_ylim([-1*ylimit,1*ylimit])\n",
    "        ax4.set_ylim([-1*ylimit,1*ylimit])\n",
    "    ax1.set(\n",
    "        ylabel=f\"sum of {metrics_name}\",\n",
    "        #xlabel=\"frame\",\n",
    "        xlabel=\"Time (s)\",\n",
    "        xticks=[0,analysis_window[1]*monitor_fps],\n",
    "        xticklabels=(['0', str(analysis_window[1])]),\n",
    "    )\n",
    "    ax2.set(\n",
    "        ylabel=f\"sum of {metrics_name}\",\n",
    "        #xlabel=\"frame\",\n",
    "        xlabel=\"Time (s)\",\n",
    "        xticks=[0,analysis_window[1]*monitor_fps],\n",
    "        xticklabels=(['0', str(analysis_window[1])]),\n",
    "    )\n",
    "    ax3.set(\n",
    "        ylabel=metrics_name,\n",
    "        #xlabel=\"frame\",\n",
    "        xlabel=\"Time (s)\",\n",
    "        xticks=[0,analysis_window[1]*monitor_fps],\n",
    "        xticklabels=(['0', str(analysis_window[1])]),\n",
    "    )\n",
    "    ax4.set(\n",
    "        ylabel=metrics_name,\n",
    "        #xlabel=\"frame\",\n",
    "        xlabel=\"Time (s)\",\n",
    "        xticks=[0,analysis_window[1]*monitor_fps],\n",
    "        xticklabels=(['0', str(analysis_window[1])]),\n",
    "    )\n",
    "    ax5.set(\n",
    "        ylabel=\"normalised values (ratio)\",\n",
    "        xlabel=\"frame\",\n",
    "        ylim=[1/ylimit_log,ylimit_log],\n",
    "    )\n",
    "    ax6.set(\n",
    "        ylabel=\"normalised values (ratio)\",\n",
    "        xlabel=\"frame\",\n",
    "        ylim=[1/ylimit_log,ylimit_log],\n",
    "    )\n",
    "    # if metrics_name=='velocity':\n",
    "    #     peak_distribution_stationary=np.argmax(p3, axis=1)\n",
    "    #     peak_distribution_move=np.argmax(p4, axis=1)\n",
    "    # else:\n",
    "    peak_distribution_stationary=np.argmax(abs(p3[:,number_frame_scene_changing:]), axis=1)\n",
    "    peak_distribution_stationary=peak_distribution_stationary+number_frame_scene_changing\n",
    "    \n",
    "    #print(peak_distribution_stationary.shape)\n",
    "    peak_distribution_move=np.argmax(abs(p4[:,number_frame_scene_changing:]), axis=1)\n",
    "    peak_distribution_move=peak_distribution_move+number_frame_scene_changing\n",
    "    #print(peak_distribution_move.shape)\n",
    "    ax5.set_yscale('log')\n",
    "    ax6.set_yscale('log')\n",
    "    ax7.hist(abs(peak_distribution_stationary),bins=10)\n",
    "    ax7.set(\n",
    "        ylabel=\"Count\",\n",
    "        xlabel=\"Time (s)\",\n",
    "        xticks=[0,analysis_window[1]*monitor_fps],\n",
    "        xticklabels=(['0', str(analysis_window[1])]),\n",
    "    )\n",
    "    ax8.hist(abs(peak_distribution_move),bins=10)\n",
    "    ax8.set(\n",
    "        ylabel=\"Count\",\n",
    "        xlabel=\"Time (s)\",\n",
    "        xticks=[0,analysis_window[1]*monitor_fps],\n",
    "        xticklabels=(['0', str(analysis_window[1])]),\n",
    "    )\n",
    "    if save_output==True:\n",
    "        fig_name=f\"ts_plot_{exp_name}_{variable_values}_{metrics_name}_{type_key}.png\"\n",
    "        fig.savefig(fig_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_methods.update({\"save_output\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "##criteria to sort out data from good followers\n",
    "# fair_follower_threshold=0.20\n",
    "#fair_follower_threshold=0.1667\n",
    "# good_follower_threshold=0.3\n",
    "#data from grass1 secene\n",
    "# first 1/3 best of followers: 0.04542389707280022\n",
    "# middle 1/3 best of followers: 0.024365961537554066\n",
    "use_follower_threshold=True\n",
    "if use_follower_threshold==True:\n",
    "    p_follow=all_evaluation.groupby(['animal_id'])['num_follow_epochs'].sum()/all_evaluation.groupby(['animal_id'])['number_frames'].sum()\n",
    "    #follower_of_interest=p_follow>good_follower_threshold\n",
    "    #follower_of_interest=(p_follow>fair_follower_threshold) & (p_follow<good_follower_threshold)\n",
    "    follower_of_interest=p_follow>0.025\n",
    "    #rows_of_follower=follower_of_interest.repeat(int(all_evaluation.shape[0]/follower_of_interest.shape[0]))\n",
    "else:\n",
    "    #good_trial_threshold=0.125\n",
    "    good_trial_threshold=0.125\n",
    "    long_following_epochs=all_evaluation['num_follow_epochs']/all_evaluation['number_frames']>good_trial_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if all_evaluation['object'].unique().shape[0]>1:\n",
    "for type_key in all_evaluation['radial_distance'].unique():\n",
    "    print(type_key)\n",
    "    rows_of_follower=follower_of_interest.repeat(int(all_evaluation.shape[0]/follower_of_interest.shape[0]))\n",
    "    for this_condition in all_evaluation['mu'].unique():\n",
    "        print(this_condition)\n",
    "        if use_follower_threshold==True:\n",
    "            row_of_interest=(all_evaluation['mu'].reset_index(drop=True)==this_condition)&(all_evaluation['radial_distance'].reset_index(drop=True)==type_key) &(rows_of_follower.reset_index(drop=True))\n",
    "        else:\n",
    "            row_of_interest=(all_evaluation['mu'].reset_index(drop=True)==this_condition)&(all_evaluation['radial_distance'].reset_index(drop=True)==type_key)&(long_following_epochs.reset_index(drop=True))\n",
    "        plot_visual_evoked_behaviour(these_metrics,these_normalised_metrics,after_movement_ith_trial,after_no_movement_ith_trial,analysis_methods,metrics_name,row_of_interest,type_key,this_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if all_evaluation['object'].unique().shape[0]>1:\n",
    "for type_key in all_evaluation['object'].unique():\n",
    "    print(type_key)\n",
    "    rows_of_follower=follower_of_interest.repeat(int(all_evaluation.shape[0]/follower_of_interest.shape[0]))\n",
    "    for this_condition in all_evaluation['mu'].unique():\n",
    "        print(this_condition)\n",
    "        if use_follower_threshold==True:\n",
    "            row_of_interest=(all_evaluation['mu'].reset_index(drop=True)==this_condition)&(all_evaluation['object'].reset_index(drop=True)==type_key) &(rows_of_follower.reset_index(drop=True))\n",
    "        else:\n",
    "            row_of_interest=(all_evaluation['mu'].reset_index(drop=True)==this_condition)&(all_evaluation['object'].reset_index(drop=True)==type_key)&(long_following_epochs.reset_index(drop=True))\n",
    "        plot_visual_evoked_behaviour(these_metrics,these_normalised_metrics,after_movement_ith_trial,after_no_movement_ith_trial,analysis_methods,metrics_name,row_of_interest,type_key,this_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movement_trial_boolean,these_metrics,these_normalised_metrics=split_trials(analysis_methods,all_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_name='velocity'\n",
    "for type_key in all_evaluation['object'].unique():\n",
    "    print(type_key)\n",
    "    rows_of_follower=follower_of_interest.repeat(int(all_evaluation.shape[0]/follower_of_interest.shape[0]))\n",
    "    for this_condition in all_evaluation['mu'].unique():\n",
    "        print(this_condition)\n",
    "        if use_follower_threshold==True:\n",
    "            row_of_interest=(all_evaluation['mu'].reset_index(drop=True)==this_condition)&(all_evaluation['object'].reset_index(drop=True)==type_key)&(rows_of_follower.reset_index(drop=True))\n",
    "        else:\n",
    "            row_of_interest=(all_evaluation['mu'].reset_index(drop=True)==this_condition)&(all_evaluation['object'].reset_index(drop=True)==type_key)&(long_following_epochs.reset_index(drop=True))\n",
    "        plot_visual_evoked_behaviour(these_metrics,these_normalised_metrics,after_movement_ith_trial,after_no_movement_ith_trial,analysis_methods,metrics_name,row_of_interest,type_key,this_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_name='velocity'\n",
    "for type_key in all_evaluation['object'].unique():\n",
    "    print(type_key)\n",
    "    if use_follower_threshold==True:\n",
    "        rows_of_follower=follower_of_interest.repeat(int(all_evaluation.shape[0]/follower_of_interest.shape[0]))\n",
    "        row_of_interest=(all_evaluation['object'].reset_index(drop=True)==type_key)&rows_of_follower.reset_index(drop=True)\n",
    "    else:\n",
    "        row_of_interest=(all_evaluation['object'].reset_index(drop=True)==type_key)&(long_following_epochs.reset_index(drop=True))\n",
    "    #print(row_of_interest[row_of_interest==True].index.to_list())\n",
    "    plot_visual_evoked_behaviour(these_metrics,these_normalised_metrics,after_movement_ith_trial,after_no_movement_ith_trial,analysis_methods,'velocity',row_of_interest,type_key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unity_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
