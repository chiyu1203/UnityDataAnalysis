{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Session 0.0: Load packages and customised functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a useful function to generate a data list for further analysis\n",
    "import os,json,sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "##need to add this additional cell because useful tools are in another folder. Need to integrate these two folders one day\n",
    "current_working_directory = Path.cwd()\n",
    "parent_dir = current_working_directory.resolve().parents[0]\n",
    "sys.path.insert(0, str(parent_dir) + \"\\\\utilities\")\n",
    "from useful_tools import select_animals_gpt,find_file\n",
    "from data_cleaning import preprocess_fictrac_data\n",
    "from locustvr_converter import preprocess_matrex_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Session 0.1: Load analysis methods in python dictionary form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = \"./analysis_methods_dictionary.json\"\n",
    "with open(json_file, \"r\") as f:\n",
    "    analysis_methods = json.loads(f.read())\n",
    "    \n",
    "# sheet_name=\"Zball\"\n",
    "# Datasets=\"Z:/DATA/experiment_trackball_Optomotor\"\n",
    "# thisDataset = f\"{Datasets}/{sheet_name}\"\n",
    "thisDataset =\"D:/MatrexVR_Swarm_Data/RunData\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Session 0.2: Load animals' experiment directory into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this cell searches for a folder with a specified experiment_name under the dataset path and list up all the dat file in that folder.\n",
    "## In this project, we usually have one dat file in that folder so there is no confusion\n",
    "dir_list = []\n",
    "file_type=\".csv\"\n",
    "for root, dirs, files in os.walk(thisDataset):\n",
    "    for folder in dirs:\n",
    "        folder_path=os.path.join(root,folder)\n",
    "        if any(name.endswith(file_type) for name in os.listdir(folder_path)):\n",
    "            dir_list.append(folder_path.replace(\"\\\\\", \"/\"))\n",
    "\n",
    "\n",
    "print(f\"these directories are found {dir_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list[:-11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Session 1.0: Create curated dataset based on a list of experiment directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern=\"VR*.h5\"\n",
    "for this_dir in dir_list[3:]:\n",
    "    if any(Path(this_dir).glob(pattern)) and analysis_methods.get(\"overwrite_curated_dataset\")==False:\n",
    "        print(f\"curated matrexvr h5 database found in {this_dir}. Skip this file\")\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"no curated matrexvr h5 database in {this_dir}. Create curated file\")\n",
    "        preprocess_matrex_data(this_dir,analysis_methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Session 1.1: plot responses from individual experiments (usually 4 animal an experiment). Different colour mark different animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_pattern=(\"VR*score.h5\")\n",
    "colour_code=analysis_methods.get(\"graph_colour_code\")\n",
    "for this_dir in dir_list[:-11]:\n",
    "    h5_dirs=find_file(this_dir,h5_pattern)\n",
    "    fig = plt.figure(figsize=(18, 5),tight_layout=True)\n",
    "    ax1 = plt.subplot2grid((1, 18), (0, 0),colspan=8)\n",
    "    ax2 = plt.subplot2grid((1, 18), (0, 8))\n",
    "    ax3 = plt.subplot2grid((1, 18), (0, 9),colspan=8)\n",
    "    ax4 = plt.subplot2grid((1, 18), (0, 17))\n",
    "    for idx,this_file in enumerate(h5_dirs):\n",
    "        this_color=colour_code[idx]\n",
    "        if this_file.stem in ['VR4_Swarm_2024-08-16_131719_score','VR4_Swarm_2024-08-16_145857_score']:\n",
    "            continue\n",
    "        df = pd.read_hdf(this_file)\n",
    "        df_stim = df.loc[(df['loss'] < 0.05) & (df['distTotal'] >= 1.0)&(df ['density'] > 0)] \n",
    "        df_stim = df_stim.reset_index(drop=True)\n",
    "        ax1.set_xscale('log')\n",
    "        ax1.set_ylim([-4,4])        \n",
    "        ax3.set_xscale('log')\n",
    "        ax3.set_ylim([1,900])\n",
    "        ax1.scatter(df_stim['order'], df_stim['mean_angle'],c=this_color)\n",
    "        ax3.scatter(df_stim['order'], df_stim['distTotal'],c=this_color)\n",
    "        ax2.set_ylim([-4,4])\n",
    "        ax2.set_yticks([])\n",
    "        ax2.set_xticks([])\n",
    "        ax4.set_ylim([1,900])\n",
    "        ax4.set_yticks([])\n",
    "        ax4.set_xticks([])\n",
    "        df_isi = df.loc[(df['loss'] < 0.05) & (df['distTotal'] >= 1.0)&(df ['density'] == 0)]\n",
    "        df_isi = df_isi.reset_index(drop=True)\n",
    "        if len(df_isi)>0:\n",
    "            ax2.scatter(df_isi.iloc[0]['order']/2, df_isi.iloc[0]['mean_angle'],c=this_color)\n",
    "            #ax2.scatter(df.iloc[-1]['order'], df.iloc[-1]['mean_angle'],c=this_color,alpha=0.2)\n",
    "            ax4.scatter(df_isi.iloc[0]['order']/2, df_isi.iloc[0]['distTotal'],c=this_color)\n",
    "            #ax4.scatter(df.iloc[-1]['order'], df.iloc[-1]['distTotal'],c=this_color,alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Session 2.0: select animal based on condition and return which a directory list and a list of vr rig number to specify which animal to analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your Excel file\n",
    "dir_list = []\n",
    "file_type=\".h5\"\n",
    "using_google_sheet=True\n",
    "sheet_name = \"Unity_MatrexVR\"\n",
    "experiment_name=analysis_methods.get(\"experiment_name\")\n",
    "# if type(thisDataset) == str:\n",
    "#     thisDataset = Path(thisDataset)\n",
    "if analysis_methods.get(\"load_individual_data\") == True:\n",
    "    if using_google_sheet==True:\n",
    "        database_id = \"1UL4eEUrQMapx9xz11-IyOSlPBcep1I9vBJ2uGgVudb8\"\n",
    "                #https://docs.google.com/spreadsheets/d/1UL4eEUrQMapx9xz11-IyOSlPBcep1I9vBJ2uGgVudb8/edit?usp=sharing\n",
    "        url = f\"https://docs.google.com/spreadsheets/d/{database_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
    "        #df = pd.read_excel(url, engine='openpyxl')## use this function if the file is not google sheet but uploaded excel file\n",
    "        df = pd.read_csv(url)\n",
    "    else:\n",
    "        excel_file_path = \"Z:/DATA/experiment_trackball_Optomotor/Locusts Management.xlsx\"\n",
    "        print(f\"using a database {excel_file_path} from the server but this file might be outdated\")\n",
    "        # Create a 'with' statement to open and read the Excel file\n",
    "        with pd.ExcelFile(excel_file_path) as xls:\n",
    "            # Read the Excel sheet into a DataFrame with the sheet name (folder name)\n",
    "            df = pd.read_excel(xls, sheet_name)\n",
    "        ##list up the conditions and answers as strings for input argument to select animal. One condition must pair with one answer\n",
    "    if analysis_methods.get(\"select_animals_by_condition\") == True:\n",
    "        animal_of_interest=select_animals_gpt(df,\"Stimulus Type (list up all the stimulus paradigm this animal receive)\",\"kappa\")\n",
    "        #print(animal_of_interest)\n",
    "    else:\n",
    "        animal_of_interest=df\n",
    "    folder_name=animal_of_interest[\"folder name\"].values\n",
    "    dir_tile=np.tile(thisDataset, (len(folder_name), 1))\n",
    "    vr_no=animal_of_interest[\"VR number\"].values\n",
    "    dir_list = [''.join([x[0], '/', y]) for x,y in zip(dir_tile,folder_name)]\n",
    "else:\n",
    "    for root, dirs, files in os.walk(thisDataset):\n",
    "        for folder in dirs:\n",
    "            folder_path=os.path.join(root,folder)\n",
    "            if any(name.endswith(file_type) for name in os.listdir(folder_path)):\n",
    "                dir_list.append(folder_path.replace(\"\\\\\", \"/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Session 2.1: plot individual trials and mark dots based on which rigs that animal is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2,ax3) = plt.subplots(\n",
    "    nrows=1, ncols=3, figsize=(18, 7), tight_layout=True\n",
    ")\n",
    "alpha_dictionary = {0.1: 0.2, 1.0: 0.4, 10.0: 0.6,100000.0:1}\n",
    "colour_code=analysis_methods.get(\"graph_colour_code\")\n",
    "y_axis_lim=[0.1,100]\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_ylim([y_axis_lim[0],y_axis_lim[1]])\n",
    "ax1.set(\n",
    "    yticks=[y_axis_lim[0], y_axis_lim[1]],\n",
    "    ylabel=\"Change in Travel distance (ratio)\",\n",
    "    xticks=list(alpha_dictionary.keys()),\n",
    "    xlabel=\"Order (kappa)\",\n",
    ")\n",
    "ax2.set(\n",
    "    ylabel=\"trial n travel distance\",\n",
    "    xlabel=\"pre-stim interval n travel distance\",\n",
    ")\n",
    "ax3.set(\n",
    "    ylabel=\"trial n travel distance\",\n",
    "    xlabel=\"trial n-1 travel distance\",\n",
    ")\n",
    "df_all=[]\n",
    "\n",
    "if 'vr_no' in locals():\n",
    "    for this_dir,this_vr in zip(dir_list,vr_no):\n",
    "        this_color=colour_code[this_vr]\n",
    "        locust_pattern = f\"VR{this_vr}*score.h5\"\n",
    "        found_result = find_file(Path(this_dir), locust_pattern)        \n",
    "        df = pd.read_hdf(found_result)\n",
    "        #set some thresholds to remove back tracking \n",
    "        df.loc[(df[\"distTotal\"]<5.0) | (df[\"loss\"]> 0.05), \"distTotal\"] = np.nan\n",
    "        df['alpha'] = df['order'].map(alpha_dictionary)\n",
    "        df_all.append(df)\n",
    "        ax1.scatter(df.iloc[3::2][\"order\"], df[3::2][\"distTotal\"]/df[2:-1:2][\"distTotal\"],c=this_color)\n",
    "        ax1.scatter(df.iloc[1][\"order\"], df.iloc[1][\"distTotal\"]/df.iloc[0][\"distTotal\"]*5,c=this_color)\n",
    "        ax2.scatter(df.iloc[0][\"distTotal\"]/5,df.iloc[1][\"distTotal\"],c=this_color,alpha=df.iloc[1]['alpha'])\n",
    "        ax2.scatter(df[2:-1:2][\"distTotal\"],df.iloc[3::2][\"distTotal\"],c=this_color,alpha=df.iloc[3::2]['alpha'])\n",
    "        ax3.scatter(df.iloc[1:-2:2][\"distTotal\"], df[3::2][\"distTotal\"],c=this_color,alpha=df.iloc[3::2]['alpha']) \n",
    "\n",
    "else:\n",
    "    h5_pattern=(\"VR*score.h5\")\n",
    "    for this_dir in dir_list:\n",
    "        h5_dirs=find_file(this_dir,h5_pattern)\n",
    "plt.show()\n",
    "print(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.concat(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stim=test[test['density']>0]\n",
    "df_isi=test[test['density']==0]\n",
    "for key, grp in df_stim.groupby('order'):\n",
    "    print(f\"order:{key}\")\n",
    "    plot_sercansincos(grp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, grp in df_isi.groupby('order'):\n",
    "    print(f\"order:{key}\")\n",
    "    plot_sercansincos(grp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'vr_no' in locals():\n",
    "    for this_dir,this_vr in zip(dir_list,vr_no):\n",
    "        this_color=colour_code[this_vr]\n",
    "        locust_pattern = f\"VR{this_vr}*score.h5\"\n",
    "        found_result = find_file(Path(this_dir), locust_pattern)        \n",
    "        df = pd.read_hdf(found_result)\n",
    "        df.loc[(df[\"distTotal\"]<5) | (df[\"loss\"]> 0.05), \"distTotal\"] = np.nan\n",
    "        df_stim=df[df['density']>0]\n",
    "        df_isi=df[df['density']==0]\n",
    "        for key, grp in df_stim.groupby('order'):\n",
    "            print(f\"order:{key}\")\n",
    "            plot_sercansincos(grp)\n",
    "else:\n",
    "    h5_pattern=(\"VR*score.h5\")\n",
    "    for this_dir in dir_list:\n",
    "        h5_dirs=find_file(this_dir,h5_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stim=df[df['density']>0]\n",
    "df_isi=df[df['density']==0]\n",
    "for key, grp in df_stim.groupby('order'):\n",
    "    grp.loc[(grp[\"distTotal\"]<5) | (grp[\"loss\"]> 0.05), \"distTotal\"] = np.nan\n",
    "    print(f\"order:{key}\")\n",
    "    plot_sercansincos(grp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "grouped = df.groupby('groups')\n",
    "for key, grp in grouped:\n",
    "    grp = mahalanobis_outliers(grp, ['sin', 'cos'])\n",
    "    df.loc[grp.index, 'outlier'] = grp['outlier']\n",
    "\n",
    "df = df[df['outlier'] == False]\n",
    "\n",
    "o = 1.0\n",
    "d = 16\n",
    "df = df.loc[df[\"order\"] == o]\n",
    "df = df.loc[df[\"density\"] == d]\n",
    "'''\n",
    "def plot_sercansincos(df):\n",
    "    cos = df[\"cos\"]\n",
    "    sin = df[\"sin\"]\n",
    "    fig, ax = plt.subplots(dpi=300, figsize=(1.1,0.25))\n",
    "    plt.rcParams.update(plt.rcParamsDefault)\n",
    "    plt.rcParams.update({'font.size': 8})\n",
    "    plt.set_cmap('cividis')\n",
    "\n",
    "    # Set the axis line width to 2\n",
    "    plt.rcParams['ytick.major.width'] = 2\n",
    "    plt.rcParams['xtick.major.width'] = 2\n",
    "    plt.rcParams['axes.linewidth'] = 2\n",
    "    plt.rcParams['font.family'] = 'Arial'\n",
    "    sns.kdeplot(cos, cut=0, color=\"#21918c\", fill=True, alpha=0.9)#)#, lw=1\n",
    "\n",
    "\n",
    "    plt.xlim(-1,1)\n",
    "    #ax.xaxis.tick_top()\n",
    "    #plt.ylim(0,np.max(sin))\n",
    "    plt.title(\"r cos\\u03F4\")\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "\n",
    "    #plt.xticks(rotation = 90)\n",
    "    ax.set_yticks([])\n",
    "    plt.ylabel(\"\")\n",
    "    plt.xlabel(\"\")\n",
    "    #ax.set_xticks([])\n",
    "    plt.rcParams.update({'font.size': 8})\n",
    "    #plt.gca().set_aspect('equal')\n",
    "    #plt.savefig(savefolder + \"/plot_noabs.svg\")\n",
    "    #plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.4)\n",
    "    #plt.savefig(savefolder + \"/\"  + str(o) + \"_\" + str(d) + \"cos.svg\")\n",
    "    plt.show()   \n",
    "\n",
    "    fig, ax = plt.subplots(dpi=300, figsize=(1.1,0.25))\n",
    "    plt.rcParams['ytick.major.width'] = 2\n",
    "    plt.rcParams['xtick.major.width'] = 2\n",
    "    plt.rcParams['axes.linewidth'] = 2\n",
    "    plt.rcParams['font.family'] = 'Arial'\n",
    "    sns.kdeplot(sin, cut=0, color=\"#21918c\",  fill=True, alpha=0.9)#),lw=1,\n",
    "    plt.xlim(1,-1)\n",
    "\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "\n",
    "    plt.xticks(rotation = 90)\n",
    "    ax.set_yticks([])\n",
    "    plt.ylabel(\"\")\n",
    "    plt.xlabel(\"\")\n",
    "    plt.title(\"r sin\\u03F4\")\n",
    "\n",
    "    plt.rcParams.update({'font.size': 8})\n",
    "\n",
    "    plt.subplots_adjust(bottom=0.4)\n",
    "    #plt.savefig(savefolder + \"/\"  + str(o) + \"_\" + str(d) + \"sin.svg\")\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfXY = pd.read_hdf(savefolder + '/' +'XY.h5')\n",
    "print(dfXY.columns.tolist())\n",
    "#print(dfXY.index)\n",
    "print(dfXY[\"X\"])\n",
    "\n",
    "outliers = df[df[\"outlier\"]]\n",
    "print(df)\n",
    "outlier_fnames = outliers[\"fname\"].unique()\n",
    "mask = dfXY[\"fname\"].isin(outlier_fnames)\n",
    "filtdfXY = dfXY[~mask]\n",
    "dfXY = filtdfXY \n",
    "\n",
    "\n",
    "\n",
    "o = 1.0\n",
    "d = 16\n",
    "dfXY = dfXY.loc[dfXY[\"order\"] == o]\n",
    "dfXY = dfXY.loc[dfXY[\"density\"] == d]\n",
    "\n",
    "\n",
    "a = dfXY.groupby('fname')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(1.1,1.1), dpi=300) \n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "# Set the axis line width to 2\n",
    "plt.rcParams['ytick.major.width'] = 2\n",
    "plt.rcParams['xtick.major.width'] = 2\n",
    "plt.rcParams['axes.linewidth'] = 2\n",
    "plt.rcParams['font.family'] = 'Helvetica'\n",
    "# Get the colormap\n",
    "cmap = plt.get_cmap('viridis')\n",
    "# Get the total number of trajectories\n",
    "n = len(a)\n",
    "\n",
    "#plt.style.use('dark_background') \n",
    "for i, (key2, grp2) in enumerate(a):\n",
    "    xx = grp2[\"X\"].values\n",
    "    yy = grp2[\"Y\"].values\n",
    "    color = cmap(i/n)\n",
    "    plt.plot(xx, yy, color=color, linewidth=1)\n",
    "\n",
    "    # Calculate angles, radii, etc. (your existing code)\n",
    "\n",
    "plt.xlim(-40, 40)\n",
    "plt.ylim(-40, 40)\n",
    "plt.yticks([-40, 0, 40])\n",
    "plt.xticks([-40, 0, 40])\n",
    "\n",
    "\n",
    "plt.title(\"Density 16\\nOrder 1\", fontsize=8)                                                           \n",
    "\n",
    "\n",
    "# Set the aspect ratio to be equal\n",
    "plt.gca().set_aspect('equal')\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(savefolder + \"/\" + str(o) + \"_\" + str(d) +\".svg\")\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Session 2.1: Analyse optomotor response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because the import does not update the new version of python.\n",
    "# Need to restart kernel \n",
    "output0_across_exp=[]\n",
    "output1_across_exp=[]\n",
    "output2_across_exp=[]\n",
    "output3_across_exp=[]\n",
    "output4_across_exp=[]\n",
    "for this_dir in dir_list:\n",
    "    if \"archive\" in this_dir:\n",
    "        print(f\"skip archive folder for {this_dir}\")\n",
    "        continue\n",
    "    summary,speed,rotation,travel_distance_whole_session=main(this_dir,analysis_methods)\n",
    "    output0_across_exp.append(summary)\n",
    "    output1_across_exp.append(speed)\n",
    "    output2_across_exp.append(rotation)\n",
    "    output3_across_exp.append(travel_distance_whole_session)\n",
    "    output4_across_exp.append(this_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Session 2.2: Analyse optomotor response with multi-engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##this cell start the multi-engines. Make sure to run only once\n",
    "import time\n",
    "import ipyparallel as ipp\n",
    "def show_clusters():\n",
    "    clusters = ipp.ClusterManager().load_clusters() \n",
    "    print(\"{:15} {:^10} {}\".format(\"cluster_id\", \"state\", \"cluster_file\")) \n",
    "    for c in clusters:\n",
    "        cd = clusters[c].to_dict()\n",
    "        cluster_id = cd['cluster']['cluster_id']\n",
    "        controller_state = cd['controller']['state']['state']\n",
    "        cluster_file = getattr(clusters[c], '_trait_values')['cluster_file']\n",
    "        print(\"{:15} {:^10} {}\".format(cluster_id, controller_state, cluster_file))\n",
    "    return cluster_id\n",
    "\n",
    "cluster = ipp.Cluster(n=6)\n",
    "await cluster.start_cluster()\n",
    "cluster_neuropc=show_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##input cluster_id from previous cell\n",
    "rc = ipp.Client(cluster_id=cluster_neuropc)\n",
    "\n",
    "# Create a DirectView for parallel execution\n",
    "dview = rc.direct_view()\n",
    "\n",
    "# Define a function for parallel processing\n",
    "def process_directory(this_dir, analysis_methods):\n",
    "    from pathlib import Path\n",
    "    import sys\n",
    "    current_working_directory = Path.cwd()\n",
    "    parent_dir = current_working_directory.resolve().parents[0]\n",
    "    sys.path.insert(0, str(parent_dir) + \"\\\\utilities\")\n",
    "    from locustvr_converter import preprocess_matrex_data\n",
    "    #from analyse_stimulus_evoked_response import main\n",
    "    # Check if the H5 file (curated dataset) exists\n",
    "    #summary,speed,rotation = main(thisDir, analysis_methods)\n",
    "    preprocess_matrex_data(this_dir,analysis_methods)\n",
    "    #return None\n",
    "\n",
    "# Define analysis_methods\n",
    "\n",
    "# Use parallel execution to process directories\n",
    "dview.map_sync(process_directory, dir_list, [analysis_methods] * len(dir_list))\n",
    "\n",
    "# # Initialize result lists\n",
    "# output0_across_exp=[]\n",
    "# output1_across_exp=[]\n",
    "# output2_across_exp=[]\n",
    "\n",
    "# # Collect and organize results\n",
    "# for result in results:\n",
    "#     if result is not None:\n",
    "#         summary,speed,rotation = result\n",
    "#         output0_across_exp.append(summary)\n",
    "#         output1_across_exp.append(speed)\n",
    "#         output2_across_exp.append(rotation)\n",
    "\n",
    "# # Now, you have the results collected in the respective lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Session 2.3: Load existing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_travel_distance_trials=df.groupby(['ID','Orientation'])['travel_distance'].mean().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_travel_distance_trials[::2]\n",
    "np.savetxt(\"inward.csv\", mean_travel_distance_trials[::2], delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.divide(mean_travel_distance_trials[::2],mean_travel_distance_trials[1::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(mean_travel_distance_trials[::2],mean_travel_distance_trials[1::2])\n",
    "plt.xlim([0, 1500])\n",
    "plt.ylim([0, 1500])\n",
    "ax = plt.gca()\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 350\n",
    "n, bins, patches = plt.hist(df['travel_distance'],num_bins)\n",
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel = df[df['travel_distance']>30]\n",
    "file_name=f\"orientation_database_stationary30.csv\"\n",
    "dfsel.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel_balance_travel_distance=dfsel[dfsel['ID'].isin([3,4,8,9,10,11,12,13,14])]\n",
    "file_name=f\"orientation_database_stationary30_balance_travel_distance.csv\"\n",
    "dfsel_balance_travel_distance.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['travel_distance']>150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for this_group in df[\"growth_condition\"].unique():\n",
    "    d_con=df[(df[\"growth_condition\"]==this_group) & (df[\"travel_distance\"]<200)]\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=1, ncols=len(d_con[\"stim_type\"].unique()), figsize=(25, 3), sharey=True,tight_layout=True\n",
    "    )\n",
    "    for this_stim, ax in zip(np.sort(d_con[\"stim_type\"].unique()),axes.reshape(-1)):\n",
    "        ax.scatter(d_con[(d_con[\"stim_type\"]==this_stim) & (d_con[\"travel_distance\"]>5)][\"travel_distance\"],d_con[(d_con[\"stim_type\"]==this_stim)&(d_con[\"travel_distance\"]>5)][\"opto_index\"])\n",
    "        ax.set_xticks([0, 200])\n",
    "        ax.set_yticks([-1, 1])\n",
    "        ax.set_xlim([0, 200])\n",
    "        ax.set_ylim([-1, 1])\n",
    "        ax.set_box_aspect(1)\n",
    "        ax.set_ylabel(\"Optomotor index\")\n",
    "        ax.set_xlabel(\"Travel distance (mm)\")\n",
    "        ax.set_title(f\"Number of dots:{this_stim}\")\n",
    "    fig.suptitle(f\"Growth condition: {this_group}\", fontsize=16)\n",
    "    fig_name=f\"scatter_plot_{this_group}.png\"\n",
    "    fig.savefig(fig_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['travel_distance']>150]['file_path'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel = df[df['travel_distance']>5]\n",
    "#df.hist('Median speed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(output3_across_exp,bins=range(0,10000,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_frame=analysis_methods.get(\"interval_duration\")*analysis_methods.get(\"frame_rate\")\n",
    "culz_stim_onset=test4[:,init_frame]\n",
    "DF=pd.DataFrame(culz_stim_onset)\n",
    "file_name=f\"culz_onset4.csv\"\n",
    "DF.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_frame=analysis_methods.get(\"interval_duration\")*analysis_methods.get(\"frame_rate\")\n",
    "prestim_analyse=5\n",
    "prestim_5_sec=init_frame-prestim_analyse*analysis_methods.get(\"frame_rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "culz_stim_onset=output2_arr[:,init_frame]\n",
    "culz_stim_onset_5sec_before=output2_arr[:,init_frame-prestim_5_sec]\n",
    "this_response=culz_stim_onset-culz_stim_onset_5sec_before\n",
    "DF=pd.DataFrame(this_response)\n",
    "file_name=f\"culz_onset.csv\"\n",
    "DF.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Session 3: plotting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_paradigm_name= analysis_methods.get(\"experiment_name\")\n",
    "colormap = np.array(analysis_methods.get(\"graph_colour_code\"))\n",
    "fig2, (ax3, ax4) = plt.subplots(\n",
    "    nrows=1, ncols=2, figsize=(18, 7), tight_layout=True\n",
    ")\n",
    "for i in range(len(output0_across_exp)):\n",
    "    this_animal = output0_across_exp[i]\n",
    "    tmp=this_animal.groupby(\"stim_type\").count()\n",
    "    follow_count_coherence = tmp.index.values\n",
    "    for j in range(len(this_animal.groupby(\"stim_type\"))):\n",
    "        this_coherence=follow_count_coherence[j]\n",
    "        this_response = this_animal.loc[\n",
    "            this_animal[\"stim_type\"] == this_coherence, \"opto_index\"\n",
    "        ].values\n",
    "        # this_coherence = x_axis_value_running_trials[i]\n",
    "        mean_response = np.mean(this_response, axis=0)\n",
    "        sem_response = np.std(this_response, axis=0, ddof=1) / np.sqrt(\n",
    "            this_response.shape[0]\n",
    "        )\n",
    "        ax3.errorbar(\n",
    "            this_coherence,\n",
    "            mean_response,\n",
    "            yerr=sem_response,\n",
    "            c=colormap[5],\n",
    "            fmt=\"o\",\n",
    "            elinewidth=2,\n",
    "            capsize=3,\n",
    "        )\n",
    "    ax3.set_ylim(-1, 1)\n",
    "    ax3.set(\n",
    "        yticks=[-1, 0, 1],\n",
    "        ylabel=\"Optomotor Index\",\n",
    "        xlabel=visual_paradigm_name,)\n",
    "    # ax4.scatter(follow_count_coherence, follow_count, c=colormap[0], marker=\"o\")\n",
    "    # ax4.set_ylim(0, 15)\n",
    "    # ax4.set(\n",
    "    #     yticks=[0, 15],\n",
    "    #     ylabel=\"Follow response (count)\",\n",
    "    #     xticks=[100, 50, 0, -50, -100],\n",
    "    #     xlabel=\"Coherence level (%)\",\n",
    "    # )\n",
    "    ##following one dot (dot lifetime)\n",
    "    ##memory part (30s)\n",
    "    ##interval: rondot\n",
    "    ##continous"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_calib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
